{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apache Celeborn\u2122 (Incubating)","title":"Home"},{"location":"#apache-celeborntm-incubating","text":"","title":"Apache Celeborn\u2122 (Incubating)"},{"location":"celeborn_ratis_shell/","text":"Celeborn Ratis-shell Ratis-shell is the command line interface of Ratis. Celeborn uses Ratis to implement the HA function of the master, Celeborn directly introduces ratis-shell package into the project then it's convenient for Celeborn Admin to operate the master ratis service. Note : Ratis-shell is currently only experimental . The compatibility story is not considered for the time being. Availability Version Available in src tarball? Available in bin tarball? < 0.3.0 No No >= 0.3.0 Yes Yes Setting up the Celeborn ratis-shell Celeborn directly introduces the ratis-shell into the project, users don't need to set up ratis-shell env from ratis repo. User can directly download the Celeborn source tarball from Download and build the Celeborn according to build_and_test or just download the pre-built binary tarball from Download to get the binary package apache-celeborn-<VERSION>-bin.tgz . After getting the binary package apache-celeborn-<VERSION>-bin.tgz : $ tar -C <DST_DIR> -zxvf apache-celeborn-<VERSION>-bin.tgz $ ln -s <DST_DIR>/apache-celeborn-<VERSION>-bin <DST_DIR>/celeborn Export the following environment variable and add the bin directory to the $PATH . $ export CELEBORN_HOME=<DST_DIR>/celeborn $ export PATH=${CELEBORN_HOME}/bin:$PATH The following command can be invoked in order to get the basic usage: $ celeborn-ratis sh Usage: celeborn-ratis sh [ generic options ] [ election [ transfer ] [ stepDown ] [ pause ] [ resume ]] [ group [ info ] [ list ]] [ peer [ add ] [ remove ] [ setPriority ]] [ snapshot [ create ]] generic options The generic options pass values for a given ratis-shell property. It supports the following content: -D* , -X* , -agentlib* , -javaagent* $ celeborn-ratis sh -D<property=value> ... Note: Celeborn HA uses NETTY as the default RPC type, for details please refer to configuration celeborn.master.ha.ratis.raft.rpc.type . But Ratis uses GRPC as the default RPC type. So if the user wants to use Ratis shell to access Ratis cluster which uses NETTY RPC type, the generic option -Draft.rpc.type=NETTY should be set to change the RPC type of Ratis shell to Netty. election The election command manages leader election. It has the following subcommands: transfer , stepDown , pause , resume election transfer Transfer a group leader to the specified server. $ celeborn-ratis sh election transfer -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -address <HOSTNAME:PORT> [-groupid <RAFT_GROUP_ID>] election stepDown Make a group leader of the given group step down its leadership. $ celeborn-ratis sh election stepDown -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] election pause Pause leader election at the specified server. Then, the specified server would not start a leader election. $ celeborn-ratis sh election pause -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -address <HOSTNAME:PORT> [-groupid <RAFT_GROUP_ID>] election resume Resume leader election at the specified server. $ celeborn-ratis sh election resume -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -address <HOSTNAME:PORT> [-groupid <RAFT_GROUP_ID>] group The group command manages ratis groups. It has the following subcommands: info , list group info Display the information of a specific raft group. $ celeborn-ratis sh group info -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] group list Display the group information of a specific raft server $ celeborn-ratis sh group list -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] <[-serverAddress <P0_HOST:P0_PORT>]|[-peerId <peerId0>]> peer The peer command manages ratis cluster peers. It has the following subcommands: add , remove , setPriority peer add Add peers to a ratis group. $ celeborn-ratis sh peer add -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] -address <P4_HOST:P4_PORT,...,PN_HOST:PN_PORT> peer remove Remove peers to from a ratis group. $ celeborn-ratis sh peer remove -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] -address <P0_HOST:P0_PORT,...> peer setPriority Set priority to ratis peers. The priority of ratis peer can affect the leader election, the server with the highest priority will eventually become the leader of the cluster. $ celeborn-ratis sh peer setPriority -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] -addressPriority <P0_HOST:P0_PORT|PRIORITY> snapshot The snapshot command manages ratis snapshot. It has the following subcommands: create snapshot create Trigger the specified server take snapshot. $ celeborn-ratis sh snapshot create -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -peerId <peerId0> [-groupid <RAFT_GROUP_ID>]","title":"Ratis Shell"},{"location":"celeborn_ratis_shell/#celeborn-ratis-shell","text":"Ratis-shell is the command line interface of Ratis. Celeborn uses Ratis to implement the HA function of the master, Celeborn directly introduces ratis-shell package into the project then it's convenient for Celeborn Admin to operate the master ratis service. Note : Ratis-shell is currently only experimental . The compatibility story is not considered for the time being.","title":"Celeborn Ratis-shell"},{"location":"celeborn_ratis_shell/#availability","text":"Version Available in src tarball? Available in bin tarball? < 0.3.0 No No >= 0.3.0 Yes Yes","title":"Availability"},{"location":"celeborn_ratis_shell/#setting-up-the-celeborn-ratis-shell","text":"Celeborn directly introduces the ratis-shell into the project, users don't need to set up ratis-shell env from ratis repo. User can directly download the Celeborn source tarball from Download and build the Celeborn according to build_and_test or just download the pre-built binary tarball from Download to get the binary package apache-celeborn-<VERSION>-bin.tgz . After getting the binary package apache-celeborn-<VERSION>-bin.tgz : $ tar -C <DST_DIR> -zxvf apache-celeborn-<VERSION>-bin.tgz $ ln -s <DST_DIR>/apache-celeborn-<VERSION>-bin <DST_DIR>/celeborn Export the following environment variable and add the bin directory to the $PATH . $ export CELEBORN_HOME=<DST_DIR>/celeborn $ export PATH=${CELEBORN_HOME}/bin:$PATH The following command can be invoked in order to get the basic usage: $ celeborn-ratis sh Usage: celeborn-ratis sh [ generic options ] [ election [ transfer ] [ stepDown ] [ pause ] [ resume ]] [ group [ info ] [ list ]] [ peer [ add ] [ remove ] [ setPriority ]] [ snapshot [ create ]]","title":"Setting up the Celeborn ratis-shell"},{"location":"celeborn_ratis_shell/#generic-options","text":"The generic options pass values for a given ratis-shell property. It supports the following content: -D* , -X* , -agentlib* , -javaagent* $ celeborn-ratis sh -D<property=value> ... Note: Celeborn HA uses NETTY as the default RPC type, for details please refer to configuration celeborn.master.ha.ratis.raft.rpc.type . But Ratis uses GRPC as the default RPC type. So if the user wants to use Ratis shell to access Ratis cluster which uses NETTY RPC type, the generic option -Draft.rpc.type=NETTY should be set to change the RPC type of Ratis shell to Netty.","title":"generic options"},{"location":"celeborn_ratis_shell/#election","text":"The election command manages leader election. It has the following subcommands: transfer , stepDown , pause , resume","title":"election"},{"location":"celeborn_ratis_shell/#election-transfer","text":"Transfer a group leader to the specified server. $ celeborn-ratis sh election transfer -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -address <HOSTNAME:PORT> [-groupid <RAFT_GROUP_ID>]","title":"election transfer"},{"location":"celeborn_ratis_shell/#election-stepdown","text":"Make a group leader of the given group step down its leadership. $ celeborn-ratis sh election stepDown -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>]","title":"election stepDown"},{"location":"celeborn_ratis_shell/#election-pause","text":"Pause leader election at the specified server. Then, the specified server would not start a leader election. $ celeborn-ratis sh election pause -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -address <HOSTNAME:PORT> [-groupid <RAFT_GROUP_ID>]","title":"election pause"},{"location":"celeborn_ratis_shell/#election-resume","text":"Resume leader election at the specified server. $ celeborn-ratis sh election resume -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -address <HOSTNAME:PORT> [-groupid <RAFT_GROUP_ID>]","title":"election resume"},{"location":"celeborn_ratis_shell/#group","text":"The group command manages ratis groups. It has the following subcommands: info , list","title":"group"},{"location":"celeborn_ratis_shell/#group-info","text":"Display the information of a specific raft group. $ celeborn-ratis sh group info -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>]","title":"group info"},{"location":"celeborn_ratis_shell/#group-list","text":"Display the group information of a specific raft server $ celeborn-ratis sh group list -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] <[-serverAddress <P0_HOST:P0_PORT>]|[-peerId <peerId0>]>","title":"group list"},{"location":"celeborn_ratis_shell/#peer","text":"The peer command manages ratis cluster peers. It has the following subcommands: add , remove , setPriority","title":"peer"},{"location":"celeborn_ratis_shell/#peer-add","text":"Add peers to a ratis group. $ celeborn-ratis sh peer add -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] -address <P4_HOST:P4_PORT,...,PN_HOST:PN_PORT>","title":"peer add"},{"location":"celeborn_ratis_shell/#peer-remove","text":"Remove peers to from a ratis group. $ celeborn-ratis sh peer remove -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] -address <P0_HOST:P0_PORT,...>","title":"peer remove"},{"location":"celeborn_ratis_shell/#peer-setpriority","text":"Set priority to ratis peers. The priority of ratis peer can affect the leader election, the server with the highest priority will eventually become the leader of the cluster. $ celeborn-ratis sh peer setPriority -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] -addressPriority <P0_HOST:P0_PORT|PRIORITY>","title":"peer setPriority"},{"location":"celeborn_ratis_shell/#snapshot","text":"The snapshot command manages ratis snapshot. It has the following subcommands: create","title":"snapshot"},{"location":"celeborn_ratis_shell/#snapshot-create","text":"Trigger the specified server take snapshot. $ celeborn-ratis sh snapshot create -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -peerId <peerId0> [-groupid <RAFT_GROUP_ID>]","title":"snapshot create"},{"location":"cluster_planning/","text":"Cluster Planning Node Spec Empirical size configs for Celeborn nodes The principle is to try to avoid any hardware(CPU, Memory, Disk Bandwidth/IOPS, Network Bandwidth/PPS)becoming the bottleneck. The goal is to let all the hardware usage be close to each other, for example let the disk IOPS/Bandwidth usage and network usage stay roughly the same so that data will be perfectly pipelined and no back-pressure will be triggered. The goal is hard to reach, and perhaps has a relationship with workload characteristics, and also Celeborn\u2019s configs can have some impact. In our former experience, vCores: memory(GB): Bandwidth( Gbps): Disk IO (KIOps) is better to be 2: 5: 2: 1. We didn\u2019t thoroughly conduct experiments on various configs(it\u2019s hard to do so), so it\u2019s merely a reference. Worker Scale You need to estimate your cluster's max concurrent shuffle size(ES), and get the total usable disk space of a node(NS). The worker count can be (ES * 2 / NS) .","title":"Cluster Planning"},{"location":"cluster_planning/#cluster-planning","text":"","title":"Cluster Planning"},{"location":"cluster_planning/#node-spec","text":"Empirical size configs for Celeborn nodes The principle is to try to avoid any hardware(CPU, Memory, Disk Bandwidth/IOPS, Network Bandwidth/PPS)becoming the bottleneck. The goal is to let all the hardware usage be close to each other, for example let the disk IOPS/Bandwidth usage and network usage stay roughly the same so that data will be perfectly pipelined and no back-pressure will be triggered. The goal is hard to reach, and perhaps has a relationship with workload characteristics, and also Celeborn\u2019s configs can have some impact. In our former experience, vCores: memory(GB): Bandwidth( Gbps): Disk IO (KIOps) is better to be 2: 5: 2: 1. We didn\u2019t thoroughly conduct experiments on various configs(it\u2019s hard to do so), so it\u2019s merely a reference.","title":"Node Spec"},{"location":"cluster_planning/#worker-scale","text":"You need to estimate your cluster's max concurrent shuffle size(ES), and get the total usable disk space of a node(NS). The worker count can be (ES * 2 / NS) .","title":"Worker Scale"},{"location":"deploy/","text":"Deploy Celeborn Unzip the tarball to $CELEBORN_HOME Modify environment variables in $CELEBORN_HOME/conf/celeborn-env.sh EXAMPLE: #!/usr/bin/env bash CELEBORN_MASTER_MEMORY = 4g CELEBORN_WORKER_MEMORY = 2g CELEBORN_WORKER_OFFHEAP_MEMORY = 4g 3. Modify configurations in $CELEBORN_HOME/conf/celeborn-defaults.conf EXAMPLE: single master cluster # used by client and worker to connect to master celeborn.master.endpoints clb-master : 9097 # used by master to bootstrap celeborn.master.host clb-master celeborn.master.port 9097 celeborn.metrics.enabled true celeborn.worker.flusher.buffer.size 256k # If Celeborn workers have local disks and HDFS. Following configs should be added. # If Celeborn workers have local disks, use following config. # Disk type is HDD by defaut. celeborn.worker.storage.dirs /mnt/disk1 : disktype=SSD,/mnt/disk2:disktype=SSD # If Celeborn workers don't have local disks. You can use HDFS. # Do not set `celeborn.worker.storage.dirs` and use following configs. celeborn.storage.activeTypes HDFS celeborn.worker.sortPartition.threads 64 celeborn.worker.commitFiles.timeout 240s celeborn.worker.commitFiles.threads 128 celeborn.master.slot.assign.policy roundrobin celeborn.rpc.askTimeout 240s celeborn.worker.flusher.hdfs.buffer.size 4m celeborn.storage.hdfs.dir hdfs : //<namenode>/celeborn celeborn.worker.replicate.fastFail.duration 240s # Either principal/keytab or valid TGT cache is required to access kerberized HDFS celeborn.storage.hdfs.kerberos.principal user@REALM celeborn.storage.hdfs.kerberos.keytab /path/to/user.keytab # If your hosts have disk raid or use lvm, set celeborn.worker.monitor.disk.enabled to false celeborn.worker.monitor.disk.enabled false EXAMPLE: HA cluster # used by client and worker to connect to master celeborn.master.endpoints clb-1 : 9097,clb-2:9097,clb-3:9097 # used by master nodes to bootstrap, every node should know the topology of whole cluster, for each node, # `celeborn.master.ha.node.id` should be unique, and `celeborn.master.ha.node.<id>.host` is required. celeborn.master.ha.enabled true celeborn.master.ha.node.id 1 celeborn.master.ha.node.1.host clb-1 celeborn.master.ha.node.1.port 9097 celeborn.master.ha.node.1.ratis.port 9872 celeborn.master.ha.node.2.host clb-2 celeborn.master.ha.node.2.port 9097 celeborn.master.ha.node.2.ratis.port 9872 celeborn.master.ha.node.3.host clb-3 celeborn.master.ha.node.3.port 9097 celeborn.master.ha.node.3.ratis.port 9872 celeborn.master.ha.ratis.raft.server.storage.dir /mnt/disk1/celeborn_ratis/ celeborn.metrics.enabled true # If you want to use HDFS as shuffle storage, make sure that flush buffer size is at least 4MB or larger. celeborn.worker.flusher.buffer.size 256k # If Celeborn workers have local disks and HDFS. Following configs should be added. # Celeborn will use local disks until local disk become unavailable to gain the best performance. # Increase Celeborn's off-heap memory if Celeborn write to HDFS. # If Celeborn workers have local disks, use following config. # Disk type is HDD by default. celeborn.worker.storage.dirs /mnt/disk1 : disktype=SSD,/mnt/disk2:disktype=SSD # If Celeborn workers don't have local disks. You can use HDFS. # Do not set `celeborn.worker.storage.dirs` and use following configs. celeborn.storage.activeTypes HDFS celeborn.worker.sortPartition.threads 64 celeborn.worker.commitFiles.timeout 240s celeborn.worker.commitFiles.threads 128 celeborn.master.slot.assign.policy roundrobin celeborn.rpc.askTimeout 240s celeborn.worker.flusher.hdfs.buffer.size 4m celeborn.storage.hdfs.dir hdfs : //<namenode>/celeborn celeborn.worker.replicate.fastFail.duration 240s # If your hosts have disk raid or use lvm, set celeborn.worker.monitor.disk.enabled to false celeborn.worker.monitor.disk.enabled false Flink engine related configurations: # if you are using Celeborn for flink, these settings will be needed celeborn.worker.directMemoryRatioForReadBuffer 0.4 celeborn.worker.directMemoryRatioToResume 0.5 # these setting will affect performance. # If there is enough off-heap memory you can try to increase read buffers. # Read buffer max memory usage for a data partition is `taskmanager.memory.segment-size * readBuffersMax` celeborn.worker.partition.initial.readBuffersMin 512 celeborn.worker.partition.initial.readBuffersMax 1024 celeborn.worker.readBuffer.allocationWait 10ms Copy Celeborn and configurations to all nodes Start all services. If you install Celeborn distribution in same path on every node and your cluster can perform SSH login then you can fill $CELEBORN_HOME/conf/hosts and use $CELEBORN_HOME/sbin/start-all.sh to start all services. If the installation paths are not identical, you will need to start service manually. Start Celeborn master $CELEBORN_HOME/sbin/start-master.sh Start Celeborn worker $CELEBORN_HOME/sbin/start-worker.sh If Celeborn start success, the output of Master's log should be like this: 22/10/08 19:29:11,805 INFO [main] Dispatcher: Dispatcher numThreads: 64 22/10/08 19:29:11,875 INFO [main] TransportClientFactory: mode NIO threads 64 22/10/08 19:29:12,057 INFO [main] Utils: Successfully started service 'MasterSys' on port 9097. 22/10/08 19:29:12,113 INFO [main] Master: Metrics system enabled. 22/10/08 19:29:12,125 INFO [main] HttpServer: master: HttpServer started on port 9098. 22/10/08 19:29:12,126 INFO [main] Master: Master started. 22/10/08 19:29:57,842 INFO [dispatcher-event-loop-19] Master: Registered worker Host: 192.168.15.140 RpcPort: 37359 PushPort: 38303 FetchPort: 37569 ReplicatePort: 37093 SlotsUsed: 0() LastHeartbeat: 0 Disks: {/mnt/disk1=DiskInfo(maxSlots: 6679, committed shuffles 0 shuffleAllocations: Map(), mountPoint: /mnt/disk1, usableSpace: 448284381184, avgFlushTime: 0, activeSlots: 0) status: HEALTHY dirs , /mnt/disk3=DiskInfo(maxSlots: 6716, committed shuffles 0 shuffleAllocations: Map(), mountPoint: /mnt/disk3, usableSpace: 450755608576, avgFlushTime: 0, activeSlots: 0) status: HEALTHY dirs , /mnt/disk2=DiskInfo(maxSlots: 6713, committed shuffles 0 shuffleAllocations: Map(), mountPoint: /mnt/disk2, usableSpace: 450532900864, avgFlushTime: 0, activeSlots: 0) status: HEALTHY dirs , /mnt/disk4=DiskInfo(maxSlots: 6712, committed shuffles 0 shuffleAllocations: Map(), mountPoint: /mnt/disk4, usableSpace: 450456805376, avgFlushTime: 0, activeSlots: 0) status: HEALTHY dirs } WorkerRef: null Deploy Spark client Copy $CELEBORN_HOME/spark/*.jar to $SPARK_HOME/jars/ Spark Configuration To use Celeborn, following spark configurations should be added. # Shuffle manager class name changed in 0.3.0: # before 0.3.0: org.apache.spark.shuffle.celeborn.RssShuffleManager # since 0.3.0: org.apache.spark.shuffle.celeborn.SparkShuffleManager spark.shuffle.manager org.apache.spark.shuffle.celeborn.SparkShuffleManager # must use kryo serializer because java serializer do not support relocation spark.serializer org.apache.spark.serializer.KryoSerializer # celeborn master spark.celeborn.master.endpoints clb-1 : 9097,clb-2:9097,clb-3:9097 spark.shuffle.service.enabled false # options: hash, sort # Hash shuffle writer use (partition count) * (celeborn.push.buffer.max.size) * (spark.executor.cores) memory. # Sort shuffle writer uses less memory than hash shuffle writer, if your shuffle partition count is large, try to use sort hash writer. spark.celeborn.client.spark.shuffle.writer hash # We recommend setting spark.celeborn.client.push.replicate.enabled to true to enable server-side data replication # If you have only one worker, this setting must be false # If your Celeborn is using HDFS, it's recommended to set this setting to false spark.celeborn.client.push.replicate.enabled true # Support for Spark AQE only tested under Spark 3 # we recommend setting localShuffleReader to false to get better performance of Celeborn spark.sql.adaptive.localShuffleReader.enabled false # If Celeborn is using HDFS spark.celeborn.storage.hdfs.dir hdfs : //<namenode>/celeborn # we recommend enabling aqe support to gain better performance spark.sql.adaptive.enabled true spark.sql.adaptive.skewJoin.enabled true Deploy Flink client Copy $CELEBORN_HOME/flink/*.jar to $FLINK_HOME/lib/ Flink Configuration TO use Celeborn, following flink configurations should be added. shuffle-service-factory.class : org.apache.celeborn.plugin.flink.RemoteShuffleServiceFactory celeborn.master.endpoints : clb-1:9097,clb-2:9097,clb-3:9097 celeborn.client.shuffle.batchHandleReleasePartition.enabled : true celeborn.client.push.maxReqsInFlight : 128 # Network connections between peers celeborn.data.io.numConnectionsPerPeer : 16 # threads number may vary according to your cluster but do not set to 1 celeborn.data.io.threads : 32 celeborn.client.shuffle.batchHandleCommitPartition.threads : 32 celeborn.rpc.dispatcher.numThreads : 32 # Floating buffers may need to change `taskmanager.network.memory.fraction` and `taskmanager.network.memory.max` taskmanager.network.memory.floating-buffers-per-gate : 4096 taskmanager.network.memory.buffers-per-channel : 0 taskmanager.memory.task.off-heap.size : 512m","title":"Overview"},{"location":"deploy/#deploy-celeborn","text":"Unzip the tarball to $CELEBORN_HOME Modify environment variables in $CELEBORN_HOME/conf/celeborn-env.sh EXAMPLE: #!/usr/bin/env bash CELEBORN_MASTER_MEMORY = 4g CELEBORN_WORKER_MEMORY = 2g CELEBORN_WORKER_OFFHEAP_MEMORY = 4g 3. Modify configurations in $CELEBORN_HOME/conf/celeborn-defaults.conf EXAMPLE: single master cluster # used by client and worker to connect to master celeborn.master.endpoints clb-master : 9097 # used by master to bootstrap celeborn.master.host clb-master celeborn.master.port 9097 celeborn.metrics.enabled true celeborn.worker.flusher.buffer.size 256k # If Celeborn workers have local disks and HDFS. Following configs should be added. # If Celeborn workers have local disks, use following config. # Disk type is HDD by defaut. celeborn.worker.storage.dirs /mnt/disk1 : disktype=SSD,/mnt/disk2:disktype=SSD # If Celeborn workers don't have local disks. You can use HDFS. # Do not set `celeborn.worker.storage.dirs` and use following configs. celeborn.storage.activeTypes HDFS celeborn.worker.sortPartition.threads 64 celeborn.worker.commitFiles.timeout 240s celeborn.worker.commitFiles.threads 128 celeborn.master.slot.assign.policy roundrobin celeborn.rpc.askTimeout 240s celeborn.worker.flusher.hdfs.buffer.size 4m celeborn.storage.hdfs.dir hdfs : //<namenode>/celeborn celeborn.worker.replicate.fastFail.duration 240s # Either principal/keytab or valid TGT cache is required to access kerberized HDFS celeborn.storage.hdfs.kerberos.principal user@REALM celeborn.storage.hdfs.kerberos.keytab /path/to/user.keytab # If your hosts have disk raid or use lvm, set celeborn.worker.monitor.disk.enabled to false celeborn.worker.monitor.disk.enabled false EXAMPLE: HA cluster # used by client and worker to connect to master celeborn.master.endpoints clb-1 : 9097,clb-2:9097,clb-3:9097 # used by master nodes to bootstrap, every node should know the topology of whole cluster, for each node, # `celeborn.master.ha.node.id` should be unique, and `celeborn.master.ha.node.<id>.host` is required. celeborn.master.ha.enabled true celeborn.master.ha.node.id 1 celeborn.master.ha.node.1.host clb-1 celeborn.master.ha.node.1.port 9097 celeborn.master.ha.node.1.ratis.port 9872 celeborn.master.ha.node.2.host clb-2 celeborn.master.ha.node.2.port 9097 celeborn.master.ha.node.2.ratis.port 9872 celeborn.master.ha.node.3.host clb-3 celeborn.master.ha.node.3.port 9097 celeborn.master.ha.node.3.ratis.port 9872 celeborn.master.ha.ratis.raft.server.storage.dir /mnt/disk1/celeborn_ratis/ celeborn.metrics.enabled true # If you want to use HDFS as shuffle storage, make sure that flush buffer size is at least 4MB or larger. celeborn.worker.flusher.buffer.size 256k # If Celeborn workers have local disks and HDFS. Following configs should be added. # Celeborn will use local disks until local disk become unavailable to gain the best performance. # Increase Celeborn's off-heap memory if Celeborn write to HDFS. # If Celeborn workers have local disks, use following config. # Disk type is HDD by default. celeborn.worker.storage.dirs /mnt/disk1 : disktype=SSD,/mnt/disk2:disktype=SSD # If Celeborn workers don't have local disks. You can use HDFS. # Do not set `celeborn.worker.storage.dirs` and use following configs. celeborn.storage.activeTypes HDFS celeborn.worker.sortPartition.threads 64 celeborn.worker.commitFiles.timeout 240s celeborn.worker.commitFiles.threads 128 celeborn.master.slot.assign.policy roundrobin celeborn.rpc.askTimeout 240s celeborn.worker.flusher.hdfs.buffer.size 4m celeborn.storage.hdfs.dir hdfs : //<namenode>/celeborn celeborn.worker.replicate.fastFail.duration 240s # If your hosts have disk raid or use lvm, set celeborn.worker.monitor.disk.enabled to false celeborn.worker.monitor.disk.enabled false Flink engine related configurations: # if you are using Celeborn for flink, these settings will be needed celeborn.worker.directMemoryRatioForReadBuffer 0.4 celeborn.worker.directMemoryRatioToResume 0.5 # these setting will affect performance. # If there is enough off-heap memory you can try to increase read buffers. # Read buffer max memory usage for a data partition is `taskmanager.memory.segment-size * readBuffersMax` celeborn.worker.partition.initial.readBuffersMin 512 celeborn.worker.partition.initial.readBuffersMax 1024 celeborn.worker.readBuffer.allocationWait 10ms Copy Celeborn and configurations to all nodes Start all services. If you install Celeborn distribution in same path on every node and your cluster can perform SSH login then you can fill $CELEBORN_HOME/conf/hosts and use $CELEBORN_HOME/sbin/start-all.sh to start all services. If the installation paths are not identical, you will need to start service manually. Start Celeborn master $CELEBORN_HOME/sbin/start-master.sh Start Celeborn worker $CELEBORN_HOME/sbin/start-worker.sh If Celeborn start success, the output of Master's log should be like this: 22/10/08 19:29:11,805 INFO [main] Dispatcher: Dispatcher numThreads: 64 22/10/08 19:29:11,875 INFO [main] TransportClientFactory: mode NIO threads 64 22/10/08 19:29:12,057 INFO [main] Utils: Successfully started service 'MasterSys' on port 9097. 22/10/08 19:29:12,113 INFO [main] Master: Metrics system enabled. 22/10/08 19:29:12,125 INFO [main] HttpServer: master: HttpServer started on port 9098. 22/10/08 19:29:12,126 INFO [main] Master: Master started. 22/10/08 19:29:57,842 INFO [dispatcher-event-loop-19] Master: Registered worker Host: 192.168.15.140 RpcPort: 37359 PushPort: 38303 FetchPort: 37569 ReplicatePort: 37093 SlotsUsed: 0() LastHeartbeat: 0 Disks: {/mnt/disk1=DiskInfo(maxSlots: 6679, committed shuffles 0 shuffleAllocations: Map(), mountPoint: /mnt/disk1, usableSpace: 448284381184, avgFlushTime: 0, activeSlots: 0) status: HEALTHY dirs , /mnt/disk3=DiskInfo(maxSlots: 6716, committed shuffles 0 shuffleAllocations: Map(), mountPoint: /mnt/disk3, usableSpace: 450755608576, avgFlushTime: 0, activeSlots: 0) status: HEALTHY dirs , /mnt/disk2=DiskInfo(maxSlots: 6713, committed shuffles 0 shuffleAllocations: Map(), mountPoint: /mnt/disk2, usableSpace: 450532900864, avgFlushTime: 0, activeSlots: 0) status: HEALTHY dirs , /mnt/disk4=DiskInfo(maxSlots: 6712, committed shuffles 0 shuffleAllocations: Map(), mountPoint: /mnt/disk4, usableSpace: 450456805376, avgFlushTime: 0, activeSlots: 0) status: HEALTHY dirs } WorkerRef: null","title":"Deploy Celeborn"},{"location":"deploy/#deploy-spark-client","text":"Copy $CELEBORN_HOME/spark/*.jar to $SPARK_HOME/jars/","title":"Deploy Spark client"},{"location":"deploy/#spark-configuration","text":"To use Celeborn, following spark configurations should be added. # Shuffle manager class name changed in 0.3.0: # before 0.3.0: org.apache.spark.shuffle.celeborn.RssShuffleManager # since 0.3.0: org.apache.spark.shuffle.celeborn.SparkShuffleManager spark.shuffle.manager org.apache.spark.shuffle.celeborn.SparkShuffleManager # must use kryo serializer because java serializer do not support relocation spark.serializer org.apache.spark.serializer.KryoSerializer # celeborn master spark.celeborn.master.endpoints clb-1 : 9097,clb-2:9097,clb-3:9097 spark.shuffle.service.enabled false # options: hash, sort # Hash shuffle writer use (partition count) * (celeborn.push.buffer.max.size) * (spark.executor.cores) memory. # Sort shuffle writer uses less memory than hash shuffle writer, if your shuffle partition count is large, try to use sort hash writer. spark.celeborn.client.spark.shuffle.writer hash # We recommend setting spark.celeborn.client.push.replicate.enabled to true to enable server-side data replication # If you have only one worker, this setting must be false # If your Celeborn is using HDFS, it's recommended to set this setting to false spark.celeborn.client.push.replicate.enabled true # Support for Spark AQE only tested under Spark 3 # we recommend setting localShuffleReader to false to get better performance of Celeborn spark.sql.adaptive.localShuffleReader.enabled false # If Celeborn is using HDFS spark.celeborn.storage.hdfs.dir hdfs : //<namenode>/celeborn # we recommend enabling aqe support to gain better performance spark.sql.adaptive.enabled true spark.sql.adaptive.skewJoin.enabled true","title":"Spark Configuration"},{"location":"deploy/#deploy-flink-client","text":"Copy $CELEBORN_HOME/flink/*.jar to $FLINK_HOME/lib/","title":"Deploy Flink client"},{"location":"deploy/#flink-configuration","text":"TO use Celeborn, following flink configurations should be added. shuffle-service-factory.class : org.apache.celeborn.plugin.flink.RemoteShuffleServiceFactory celeborn.master.endpoints : clb-1:9097,clb-2:9097,clb-3:9097 celeborn.client.shuffle.batchHandleReleasePartition.enabled : true celeborn.client.push.maxReqsInFlight : 128 # Network connections between peers celeborn.data.io.numConnectionsPerPeer : 16 # threads number may vary according to your cluster but do not set to 1 celeborn.data.io.threads : 32 celeborn.client.shuffle.batchHandleCommitPartition.threads : 32 celeborn.rpc.dispatcher.numThreads : 32 # Floating buffers may need to change `taskmanager.network.memory.fraction` and `taskmanager.network.memory.max` taskmanager.network.memory.floating-buffers-per-gate : 4096 taskmanager.network.memory.buffers-per-channel : 0 taskmanager.memory.task.off-heap.size : 512m","title":"Flink Configuration"},{"location":"deploy_on_k8s/","text":"Deploy Celeborn on Kubernetes Celeborn currently supports rapid deployment by using helm. Before Deploy You should have a Running Kubernetes Cluster. You should understand simple Kubernetes deploy related, e.g. Kubernetes Resources . You have enough permissions to create resources . Installed Helm . Deploy 1. Get Celeborn Binary Package You can find released version of Celeborn on https://celeborn.apache.org/download/. Of course, you can build binary package from master branch or your own branch by using ./build/make-distribution.sh in source code. Anyway, you should unzip and into binary package. 2. Modify Celeborn Configurations Notice: Celeborn Charts Template Files is in the experimental instability stage, the subsequent optimization will be adjusted. The configuration in ./charts/celeborn/values.yaml you should focus on modifying is: image repository - Get images from which repository image tag - Which version of image to use masterReplicas - Number of celeborn master replicas workerReplicas - Number of celeborn worker replicas volumes - How and where to mount volumes (For more information, Volumes ) [Optional] Build Celeborn Docker Image Maybe you want to make your own celeborn docker image, you can use docker build . -f docker/Dockerfile in Celeborn Binary. 3. Helm Install Celeborn Charts More details in Helm Install cd ./charts/celeborn helm install celeborn -n <namespace> . 4. Check Celeborn After the above operation, you should be able to find the corresponding Celeborn Master/Worker by kubectl get pods -n <namespace> Etc. NAME READY STATUS RESTARTS AGE celeborn-master-0 1/1 Running 0 1m ... celeborn-worker-0 1/1 Running 0 1m ... Given that Celeborn Master/Worker Pod takes time to start, you can see the following phenomenon: ** server can't find celeborn-master-0.celeborn-master-svc.default.svc.cluster.local: NXDOMAIN waiting for master Server: 172.17.0.10 Address: 172.17.0.10#53 ... Name: celeborn-master-0.celeborn-master-svc.default.svc.cluster.local Address: 10.225.139.80 Server: 172.17.0.10 Address: 172.17.0.10#53 starting org.apache.celeborn.service.deploy.master.Master, logging to /opt/celeborn/logs/celeborn--org.apache.celeborn.service.deploy.master.Master-1-celeborn-master-0.out ... 23/03/23 14:10:56,081 INFO [main] RaftServer: 0: start RPC server 23/03/23 14:10:56,132 INFO [nioEventLoopGroup-2-1] LoggingHandler: [id: 0x83032bf1] REGISTERED 23/03/23 14:10:56,132 INFO [nioEventLoopGroup-2-1] LoggingHandler: [id: 0x83032bf1] BIND: 0.0.0.0/0.0.0.0:9872 23/03/23 14:10:56,134 INFO [nioEventLoopGroup-2-1] LoggingHandler: [id: 0x83032bf1, L:/0:0:0:0:0:0:0:0:9872] ACTIVE 23/03/23 14:10:56,135 INFO [JvmPauseMonitor0] JvmPauseMonitor: JvmPauseMonitor-0: Started 23/03/23 14:10:56,208 INFO [main] Master: Metrics system enabled. 23/03/23 14:10:56,216 INFO [main] HttpServer: master: HttpServer started on port 9098. 23/03/23 14:10:56,216 INFO [main] Master: Master started. 5. Access Celeborn Service The Celeborn Master/Worker nodes deployed via official Helm charts run as StatefulSet , it can be accessed through Pod IP or Stable Network ID (DNS name) , in above case, the Master/Worker nodes can be accessed through: celeborn-master-0.celeborn-master-svc.default.svc.cluster.local` ... celeborn-worker-0.celeborn-worker-svc.default.svc.cluster.local` ... After a restart, the StatefulSet Pod IP changes but the DNS name remains, this is important for rolling upgrade. When bind address is not set explicitly, Celeborn worker is going to find the first non-loopback address to bind. By default, it use IP address both for address binding and registering, that causes the Master and Client use the IP address to access the Worker, it's problematic after Worker restart as explained above, especially when Graceful Shutdown is enabled. You may want to set celeborn.network.bind.preferIpAddress=false to address such issue. Note that, depends on your Kubernetes network infrastructure, this may cause pressure on DNS service or other network issues compared with using IP address directly. 6. Build Celeborn Client Here, without going into detail on how to configure spark/flink to find celeborn master/worker, mention the key configuration: spark.celeborn.master.endpoints: celeborn-master-0.celeborn-master-svc.<namespace>:9097,celeborn-master-1.celeborn-master-svc.<namespace>:9097,celeborn-master-2.celeborn-master-svc.<namespace>:9097 You can find why config endpoints such way in Kubernetes DNS for Service And Pods Notice: You should ensure that Spark/Flink can find the Celeborn Master/Worker via IP or the Kubernetes DNS mentioned above","title":"Kubernetes"},{"location":"deploy_on_k8s/#deploy-celeborn-on-kubernetes","text":"Celeborn currently supports rapid deployment by using helm.","title":"Deploy Celeborn on Kubernetes"},{"location":"deploy_on_k8s/#before-deploy","text":"You should have a Running Kubernetes Cluster. You should understand simple Kubernetes deploy related, e.g. Kubernetes Resources . You have enough permissions to create resources . Installed Helm .","title":"Before Deploy"},{"location":"deploy_on_k8s/#deploy","text":"","title":"Deploy"},{"location":"deploy_on_k8s/#1-get-celeborn-binary-package","text":"You can find released version of Celeborn on https://celeborn.apache.org/download/. Of course, you can build binary package from master branch or your own branch by using ./build/make-distribution.sh in source code. Anyway, you should unzip and into binary package.","title":"1. Get Celeborn Binary Package"},{"location":"deploy_on_k8s/#2-modify-celeborn-configurations","text":"Notice: Celeborn Charts Template Files is in the experimental instability stage, the subsequent optimization will be adjusted. The configuration in ./charts/celeborn/values.yaml you should focus on modifying is: image repository - Get images from which repository image tag - Which version of image to use masterReplicas - Number of celeborn master replicas workerReplicas - Number of celeborn worker replicas volumes - How and where to mount volumes (For more information, Volumes )","title":"2. Modify Celeborn Configurations"},{"location":"deploy_on_k8s/#optional-build-celeborn-docker-image","text":"Maybe you want to make your own celeborn docker image, you can use docker build . -f docker/Dockerfile in Celeborn Binary.","title":"[Optional] Build Celeborn Docker Image"},{"location":"deploy_on_k8s/#3-helm-install-celeborn-charts","text":"More details in Helm Install cd ./charts/celeborn helm install celeborn -n <namespace> .","title":"3. Helm Install Celeborn Charts"},{"location":"deploy_on_k8s/#4-check-celeborn","text":"After the above operation, you should be able to find the corresponding Celeborn Master/Worker by kubectl get pods -n <namespace> Etc. NAME READY STATUS RESTARTS AGE celeborn-master-0 1/1 Running 0 1m ... celeborn-worker-0 1/1 Running 0 1m ... Given that Celeborn Master/Worker Pod takes time to start, you can see the following phenomenon: ** server can't find celeborn-master-0.celeborn-master-svc.default.svc.cluster.local: NXDOMAIN waiting for master Server: 172.17.0.10 Address: 172.17.0.10#53 ... Name: celeborn-master-0.celeborn-master-svc.default.svc.cluster.local Address: 10.225.139.80 Server: 172.17.0.10 Address: 172.17.0.10#53 starting org.apache.celeborn.service.deploy.master.Master, logging to /opt/celeborn/logs/celeborn--org.apache.celeborn.service.deploy.master.Master-1-celeborn-master-0.out ... 23/03/23 14:10:56,081 INFO [main] RaftServer: 0: start RPC server 23/03/23 14:10:56,132 INFO [nioEventLoopGroup-2-1] LoggingHandler: [id: 0x83032bf1] REGISTERED 23/03/23 14:10:56,132 INFO [nioEventLoopGroup-2-1] LoggingHandler: [id: 0x83032bf1] BIND: 0.0.0.0/0.0.0.0:9872 23/03/23 14:10:56,134 INFO [nioEventLoopGroup-2-1] LoggingHandler: [id: 0x83032bf1, L:/0:0:0:0:0:0:0:0:9872] ACTIVE 23/03/23 14:10:56,135 INFO [JvmPauseMonitor0] JvmPauseMonitor: JvmPauseMonitor-0: Started 23/03/23 14:10:56,208 INFO [main] Master: Metrics system enabled. 23/03/23 14:10:56,216 INFO [main] HttpServer: master: HttpServer started on port 9098. 23/03/23 14:10:56,216 INFO [main] Master: Master started.","title":"4. Check Celeborn"},{"location":"deploy_on_k8s/#5-access-celeborn-service","text":"The Celeborn Master/Worker nodes deployed via official Helm charts run as StatefulSet , it can be accessed through Pod IP or Stable Network ID (DNS name) , in above case, the Master/Worker nodes can be accessed through: celeborn-master-0.celeborn-master-svc.default.svc.cluster.local` ... celeborn-worker-0.celeborn-worker-svc.default.svc.cluster.local` ... After a restart, the StatefulSet Pod IP changes but the DNS name remains, this is important for rolling upgrade. When bind address is not set explicitly, Celeborn worker is going to find the first non-loopback address to bind. By default, it use IP address both for address binding and registering, that causes the Master and Client use the IP address to access the Worker, it's problematic after Worker restart as explained above, especially when Graceful Shutdown is enabled. You may want to set celeborn.network.bind.preferIpAddress=false to address such issue. Note that, depends on your Kubernetes network infrastructure, this may cause pressure on DNS service or other network issues compared with using IP address directly.","title":"5. Access Celeborn Service"},{"location":"deploy_on_k8s/#6-build-celeborn-client","text":"Here, without going into detail on how to configure spark/flink to find celeborn master/worker, mention the key configuration: spark.celeborn.master.endpoints: celeborn-master-0.celeborn-master-svc.<namespace>:9097,celeborn-master-1.celeborn-master-svc.<namespace>:9097,celeborn-master-2.celeborn-master-svc.<namespace>:9097 You can find why config endpoints such way in Kubernetes DNS for Service And Pods Notice: You should ensure that Spark/Flink can find the Celeborn Master/Worker via IP or the Kubernetes DNS mentioned above","title":"6. Build Celeborn Client"},{"location":"migration/","text":"Migration Guide Upgrading from 0.3.1 to 0.3.2 Since 0.3.1, Celeborn changed the default value of raft.client.rpc.request.timeout from 3s to 10s . Since 0.3.1, Celeborn changed the default value of raft.client.rpc.watch.request.timeout from 10s to 20s . Upgrading from 0.3.0 to 0.3.1 Since 0.3.1, Celeborn changed the default value of celeborn.worker.directMemoryRatioToResume from 0.5 to 0.7 . Since 0.3.1, Celeborn changed the default value of celeborn.worker.monitor.disk.check.interval from 60 to 30 . Since 0.3.1, name of JVM metrics changed, see details at CELEBORN-1007. Upgrading from 0.2 to 0.3 Celeborn 0.2 Client is compatible with 0.3 Master/Server, it allows to upgrade Master/Worker first then Client. Note that: It's strongly recommended to use the same version of Client and Celeborn Master/Worker in production. Since 0.3.0, the support of deprecated configurations rss.* is removed. All configurations listed in 0.2.1 docs still take effect, but some of those are deprecated too, please read the bootstrap logs and follow the suggestion to migrate to the new configuration. From 0.3.0 on the default value for celeborn.client.push.replicate.enabled is changed from true to false , users who want replication on should explicitly enable replication. For example, to enable replication for Spark users should add the spark config when submitting job: spark.celeborn.client.push.replicate.enabled=true From 0.3.0 on the default value for celeborn.worker.storage.workingDir is changed from hadoop/rss-worker/shuffle_data to celeborn-worker/shuffle_data , users who want to use origin working dir path should set this configuration. Since 0.3.0, configuration namespace celeborn.ha.master is deprecated, and will be removed in the future versions. All configurations celeborn.ha.master.* should migrate to celeborn.master.ha.* . Since 0.3.0, environment variables CELEBORN_MASTER_HOST and CELEBORN_MASTER_PORT are removed. Instead CELEBORN_LOCAL_HOSTNAME works on both master and worker, which takes high priority than configurations defined in properties file. Since 0.3.0, the Celeborn Master URL schema is changed from rss:// to celeborn:// , for users who start Worker by sbin/start-worker.sh rss://<master-host>:<master-port> , should migrate to sbin/start-worker.sh celeborn://<master-host>:<master-port> . Since 0.3.0, Celeborn supports overriding Hadoop configuration( core-site.xml , hdfs-site.xml , etc.) from Celeborn configuration with the additional prefix celeborn.hadoop. . On Spark client side, user should set Hadoop configuration like spark.celeborn.hadoop.foo=bar , note that spark.hadoop.foo=bar does not take effect; on Flink client and Celeborn Master/Worker side, user should set like celeborn.hadoop.foo=bar . Since 0.3.0, Celeborn master metrics BlacklistedWorkerCount is renamed as ExcludedWorkerCount . Since 0.3.0, Celeborn master http request url /blacklistedWorkers is renamed as /excludedWorkers . Since 0.3.0, introduces a terminology update for Celeborn worker data replication, replacing the previous master/slave terminology with primary/replica . In alignment with this change, corresponding metrics keywords have been adjusted. The following table presents a comprehensive overview of the changes: Key Before v0.3.0 Key After v0.3.0 MasterPushDataTime PrimaryPushDataTime MasterPushDataHandshakeTime PrimaryPushDataHandshakeTime MasterRegionStartTime PrimaryRegionStartTime MasterRegionFinishTime PrimaryRegionFinishTime SlavePushDataTime ReplicaPushDataTime SlavePushDataHandshakeTime ReplicaPushDataHandshakeTime SlaveRegionStartTime ReplicaRegionStartTime SlaveRegionFinishTime ReplicaRegionFinishTime Since 0.3.0, Celeborn's spark shuffle manager change from org.apache.spark.shuffle.celeborn.RssShuffleManager to org.apache.spark.shuffle.celeborn.SparkShuffleManager . User can set spark property spark.shuffle.manager to org.apache.spark.shuffle.celeborn.SparkShuffleManager to use Celeborn remote shuffle service. In 0.3.0, Celeborn still support org.apache.spark.shuffle.celeborn.RssShuffleManager , it will be removed in 0.4.0.","title":"Migration Guide"},{"location":"migration/#migration-guide","text":"","title":"Migration Guide"},{"location":"migration/#upgrading-from-031-to-032","text":"Since 0.3.1, Celeborn changed the default value of raft.client.rpc.request.timeout from 3s to 10s . Since 0.3.1, Celeborn changed the default value of raft.client.rpc.watch.request.timeout from 10s to 20s .","title":"Upgrading from 0.3.1 to 0.3.2"},{"location":"migration/#upgrading-from-030-to-031","text":"Since 0.3.1, Celeborn changed the default value of celeborn.worker.directMemoryRatioToResume from 0.5 to 0.7 . Since 0.3.1, Celeborn changed the default value of celeborn.worker.monitor.disk.check.interval from 60 to 30 . Since 0.3.1, name of JVM metrics changed, see details at CELEBORN-1007.","title":"Upgrading from 0.3.0 to 0.3.1"},{"location":"migration/#upgrading-from-02-to-03","text":"Celeborn 0.2 Client is compatible with 0.3 Master/Server, it allows to upgrade Master/Worker first then Client. Note that: It's strongly recommended to use the same version of Client and Celeborn Master/Worker in production. Since 0.3.0, the support of deprecated configurations rss.* is removed. All configurations listed in 0.2.1 docs still take effect, but some of those are deprecated too, please read the bootstrap logs and follow the suggestion to migrate to the new configuration. From 0.3.0 on the default value for celeborn.client.push.replicate.enabled is changed from true to false , users who want replication on should explicitly enable replication. For example, to enable replication for Spark users should add the spark config when submitting job: spark.celeborn.client.push.replicate.enabled=true From 0.3.0 on the default value for celeborn.worker.storage.workingDir is changed from hadoop/rss-worker/shuffle_data to celeborn-worker/shuffle_data , users who want to use origin working dir path should set this configuration. Since 0.3.0, configuration namespace celeborn.ha.master is deprecated, and will be removed in the future versions. All configurations celeborn.ha.master.* should migrate to celeborn.master.ha.* . Since 0.3.0, environment variables CELEBORN_MASTER_HOST and CELEBORN_MASTER_PORT are removed. Instead CELEBORN_LOCAL_HOSTNAME works on both master and worker, which takes high priority than configurations defined in properties file. Since 0.3.0, the Celeborn Master URL schema is changed from rss:// to celeborn:// , for users who start Worker by sbin/start-worker.sh rss://<master-host>:<master-port> , should migrate to sbin/start-worker.sh celeborn://<master-host>:<master-port> . Since 0.3.0, Celeborn supports overriding Hadoop configuration( core-site.xml , hdfs-site.xml , etc.) from Celeborn configuration with the additional prefix celeborn.hadoop. . On Spark client side, user should set Hadoop configuration like spark.celeborn.hadoop.foo=bar , note that spark.hadoop.foo=bar does not take effect; on Flink client and Celeborn Master/Worker side, user should set like celeborn.hadoop.foo=bar . Since 0.3.0, Celeborn master metrics BlacklistedWorkerCount is renamed as ExcludedWorkerCount . Since 0.3.0, Celeborn master http request url /blacklistedWorkers is renamed as /excludedWorkers . Since 0.3.0, introduces a terminology update for Celeborn worker data replication, replacing the previous master/slave terminology with primary/replica . In alignment with this change, corresponding metrics keywords have been adjusted. The following table presents a comprehensive overview of the changes: Key Before v0.3.0 Key After v0.3.0 MasterPushDataTime PrimaryPushDataTime MasterPushDataHandshakeTime PrimaryPushDataHandshakeTime MasterRegionStartTime PrimaryRegionStartTime MasterRegionFinishTime PrimaryRegionFinishTime SlavePushDataTime ReplicaPushDataTime SlavePushDataHandshakeTime ReplicaPushDataHandshakeTime SlaveRegionStartTime ReplicaRegionStartTime SlaveRegionFinishTime ReplicaRegionFinishTime Since 0.3.0, Celeborn's spark shuffle manager change from org.apache.spark.shuffle.celeborn.RssShuffleManager to org.apache.spark.shuffle.celeborn.SparkShuffleManager . User can set spark property spark.shuffle.manager to org.apache.spark.shuffle.celeborn.SparkShuffleManager to use Celeborn remote shuffle service. In 0.3.0, Celeborn still support org.apache.spark.shuffle.celeborn.RssShuffleManager , it will be removed in 0.4.0.","title":"Upgrading from 0.2 to 0.3"},{"location":"monitoring/","text":"Monitoring There are two ways to monitor Celeborn cluster: Prometheus metrics and REST API. Metrics Celeborn has a configurable metrics system based on the Dropwizard Metrics Library . This allows users to report Celeborn metrics to a variety of sinks including HTTP, JMX, CSV files and prometheus servlet. The metrics are generated by sources embedded in the Celeborn code base. They provide instrumentation for specific activities and Celeborn components. The metrics system is configured via a configuration file that Celeborn expects to be present at $CELEBORN_HOME/conf/metrics.properties . A custom file location can be specified via the spark.metrics.conf configuration property . Instead of using the configuration file, a set of configuration parameters with prefix celeborn.metrics.conf. can be used. Celeborn's metrics are divided into two instances corresponding to Celeborn components. The following instances are currently supported: master : The Celeborn cluster master process. worker : The Celeborn cluster worker process. Each instance can report to zero or more sinks . Sinks are contained in the org.apache.celeborn.common.metrics.sink package: CSVSink : Exports metrics data to CSV files at regular intervals. PrometheusServlet : Adds a servlet within the existing Celeborn REST API to serve metrics data in Prometheus format. GraphiteSink : Sends metrics to a Graphite node. The syntax of the metrics configuration file and the parameters available for each sink are defined in an example configuration file, $CELEBORN_HOME/conf/metrics.properties.template . When using Celeborn configuration parameters instead of the metrics configuration file, the relevant parameter names are composed by the prefix celeborn.metrics.conf. followed by the configuration details, i.e. the parameters take the following form: celeborn.metrics.conf.[instance|*].sink.[sink_name].[parameter_name] . This example shows a list of Celeborn configuration parameters for a CSV sink: \"celeborn.metrics.conf.*.sink.csv.class\"=\"org.apache.celeborn.common.metrics.sink.GraphiteSink\" \"celeborn.metrics.conf.*.sink.csv.period\"=\"1\" \"celeborn.metrics.conf.*.sink.csv.unit\"=minutes \"celeborn.metrics.conf.*.sink.csv.directory\"=/tmp/ Default values of the Celeborn metrics configuration are as follows: *.sink.prometheusServlet.class=org.apache.celeborn.common.metrics.sink.PrometheusServlet Additional sources can be configured using the metrics configuration file or the configuration parameter spark.metrics.conf.[component_name].source.jvm.class=[source_name] . At present the no source is the available optional source. For example the following configuration parameter activates the Example source: \"celeborn.metrics.conf.*.source.jvm.class\"=\"org.apache.celeborn.common.metrics.source.ExampleSource\" Available metrics providers Metrics used by Celeborn are of multiple types: gauge, counter, histogram, meter and timer, see Dropwizard library documentation for details . The following list of components and metrics reports the name and some details about the available metrics, grouped per component instance and source namespace. The most common time of metrics used in Celeborn instrumentation are gauges and counters. Counters can be recognized as they have the .count suffix. Timers, meters and histograms are annotated in the list, the rest of the list elements are metrics of type gauge. The large majority of metrics are active as soon as their parent component instance is configured, some metrics require also to be enabled via an additional configuration parameter, the details are reported in the list. Master These metrics are exposed by Celeborn master. namespace=master WorkerCount LostWorkers ExcludedWorkerCount RegisteredShuffleCount RunningApplicationCount IsActiveMaster PartitionSize The size of estimated shuffle partition. PartitionWritten The active shuffle size of workers. PartitionFileCount The active shuffle file count of workers. OfferSlotsTime The time for masters to handle RequestSlots request when registering shuffle. namespace=CPU JVMCPUTime namespace=system LastMinuteSystemLoad The average system load for the last minute. AvailableProcessors namespace=JVM This source provides information on JVM metrics using the Dropwizard/Codahale Metric Sets for JVM instrumentation and in particular the metric sets BufferPoolMetricSet, GarbageCollectorMetricSet and MemoryUsageGaugeSet. namespace=ResourceConsumption notes: This metrics data is generated for each user and they are identified using a metric tag. diskFileCount diskBytesWritten hdfsFileCount hdfsBytesWritten Worker These metrics are exposed by Celeborn worker. namespace=worker CommitFilesTime The time for a worker to flush buffers and close files related to specified shuffle. ReserveSlotsTime FlushDataTime The time for a worker to write a buffer which is 256KB by default to storage. OpenStreamTime The time for a worker to process openStream RPC and return StreamHandle. FetchChunkTime The time for a worker to fetch a chunk which is 8MB by default from a reduced partition. PrimaryPushDataTime The time for a worker to handle a pushData RPC sent from a celeborn client. ReplicaPushDataTime The time for a worker to handle a pushData RPC sent from a celeborn worker by replicating. WriteDataFailCount ReplicateDataFailCount ReplicateDataWriteFailCount ReplicateDataCreateConnectionFailCount ReplicateDataConnectionExceptionCount ReplicateDataTimeoutCount PushDataHandshakeFailCount RegionStartFailCount RegionFinishFailCount PrimaryPushDataHandshakeTime ReplicaPushDataHandshakeTime PrimaryRegionStartTime ReplicaRegionStartTime PrimaryRegionFinishTime ReplicaRegionFinishTime TakeBufferTime The time for a worker to take out a buffer from a disk flusher. RegisteredShuffleCount SlotsAllocated NettyMemory The total amount of off-heap memory used by celeborn worker. SortTime The time for a worker to sort a shuffle file. SortMemory The memory used by sorting shuffle files. SortingFiles SortedFiles SortedFileSize DiskBuffer The memory occupied by pushData and pushMergedData which should be written to disk. PausePushData The count for a worker to stop receiving pushData from clients because of back pressure. PausePushDataAndReplicate The count for a worker to stop receiving pushData from clients and other workers because of back pressure. BufferStreamReadBuffer The memory used by credit stream read buffer. ReadBufferDispatcherRequestsLength The queue size of read buffer allocation requests. ReadBufferAllocatedCount Allocated read buffer count. CreditStreamCount Stream count for map partition reading streams. ActiveMapPartitionCount DeviceOSFreeBytes DeviceOSTotalBytes DeviceCelebornFreeBytes DeviceCelebornTotalBytes PotentialConsumeSpeed UserProduceSpeed WorkerConsumeSpeed ActiveShuffleSize The active shuffle size of a worker including master replica and slave replica. ActiveShuffleFileCount The active shuffle file count of a worker including master replica and slave replica. push_server_usedHeapMemory push_server_usedDirectMemory push_server_numAllocations push_server_numTinyAllocations push_server_numSmallAllocations push_server_numNormalAllocations push_server_numHugeAllocations push_server_numDeallocations push_server_numTinyDeallocations push_server_numSmallDeallocations push_server_numNormalDeallocations push_server_numHugeDeallocations push_server_numActiveAllocations push_server_numActiveTinyAllocations push_server_numActiveSmallAllocations push_server_numActiveNormalAllocations push_server_numActiveHugeAllocations push_server_numActiveBytes replicate_server_usedHeapMemory replicate_server_usedDirectMemory replicate_server_numAllocations replicate_server_numTinyAllocations replicate_server_numSmallAllocations replicate_server_numNormalAllocations replicate_server_numHugeAllocations replicate_server_numDeallocations replicate_server_numTinyDeallocations replicate_server_numSmallDeallocations replicate_server_numNormalDeallocations replicate_server_numHugeDeallocations replicate_server_numActiveAllocations replicate_server_numActiveTinyAllocations replicate_server_numActiveSmallAllocations replicate_server_numActiveNormalAllocations replicate_server_numActiveHugeAllocations replicate_server_numActiveBytes fetch_server_usedHeapMemory fetch_server_usedDirectMemory fetch_server_numAllocations fetch_server_numTinyAllocations fetch_server_numSmallAllocations fetch_server_numNormalAllocations fetch_server_numHugeAllocations fetch_server_numDeallocations fetch_server_numTinyDeallocations fetch_server_numSmallDeallocations fetch_server_numNormalDeallocations fetch_server_numHugeDeallocations fetch_server_numActiveAllocations fetch_server_numActiveTinyAllocations fetch_server_numActiveSmallAllocations fetch_server_numActiveNormalAllocations fetch_server_numActiveHugeAllocations fetch_server_numActiveBytes namespace=CPU JVMCPUTime namespace=system LastMinuteSystemLoad Returns the system load average for the last minute. AvailableProcessors namespace=JVM This source provides information on JVM metrics using the Dropwizard/Codahale Metric Sets for JVM instrumentation and in particular the metric sets BufferPoolMetricSet, GarbageCollectorMetricSet and MemoryUsageGaugeSet. Note: The Netty DirectArenaMetrics named like push/fetch/replicate_server_numXX are not exposed by default, nor in Grafana dashboard. If there is a need, you can enable celeborn.network.memory.allocator.verbose.metric to expose these metrics. REST API In addition to viewing the metrics, Celeborn also support REST API. This gives developers an easy way to create new visualizations and monitoring tools for Celeborn and also easy for users to get the running status of the service. The REST API is available for both master and worker. The endpoints are mounted at host:port . For example, for the master, they would typically be accessible at http://<master-prometheus-host>:<master-prometheus-port><path> , and for the worker, at http://<worker-prometheus-host>:<worker-prometheus-port><path> . The configuration of <master-prometheus-host> , <master-prometheus-port> , <worker-prometheus-host> , <worker-prometheus-port> as below: Key Default Description Since celeborn.metrics.master.prometheus.host 0.0.0.0 Master's Prometheus host. 0.2.0 celeborn.metrics.master.prometheus.port 9098 Master's Prometheus port. 0.2.0 celeborn.metrics.worker.prometheus.host 0.0.0.0 Worker's Prometheus host. 0.2.0 celeborn.metrics.worker.prometheus.port 9096 Worker's Prometheus port. 0.2.0 Available API providers API path listed as below: Master Path Meaning /metrics/prometheus List the metrics data in prometheus format of the master. /conf List the conf setting of the master. /workerInfo List worker information of the service. It will list all registered workers 's information. /lostWorkers List all lost workers of the master. /excludedWorkers List all excluded workers of the master. /shutdownWorkers List all shutdown workers of the master. /threadDump List the current thread dump of the master. /hostnames List all running application's LifecycleManager's hostnames of the cluster. /applications List all running application's ids of the cluster. /shuffles List all running shuffle keys of the service. It will return all running shuffle's key of the cluster. /listTopDiskUsedApps List the top disk usage application ids. It will return the top disk usage application ids for the cluster. Worker Path Meaning /metrics/prometheus List the metrics data in prometheus format of the worker. /conf List the conf setting of the worker. /workerInfo List the worker information of the worker. /threadDump List the current thread dump of the worker. /shuffles List all the running shuffle keys of the worker. It only return keys of shuffles running in that worker. /listTopDiskUsedApps List the top disk usage application ids. It only return application ids running in that worker. /listPartitionLocationInfo List all the living PartitionLocation information in that worker. /unavailablePeers List the unavailable peers of the worker, this always means the worker connect to the peer failed. /isShutdown Show if the worker is during the process of shutdown. /isRegistered Show if the worker is registered to the master success. /exit?type=${TYPE} Trigger this worker to exit. Legal type s are 'DECOMMISSION\u2018, 'GRACEFUL' and 'IMMEDIATELY'","title":"Monitoring"},{"location":"monitoring/#monitoring","text":"There are two ways to monitor Celeborn cluster: Prometheus metrics and REST API.","title":"Monitoring"},{"location":"monitoring/#metrics","text":"Celeborn has a configurable metrics system based on the Dropwizard Metrics Library . This allows users to report Celeborn metrics to a variety of sinks including HTTP, JMX, CSV files and prometheus servlet. The metrics are generated by sources embedded in the Celeborn code base. They provide instrumentation for specific activities and Celeborn components. The metrics system is configured via a configuration file that Celeborn expects to be present at $CELEBORN_HOME/conf/metrics.properties . A custom file location can be specified via the spark.metrics.conf configuration property . Instead of using the configuration file, a set of configuration parameters with prefix celeborn.metrics.conf. can be used. Celeborn's metrics are divided into two instances corresponding to Celeborn components. The following instances are currently supported: master : The Celeborn cluster master process. worker : The Celeborn cluster worker process. Each instance can report to zero or more sinks . Sinks are contained in the org.apache.celeborn.common.metrics.sink package: CSVSink : Exports metrics data to CSV files at regular intervals. PrometheusServlet : Adds a servlet within the existing Celeborn REST API to serve metrics data in Prometheus format. GraphiteSink : Sends metrics to a Graphite node. The syntax of the metrics configuration file and the parameters available for each sink are defined in an example configuration file, $CELEBORN_HOME/conf/metrics.properties.template . When using Celeborn configuration parameters instead of the metrics configuration file, the relevant parameter names are composed by the prefix celeborn.metrics.conf. followed by the configuration details, i.e. the parameters take the following form: celeborn.metrics.conf.[instance|*].sink.[sink_name].[parameter_name] . This example shows a list of Celeborn configuration parameters for a CSV sink: \"celeborn.metrics.conf.*.sink.csv.class\"=\"org.apache.celeborn.common.metrics.sink.GraphiteSink\" \"celeborn.metrics.conf.*.sink.csv.period\"=\"1\" \"celeborn.metrics.conf.*.sink.csv.unit\"=minutes \"celeborn.metrics.conf.*.sink.csv.directory\"=/tmp/ Default values of the Celeborn metrics configuration are as follows: *.sink.prometheusServlet.class=org.apache.celeborn.common.metrics.sink.PrometheusServlet Additional sources can be configured using the metrics configuration file or the configuration parameter spark.metrics.conf.[component_name].source.jvm.class=[source_name] . At present the no source is the available optional source. For example the following configuration parameter activates the Example source: \"celeborn.metrics.conf.*.source.jvm.class\"=\"org.apache.celeborn.common.metrics.source.ExampleSource\"","title":"Metrics"},{"location":"monitoring/#available-metrics-providers","text":"Metrics used by Celeborn are of multiple types: gauge, counter, histogram, meter and timer, see Dropwizard library documentation for details . The following list of components and metrics reports the name and some details about the available metrics, grouped per component instance and source namespace. The most common time of metrics used in Celeborn instrumentation are gauges and counters. Counters can be recognized as they have the .count suffix. Timers, meters and histograms are annotated in the list, the rest of the list elements are metrics of type gauge. The large majority of metrics are active as soon as their parent component instance is configured, some metrics require also to be enabled via an additional configuration parameter, the details are reported in the list.","title":"Available metrics providers"},{"location":"monitoring/#master","text":"These metrics are exposed by Celeborn master. namespace=master WorkerCount LostWorkers ExcludedWorkerCount RegisteredShuffleCount RunningApplicationCount IsActiveMaster PartitionSize The size of estimated shuffle partition. PartitionWritten The active shuffle size of workers. PartitionFileCount The active shuffle file count of workers. OfferSlotsTime The time for masters to handle RequestSlots request when registering shuffle. namespace=CPU JVMCPUTime namespace=system LastMinuteSystemLoad The average system load for the last minute. AvailableProcessors namespace=JVM This source provides information on JVM metrics using the Dropwizard/Codahale Metric Sets for JVM instrumentation and in particular the metric sets BufferPoolMetricSet, GarbageCollectorMetricSet and MemoryUsageGaugeSet. namespace=ResourceConsumption notes: This metrics data is generated for each user and they are identified using a metric tag. diskFileCount diskBytesWritten hdfsFileCount hdfsBytesWritten","title":"Master"},{"location":"monitoring/#worker","text":"These metrics are exposed by Celeborn worker. namespace=worker CommitFilesTime The time for a worker to flush buffers and close files related to specified shuffle. ReserveSlotsTime FlushDataTime The time for a worker to write a buffer which is 256KB by default to storage. OpenStreamTime The time for a worker to process openStream RPC and return StreamHandle. FetchChunkTime The time for a worker to fetch a chunk which is 8MB by default from a reduced partition. PrimaryPushDataTime The time for a worker to handle a pushData RPC sent from a celeborn client. ReplicaPushDataTime The time for a worker to handle a pushData RPC sent from a celeborn worker by replicating. WriteDataFailCount ReplicateDataFailCount ReplicateDataWriteFailCount ReplicateDataCreateConnectionFailCount ReplicateDataConnectionExceptionCount ReplicateDataTimeoutCount PushDataHandshakeFailCount RegionStartFailCount RegionFinishFailCount PrimaryPushDataHandshakeTime ReplicaPushDataHandshakeTime PrimaryRegionStartTime ReplicaRegionStartTime PrimaryRegionFinishTime ReplicaRegionFinishTime TakeBufferTime The time for a worker to take out a buffer from a disk flusher. RegisteredShuffleCount SlotsAllocated NettyMemory The total amount of off-heap memory used by celeborn worker. SortTime The time for a worker to sort a shuffle file. SortMemory The memory used by sorting shuffle files. SortingFiles SortedFiles SortedFileSize DiskBuffer The memory occupied by pushData and pushMergedData which should be written to disk. PausePushData The count for a worker to stop receiving pushData from clients because of back pressure. PausePushDataAndReplicate The count for a worker to stop receiving pushData from clients and other workers because of back pressure. BufferStreamReadBuffer The memory used by credit stream read buffer. ReadBufferDispatcherRequestsLength The queue size of read buffer allocation requests. ReadBufferAllocatedCount Allocated read buffer count. CreditStreamCount Stream count for map partition reading streams. ActiveMapPartitionCount DeviceOSFreeBytes DeviceOSTotalBytes DeviceCelebornFreeBytes DeviceCelebornTotalBytes PotentialConsumeSpeed UserProduceSpeed WorkerConsumeSpeed ActiveShuffleSize The active shuffle size of a worker including master replica and slave replica. ActiveShuffleFileCount The active shuffle file count of a worker including master replica and slave replica. push_server_usedHeapMemory push_server_usedDirectMemory push_server_numAllocations push_server_numTinyAllocations push_server_numSmallAllocations push_server_numNormalAllocations push_server_numHugeAllocations push_server_numDeallocations push_server_numTinyDeallocations push_server_numSmallDeallocations push_server_numNormalDeallocations push_server_numHugeDeallocations push_server_numActiveAllocations push_server_numActiveTinyAllocations push_server_numActiveSmallAllocations push_server_numActiveNormalAllocations push_server_numActiveHugeAllocations push_server_numActiveBytes replicate_server_usedHeapMemory replicate_server_usedDirectMemory replicate_server_numAllocations replicate_server_numTinyAllocations replicate_server_numSmallAllocations replicate_server_numNormalAllocations replicate_server_numHugeAllocations replicate_server_numDeallocations replicate_server_numTinyDeallocations replicate_server_numSmallDeallocations replicate_server_numNormalDeallocations replicate_server_numHugeDeallocations replicate_server_numActiveAllocations replicate_server_numActiveTinyAllocations replicate_server_numActiveSmallAllocations replicate_server_numActiveNormalAllocations replicate_server_numActiveHugeAllocations replicate_server_numActiveBytes fetch_server_usedHeapMemory fetch_server_usedDirectMemory fetch_server_numAllocations fetch_server_numTinyAllocations fetch_server_numSmallAllocations fetch_server_numNormalAllocations fetch_server_numHugeAllocations fetch_server_numDeallocations fetch_server_numTinyDeallocations fetch_server_numSmallDeallocations fetch_server_numNormalDeallocations fetch_server_numHugeDeallocations fetch_server_numActiveAllocations fetch_server_numActiveTinyAllocations fetch_server_numActiveSmallAllocations fetch_server_numActiveNormalAllocations fetch_server_numActiveHugeAllocations fetch_server_numActiveBytes namespace=CPU JVMCPUTime namespace=system LastMinuteSystemLoad Returns the system load average for the last minute. AvailableProcessors namespace=JVM This source provides information on JVM metrics using the Dropwizard/Codahale Metric Sets for JVM instrumentation and in particular the metric sets BufferPoolMetricSet, GarbageCollectorMetricSet and MemoryUsageGaugeSet. Note: The Netty DirectArenaMetrics named like push/fetch/replicate_server_numXX are not exposed by default, nor in Grafana dashboard. If there is a need, you can enable celeborn.network.memory.allocator.verbose.metric to expose these metrics.","title":"Worker"},{"location":"monitoring/#rest-api","text":"In addition to viewing the metrics, Celeborn also support REST API. This gives developers an easy way to create new visualizations and monitoring tools for Celeborn and also easy for users to get the running status of the service. The REST API is available for both master and worker. The endpoints are mounted at host:port . For example, for the master, they would typically be accessible at http://<master-prometheus-host>:<master-prometheus-port><path> , and for the worker, at http://<worker-prometheus-host>:<worker-prometheus-port><path> . The configuration of <master-prometheus-host> , <master-prometheus-port> , <worker-prometheus-host> , <worker-prometheus-port> as below: Key Default Description Since celeborn.metrics.master.prometheus.host 0.0.0.0 Master's Prometheus host. 0.2.0 celeborn.metrics.master.prometheus.port 9098 Master's Prometheus port. 0.2.0 celeborn.metrics.worker.prometheus.host 0.0.0.0 Worker's Prometheus host. 0.2.0 celeborn.metrics.worker.prometheus.port 9096 Worker's Prometheus port. 0.2.0","title":"REST API"},{"location":"monitoring/#available-api-providers","text":"API path listed as below:","title":"Available API providers"},{"location":"monitoring/#master_1","text":"Path Meaning /metrics/prometheus List the metrics data in prometheus format of the master. /conf List the conf setting of the master. /workerInfo List worker information of the service. It will list all registered workers 's information. /lostWorkers List all lost workers of the master. /excludedWorkers List all excluded workers of the master. /shutdownWorkers List all shutdown workers of the master. /threadDump List the current thread dump of the master. /hostnames List all running application's LifecycleManager's hostnames of the cluster. /applications List all running application's ids of the cluster. /shuffles List all running shuffle keys of the service. It will return all running shuffle's key of the cluster. /listTopDiskUsedApps List the top disk usage application ids. It will return the top disk usage application ids for the cluster.","title":"Master"},{"location":"monitoring/#worker_1","text":"Path Meaning /metrics/prometheus List the metrics data in prometheus format of the worker. /conf List the conf setting of the worker. /workerInfo List the worker information of the worker. /threadDump List the current thread dump of the worker. /shuffles List all the running shuffle keys of the worker. It only return keys of shuffles running in that worker. /listTopDiskUsedApps List the top disk usage application ids. It only return application ids running in that worker. /listPartitionLocationInfo List all the living PartitionLocation information in that worker. /unavailablePeers List the unavailable peers of the worker, this always means the worker connect to the peer failed. /isShutdown Show if the worker is during the process of shutdown. /isRegistered Show if the worker is registered to the master success. /exit?type=${TYPE} Trigger this worker to exit. Legal type s are 'DECOMMISSION\u2018, 'GRACEFUL' and 'IMMEDIATELY'","title":"Worker"},{"location":"upgrade/","text":"Upgrade Rolling upgrade It is necessary to support a fast rolling upgrade process for the Celeborn cluster. In order to achieve a fast and unaffected rolling upgrade process, Celeborn should support that the written file in the worker should be committed and support reading after the worker restarted. Celeborn have done the following mechanism to support rolling upgrade. Background Fixed fetch port and client retry In the shuffle reduce side, the read client will obtain the worker's host/port and information of the file to be read. In order to ensure that the data can be read normally after the rolling restart process of the worker is completed, the worker needs to use a fixed fetch service port, the configuration is celeborn.worker.fetch.port , the default value is 0 . At startup, it will automatically select a free port, user need to set a fixed value, such as 9092 . At the same time, users need to adjust the number of retry times and retry wait time of the client according to cluster rolling restart situation to support the shuffle client to read data through retries after worker restarted. The shuffle client fetch data retry times configuration is celeborn.client.fetch.maxRetriesForEachReplica , default value is 3 . The shuffle client fetch data retry wait time configuration is celeborn.data.io.retryWait , default value is 5s . Users can increase the configuration value appropriately according to the situation. Worker store file meta information Shuffle client records the shuffle partition location's host, service port, and filename, to support workers recovering reading existing shuffle data after worker restart, during worker shutdown, workers should store the meta about reading shuffle partition files in LevelDB, and restore the meta after restarting workers. Users should set celeborn.worker.graceful.shutdown.enabled to true to enable graceful shutdown. During this process, worker will wait all allocated partition's in this worker to be committed within a timeout of celeborn.worker.graceful.shutdown.checkSlotsFinished.timeout , which default value is 480s . Then worker will wait for partition sorter finish all sort task within a timeout of celeborn.worker.graceful.shutdown.partitionSorter.shutdownTimeout , which default value is 120s . The whole graceful shutdown process should be finished within a timeout of celeborn.worker.graceful.shutdown.timeout , which default value is 600s . Allocated partition do hard split and Pre-commit hard split partition As mentioned in the previous section that the worker needs to wait for all allocated partition files to be committed during the restart process, which means that the worker need to wait for all the shuffle running on this worker to finish running before restarting the worker, otherwise part of the information will be lost, and abnormal partition files are left, and reading cannot be resumed. In order to speed up the restart process, worker let all push data requests return the HARD_SPLIT flag during worker shutdown, and shuffle client will re-apply for a new partition location for these allocated partitions. Then client side can record all HARD_SPLIT partition information and pre-commit these partition, then the worker side allocated partitions can be committed in a very short time. User should enable celeborn.client.shuffle.batchHandleCommitPartition.enabled , the default value is false. Example setting Worker Key Value celeborn.worker.graceful.shutdown.enabled true celeborn.worker.graceful.shutdown.checkSlotsFinished.timeout 480s celeborn.worker.graceful.shutdown.partitionSorter.shutdownTimeout 120s celeborn.worker.graceful.shutdown.timeout 600s celeborn.worker.fetch.port 9092 Client Key Value spark.celeborn.client.shuffle.batchHandleCommitPartition.enabled true spark.celeborn.client.fetch.maxRetriesForEachReplica 5 spark.celeborn.data.io.retryWait 10s","title":"Upgrade"},{"location":"upgrade/#upgrade","text":"","title":"Upgrade"},{"location":"upgrade/#rolling-upgrade","text":"It is necessary to support a fast rolling upgrade process for the Celeborn cluster. In order to achieve a fast and unaffected rolling upgrade process, Celeborn should support that the written file in the worker should be committed and support reading after the worker restarted. Celeborn have done the following mechanism to support rolling upgrade.","title":"Rolling upgrade"},{"location":"upgrade/#background","text":"Fixed fetch port and client retry In the shuffle reduce side, the read client will obtain the worker's host/port and information of the file to be read. In order to ensure that the data can be read normally after the rolling restart process of the worker is completed, the worker needs to use a fixed fetch service port, the configuration is celeborn.worker.fetch.port , the default value is 0 . At startup, it will automatically select a free port, user need to set a fixed value, such as 9092 . At the same time, users need to adjust the number of retry times and retry wait time of the client according to cluster rolling restart situation to support the shuffle client to read data through retries after worker restarted. The shuffle client fetch data retry times configuration is celeborn.client.fetch.maxRetriesForEachReplica , default value is 3 . The shuffle client fetch data retry wait time configuration is celeborn.data.io.retryWait , default value is 5s . Users can increase the configuration value appropriately according to the situation. Worker store file meta information Shuffle client records the shuffle partition location's host, service port, and filename, to support workers recovering reading existing shuffle data after worker restart, during worker shutdown, workers should store the meta about reading shuffle partition files in LevelDB, and restore the meta after restarting workers. Users should set celeborn.worker.graceful.shutdown.enabled to true to enable graceful shutdown. During this process, worker will wait all allocated partition's in this worker to be committed within a timeout of celeborn.worker.graceful.shutdown.checkSlotsFinished.timeout , which default value is 480s . Then worker will wait for partition sorter finish all sort task within a timeout of celeborn.worker.graceful.shutdown.partitionSorter.shutdownTimeout , which default value is 120s . The whole graceful shutdown process should be finished within a timeout of celeborn.worker.graceful.shutdown.timeout , which default value is 600s . Allocated partition do hard split and Pre-commit hard split partition As mentioned in the previous section that the worker needs to wait for all allocated partition files to be committed during the restart process, which means that the worker need to wait for all the shuffle running on this worker to finish running before restarting the worker, otherwise part of the information will be lost, and abnormal partition files are left, and reading cannot be resumed. In order to speed up the restart process, worker let all push data requests return the HARD_SPLIT flag during worker shutdown, and shuffle client will re-apply for a new partition location for these allocated partitions. Then client side can record all HARD_SPLIT partition information and pre-commit these partition, then the worker side allocated partitions can be committed in a very short time. User should enable celeborn.client.shuffle.batchHandleCommitPartition.enabled , the default value is false.","title":"Background"},{"location":"upgrade/#example-setting","text":"","title":"Example setting"},{"location":"upgrade/#worker","text":"Key Value celeborn.worker.graceful.shutdown.enabled true celeborn.worker.graceful.shutdown.checkSlotsFinished.timeout 480s celeborn.worker.graceful.shutdown.partitionSorter.shutdownTimeout 120s celeborn.worker.graceful.shutdown.timeout 600s celeborn.worker.fetch.port 9092","title":"Worker"},{"location":"upgrade/#client","text":"Key Value spark.celeborn.client.shuffle.batchHandleCommitPartition.enabled true spark.celeborn.client.fetch.maxRetriesForEachReplica 5 spark.celeborn.data.io.retryWait 10s","title":"Client"},{"location":"configuration/","text":"Configuration Guide This documentation contains Celeborn configuration details and a tuning guide. Important Configurations Environment Variables CELEBORN_WORKER_MEMORY=4g CELEBORN_WORKER_OFFHEAP_MEMORY=24g Celeborn workers tend to improve performance by using off-heap buffers. Off-heap memory requirement can be estimated as below: numDirs = `celeborn.worker.storage.dirs` # the amount of directory will be used by Celeborn storage bufferSize = `celeborn.worker.flusher.buffer.size` # the amount of memory will be used by a single flush buffer off-heap-memory = bufferSize * estimatedTasks * 2 + network memory For example, if a Celeborn worker has 10 storage directories or disks and the buffer size is set to 256 KiB. The necessary off-heap memory is 10 GiB. Network memory will be consumed when netty reads from a TCP channel, there will need some extra memory. Empirically, Celeborn worker off-heap memory should be set to (numDirs * bufferSize * 1.2) . All Configurations Master Key Default Description Since celeborn.master.estimatedPartitionSize.initialSize 64mb Initial partition size for estimation, it will change according to runtime stats. 0.3.0 celeborn.master.estimatedPartitionSize.update.initialDelay 5min Initial delay time before start updating partition size for estimation. 0.3.0 celeborn.master.estimatedPartitionSize.update.interval 10min Interval of updating partition size for estimation. 0.3.0 celeborn.master.hdfs.expireDirs.timeout 1h The timeout for a expire dirs to be deleted on HDFS. 0.3.0 celeborn.master.heartbeat.application.timeout 300s Application heartbeat timeout. 0.3.0 celeborn.master.heartbeat.worker.timeout 120s Worker heartbeat timeout. 0.3.0 celeborn.master.host <localhost> Hostname for master to bind. 0.2.0 celeborn.master.port 9097 Port for master to bind. 0.2.0 celeborn.master.slot.assign.extraSlots 2 Extra slots number when master assign slots. 0.3.0 celeborn.master.slot.assign.loadAware.diskGroupGradient 0.1 This value means how many more workload will be placed into a faster disk group than a slower group. 0.3.0 celeborn.master.slot.assign.loadAware.fetchTimeWeight 1.0 Weight of average fetch time when calculating ordering in load-aware assignment strategy 0.3.0 celeborn.master.slot.assign.loadAware.flushTimeWeight 0.0 Weight of average flush time when calculating ordering in load-aware assignment strategy 0.3.0 celeborn.master.slot.assign.loadAware.numDiskGroups 5 This configuration is a guidance for load-aware slot allocation algorithm. This value is control how many disk groups will be created. 0.3.0 celeborn.master.slot.assign.maxWorkers 10000 Max workers that slots of one shuffle can be allocated on. Will choose the smaller positive one from Master side and Client side, see celeborn.client.slot.assign.maxWorkers . 0.3.1 celeborn.master.slot.assign.policy ROUNDROBIN Policy for master to assign slots, Celeborn supports two types of policy: roundrobin and loadaware. Loadaware policy will be ignored when HDFS is enabled in celeborn.storage.activeTypes 0.3.0 celeborn.master.userResourceConsumption.update.interval 30s Time length for a window about compute user resource consumption. 0.3.0 celeborn.master.workerUnavailableInfo.expireTimeout 1800s Worker unavailable info would be cleared when the retention period is expired 0.3.1 celeborn.storage.availableTypes HDD Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.hdfs.dir <undefined> HDFS base directory for Celeborn to store shuffle data. 0.2.0 celeborn.storage.hdfs.kerberos.keytab <undefined> Kerberos keytab file path for HDFS storage connection. 0.3.2 celeborn.storage.hdfs.kerberos.principal <undefined> Kerberos principal for HDFS storage connection. 0.3.2 Apart from these, the following properties are also available for enable master HA: Master HA Key Default Description Since celeborn.master.ha.enabled false When true, master nodes run as Raft cluster mode. 0.3.0 celeborn.master.ha.node.<id>.host <required> Host to bind of master node in HA mode. 0.3.0 celeborn.master.ha.node.<id>.port 9097 Port to bind of master node in HA mode. 0.3.0 celeborn.master.ha.node.<id>.ratis.port 9872 Ratis port to bind of master node in HA mode. 0.3.0 celeborn.master.ha.ratis.raft.rpc.type netty RPC type for Ratis, available options: netty, grpc. 0.3.0 celeborn.master.ha.ratis.raft.server.storage.dir /tmp/ratis 0.3.0 Worker Key Default Description Since celeborn.master.endpoints <localhost>:9097 Endpoints of master nodes for celeborn client to connect, allowed pattern is: <host1>:<port1>[,<host2>:<port2>]* , e.g. clb1:9097,clb2:9098,clb3:9099 . If the port is omitted, 9097 will be used. 0.2.0 celeborn.master.estimatedPartitionSize.minSize 8mb Ignore partition size smaller than this configuration of partition size for estimation. 0.3.0 celeborn.shuffle.chunk.size 8m Max chunk size of reducer's merged shuffle data. For example, if a reducer's shuffle data is 128M and the data will need 16 fetch chunk requests to fetch. 0.2.0 celeborn.storage.availableTypes HDD Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.hdfs.dir <undefined> HDFS base directory for Celeborn to store shuffle data. 0.2.0 celeborn.storage.hdfs.kerberos.keytab <undefined> Kerberos keytab file path for HDFS storage connection. 0.3.2 celeborn.storage.hdfs.kerberos.principal <undefined> Kerberos principal for HDFS storage connection. 0.3.2 celeborn.worker.activeConnection.max <undefined> If the number of active connections on a worker exceeds this configuration value, the worker will be marked as high-load in the heartbeat report, and the master will not include that node in the response of RequestSlots. 0.3.1 celeborn.worker.bufferStream.threadsPerMountpoint 8 Threads count for read buffer per mount point. 0.3.0 celeborn.worker.clean.threads 64 Thread number of worker to clean up expired shuffle keys. 0.3.2 celeborn.worker.closeIdleConnections false Whether worker will close idle connections. 0.2.0 celeborn.worker.commitFiles.threads 32 Thread number of worker to commit shuffle data files asynchronously. It's recommended to set at least 128 when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.commitFiles.timeout 120s Timeout for a Celeborn worker to commit files of a shuffle. It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.congestionControl.check.interval 10ms Interval of worker checks congestion if celeborn.worker.congestionControl.enabled is true. 0.3.2 celeborn.worker.congestionControl.enabled false Whether to enable congestion control or not. 0.3.0 celeborn.worker.congestionControl.high.watermark <undefined> If the total bytes in disk buffer exceeds this configure, will start to congestusers whose produce rate is higher than the potential average consume rate. The congestion will stop if the produce rate is lower or equal to the average consume rate, or the total pending bytes lower than celeborn.worker.congestionControl.low.watermark 0.3.0 celeborn.worker.congestionControl.low.watermark <undefined> Will stop congest users if the total pending bytes of disk buffer is lower than this configuration 0.3.0 celeborn.worker.congestionControl.sample.time.window 10s The worker holds a time sliding list to calculate users' produce/consume rate 0.3.0 celeborn.worker.congestionControl.user.inactive.interval 10min How long will consider this user is inactive if it doesn't send data 0.3.0 celeborn.worker.decommission.checkInterval 30s The wait interval of checking whether all the shuffle expired during worker decommission 0.4.0 celeborn.worker.decommission.forceExitTimeout 6h The wait time of waiting for all the shuffle expire during worker decommission. 0.4.0 celeborn.worker.directMemoryRatioForMemoryShuffleStorage 0.0 Max ratio of direct memory to store shuffle data 0.2.0 celeborn.worker.directMemoryRatioForReadBuffer 0.1 Max ratio of direct memory for read buffer 0.2.0 celeborn.worker.directMemoryRatioToPauseReceive 0.85 If direct memory usage reaches this limit, the worker will stop to receive data from Celeborn shuffle clients. 0.2.0 celeborn.worker.directMemoryRatioToPauseReplicate 0.95 If direct memory usage reaches this limit, the worker will stop to receive replication data from other workers. This value should be higher than celeborn.worker.directMemoryRatioToPauseReceive. 0.2.0 celeborn.worker.directMemoryRatioToResume 0.7 If direct memory usage is less than this limit, worker will resume. 0.2.0 celeborn.worker.disk.clean.threads 4 Thread number of worker to clean up directories of expired shuffle keys on disk. 0.3.2 celeborn.worker.fetch.heartbeat.enabled false enable the heartbeat from worker to client when fetching data 0.3.0 celeborn.worker.fetch.io.threads <undefined> Netty IO thread number of worker to handle client fetch data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.fetch.port 0 Server port for Worker to receive fetch data request from ShuffleClient. 0.2.0 celeborn.worker.flusher.buffer.size 256k Size of buffer used by a single flusher. 0.2.0 celeborn.worker.flusher.diskTime.slidingWindow.size 20 The size of sliding windows used to calculate statistics about flushed time and count. 0.3.0 celeborn.worker.flusher.hdd.threads 1 Flusher's thread count per disk used for write data to HDD disks. 0.2.0 celeborn.worker.flusher.hdfs.buffer.size 4m Size of buffer used by a HDFS flusher. 0.3.0 celeborn.worker.flusher.hdfs.threads 8 Flusher's thread count used for write data to HDFS. 0.2.0 celeborn.worker.flusher.shutdownTimeout 3s Timeout for a flusher to shutdown. 0.2.0 celeborn.worker.flusher.ssd.threads 16 Flusher's thread count per disk used for write data to SSD disks. 0.2.0 celeborn.worker.flusher.threads 16 Flusher's thread count per disk for unknown-type disks. 0.2.0 celeborn.worker.graceful.shutdown.checkSlotsFinished.interval 1s The wait interval of checking whether all released slots to be committed or destroyed during worker graceful shutdown 0.2.0 celeborn.worker.graceful.shutdown.checkSlotsFinished.timeout 480s The wait time of waiting for the released slots to be committed or destroyed during worker graceful shutdown. 0.2.0 celeborn.worker.graceful.shutdown.enabled false When true, during worker shutdown, the worker will wait for all released slots to be committed or destroyed. 0.2.0 celeborn.worker.graceful.shutdown.partitionSorter.shutdownTimeout 120s The wait time of waiting for sorting partition files during worker graceful shutdown. 0.2.0 celeborn.worker.graceful.shutdown.recoverPath <tmp>/recover The path to store levelDB. 0.2.0 celeborn.worker.graceful.shutdown.saveCommittedFileInfo.interval 5s Interval for a Celeborn worker to flush committed file infos into Level DB. 0.3.1 celeborn.worker.graceful.shutdown.saveCommittedFileInfo.sync false Whether to call sync method to save committed file infos into Level DB to handle OS crash. 0.3.1 celeborn.worker.graceful.shutdown.timeout 600s The worker's graceful shutdown timeout time. 0.2.0 celeborn.worker.monitor.disk.check.interval 30s Intervals between device monitor to check disk. 0.3.0 celeborn.worker.monitor.disk.check.timeout 30s Timeout time for worker check device status. 0.3.0 celeborn.worker.monitor.disk.checklist readwrite,diskusage Monitor type for disk, available items are: iohang, readwrite and diskusage. 0.2.0 celeborn.worker.monitor.disk.enabled true When true, worker will monitor device and report to master. 0.3.0 celeborn.worker.monitor.disk.notifyError.expireTimeout 10m The expire timeout of non-critical device error. Only notify critical error when the number of non-critical errors for a period of time exceeds threshold. 0.3.0 celeborn.worker.monitor.disk.notifyError.threshold 64 Device monitor will only notify critical error once the accumulated valid non-critical error number exceeding this threshold. 0.3.0 celeborn.worker.monitor.disk.sys.block.dir /sys/block The directory where linux file block information is stored. 0.2.0 celeborn.worker.monitor.memory.check.interval 10ms Interval of worker direct memory checking. 0.3.0 celeborn.worker.monitor.memory.report.interval 10s Interval of worker direct memory tracker reporting to log. 0.3.0 celeborn.worker.monitor.memory.trimChannelWaitInterval 1s Wait time after worker trigger channel to trim cache. 0.3.0 celeborn.worker.monitor.memory.trimFlushWaitInterval 1s Wait time after worker trigger StorageManger to flush data. 0.3.0 celeborn.worker.partition.initial.readBuffersMax 1024 Max number of initial read buffers 0.3.0 celeborn.worker.partition.initial.readBuffersMin 1 Min number of initial read buffers 0.3.0 celeborn.worker.partitionSorter.directMemoryRatioThreshold 0.1 Max ratio of partition sorter's memory for sorting, when reserved memory is higher than max partition sorter memory, partition sorter will stop sorting. 0.2.0 celeborn.worker.push.heartbeat.enabled false enable the heartbeat from worker to client when pushing data 0.3.0 celeborn.worker.push.io.threads <undefined> Netty IO thread number of worker to handle client push data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.push.port 0 Server port for Worker to receive push data request from ShuffleClient. 0.2.0 celeborn.worker.readBuffer.allocationWait 50ms The time to wait when buffer dispatcher can not allocate a buffer. 0.3.0 celeborn.worker.readBuffer.target.changeThreshold 1mb The target ratio for pre read memory usage. 0.3.0 celeborn.worker.readBuffer.target.ratio 0.9 The target ratio for read ahead buffer's memory usage. 0.3.0 celeborn.worker.readBuffer.target.updateInterval 100ms The interval for memory manager to calculate new read buffer's target memory. 0.3.0 celeborn.worker.readBuffer.toTriggerReadMin 32 Min buffers count for map data partition to trigger read. 0.3.0 celeborn.worker.register.timeout 180s Worker register timeout. 0.2.0 celeborn.worker.replicate.fastFail.duration 60s If a replicate request not replied during the duration, worker will mark the replicate data request as failed.It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes . 0.2.0 celeborn.worker.replicate.io.threads <undefined> Netty IO thread number of worker to replicate shuffle data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.replicate.port 0 Server port for Worker to receive replicate data request from other Workers. 0.2.0 celeborn.worker.replicate.randomConnection.enabled true Whether worker will create random connection to peer when replicate data. When false, worker tend to reuse the same cached TransportClient to a specific replicate worker; when true, worker tend to use different cached TransportClient. Netty will use the same thread to serve the same connection, so with more connections replicate server can leverage more netty threads 0.2.1 celeborn.worker.replicate.threads 64 Thread number of worker to replicate shuffle data. 0.2.0 celeborn.worker.rpc.port 0 Server port for Worker to receive RPC request. 0.2.0 celeborn.worker.shuffle.partitionSplit.enabled true enable the partition split on worker side 0.3.0 celeborn.worker.shuffle.partitionSplit.max 2g Specify the maximum partition size for splitting, and ensure that individual partition files are always smaller than this limit. 0.3.0 celeborn.worker.shuffle.partitionSplit.min 1m Min size for a partition to split 0.3.0 celeborn.worker.sortPartition.reservedMemoryPerPartition 1mb Reserved memory when sorting a shuffle file off-heap. 0.3.0 celeborn.worker.sortPartition.threads <undefined> PartitionSorter's thread counts. It's recommended to set at least 64 when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.sortPartition.timeout 220s Timeout for a shuffle file to sort. 0.3.0 celeborn.worker.storage.checkDirsEmpty.maxRetries 3 The number of retries for a worker to check if the working directory is cleaned up before registering with the master. 0.3.0 celeborn.worker.storage.checkDirsEmpty.timeout 1000ms The wait time per retry for a worker to check if the working directory is cleaned up before registering with the master. 0.3.0 celeborn.worker.storage.dirs <undefined> Directory list to store shuffle data. It's recommended to configure one directory on each disk. Storage size limit can be set for each directory. For the sake of performance, there should be no more than 2 flush threads on the same disk partition if you are using HDD, and should be 8 or more flush threads on the same disk partition if you are using SSD. For example: dir1[:capacity=][:disktype=][:flushthread=],dir2[:capacity=][:disktype=][:flushthread=] 0.2.0 celeborn.worker.storage.disk.reserve.ratio <undefined> Celeborn worker reserved ratio for each disk. The minimum usable size for each disk is the max space between the reserved space and the space calculate via reserved ratio. 0.3.2 celeborn.worker.storage.disk.reserve.size 5G Celeborn worker reserved space for each disk. 0.3.0 celeborn.worker.storage.expireDirs.timeout 1h The timeout for a expire dirs to be deleted on disk. 0.3.2 celeborn.worker.storage.workingDir celeborn-worker/shuffle_data Worker's working dir path name. 0.3.0 celeborn.worker.writer.close.timeout 120s Timeout for a file writer to close 0.2.0 celeborn.worker.writer.create.maxAttempts 3 Retry count for a file writer to create if its creation was failed. 0.2.0 Client Key Default Description Since celeborn.client.application.heartbeatInterval 10s Interval for client to send heartbeat message to master. 0.3.0 celeborn.client.application.unregister.enabled true When true, Celeborn client will inform celeborn master the application is already shutdown during client exit, this allows the cluster to release resources immediately, resulting in resource savings. 0.3.2 celeborn.client.closeIdleConnections true Whether client will close idle connections. 0.3.0 celeborn.client.commitFiles.ignoreExcludedWorker false When true, LifecycleManager will skip workers which are in the excluded list. 0.3.0 celeborn.client.eagerlyCreateInputStream.threads 32 Threads count for streamCreatorPool in CelebornShuffleReader. 0.3.1 celeborn.client.excludePeerWorkerOnFailure.enabled true When true, Celeborn will exclude partition's peer worker on failure when push data to replica failed. 0.3.0 celeborn.client.excludedWorker.expireTimeout 180s Timeout time for LifecycleManager to clear reserved excluded worker. Default to be 1.5 * celeborn.master.heartbeat.worker.timeout to cover worker heartbeat timeout check period 0.3.0 celeborn.client.fetch.dfsReadChunkSize 8m Max chunk size for DfsPartitionReader. 0.3.1 celeborn.client.fetch.excludeWorkerOnFailure.enabled false Whether to enable shuffle client-side fetch exclude workers on failure. 0.3.0 celeborn.client.fetch.excludedWorker.expireTimeout <value of celeborn.client.excludedWorker.expireTimeout> ShuffleClient is a static object, it will be used in the whole lifecycle of Executor,We give a expire time for excluded workers to avoid a transient worker issues. 0.3.0 celeborn.client.fetch.maxReqsInFlight 3 Amount of in-flight chunk fetch request. 0.3.0 celeborn.client.fetch.maxRetriesForEachReplica 3 Max retry times of fetch chunk on each replica 0.3.0 celeborn.client.fetch.timeout 600s Timeout for a task to open stream and fetch chunk. 0.3.0 celeborn.client.flink.compression.enabled true Whether to compress data in Flink plugin. 0.3.0 celeborn.client.flink.inputGate.concurrentReadings 2147483647 Max concurrent reading channels for a input gate. 0.3.0 celeborn.client.flink.inputGate.memory 32m Memory reserved for a input gate. 0.3.0 celeborn.client.flink.inputGate.minMemory 8m Min memory reserved for a input gate. 0.3.0 celeborn.client.flink.inputGate.supportFloatingBuffer true Whether to support floating buffer in Flink input gates. 0.3.0 celeborn.client.flink.resultPartition.memory 64m Memory reserved for a result partition. 0.3.0 celeborn.client.flink.resultPartition.minMemory 8m Min memory reserved for a result partition. 0.3.0 celeborn.client.flink.resultPartition.supportFloatingBuffer true Whether to support floating buffer for result partitions. 0.3.0 celeborn.client.push.buffer.initial.size 8k 0.3.0 celeborn.client.push.buffer.max.size 64k Max size of reducer partition buffer memory for shuffle hash writer. The pushed data will be buffered in memory before sending to Celeborn worker. For performance consideration keep this buffer size higher than 32K. Example: If reducer amount is 2000, buffer size is 64K, then each task will consume up to 64KiB * 2000 = 125MiB heap memory. 0.3.0 celeborn.client.push.excludeWorkerOnFailure.enabled false Whether to enable shuffle client-side push exclude workers on failures. 0.3.0 celeborn.client.push.limit.inFlight.sleepInterval 50ms Sleep interval when check netty in-flight requests to be done. 0.3.0 celeborn.client.push.limit.inFlight.timeout <undefined> Timeout for netty in-flight requests to be done.Default value should be celeborn.client.push.timeout * 2 . 0.3.0 celeborn.client.push.limit.strategy SIMPLE The strategy used to control the push speed. Valid strategies are SIMPLE and SLOWSTART. The SLOWSTART strategy usually works with congestion control mechanism on the worker side. 0.3.0 celeborn.client.push.maxReqsInFlight.perWorker 32 Amount of Netty in-flight requests per worker. Default max memory of in flight requests per worker is celeborn.client.push.maxReqsInFlight.perWorker * celeborn.client.push.buffer.max.size * compression ratio(1 in worst case): 64KiB * 32 = 2MiB. The maximum memory will not exceed celeborn.client.push.maxReqsInFlight.total . 0.3.0 celeborn.client.push.maxReqsInFlight.total 256 Amount of total Netty in-flight requests. The maximum memory is celeborn.client.push.maxReqsInFlight.total * celeborn.client.push.buffer.max.size * compression ratio(1 in worst case): 64KiB * 256 = 16MiB 0.3.0 celeborn.client.push.queue.capacity 512 Push buffer queue size for a task. The maximum memory is celeborn.client.push.buffer.max.size * celeborn.client.push.queue.capacity , default: 64KiB * 512 = 32MiB 0.3.0 celeborn.client.push.replicate.enabled false When true, Celeborn worker will replicate shuffle data to another Celeborn worker asynchronously to ensure the pushed shuffle data won't be lost after the node failure. It's recommended to set false when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.client.push.retry.threads 8 Thread number to process shuffle re-send push data requests. 0.3.0 celeborn.client.push.revive.batchSize 2048 Max number of partitions in one Revive request. 0.3.0 celeborn.client.push.revive.interval 100ms Interval for client to trigger Revive to LifecycleManager. The number of partitions in one Revive request is celeborn.client.push.revive.batchSize . 0.3.0 celeborn.client.push.revive.maxRetries 5 Max retry times for reviving when celeborn push data failed. 0.3.0 celeborn.client.push.sendBufferPool.checkExpireInterval 30s Interval to check expire for send buffer pool. If the pool has been idle for more than celeborn.client.push.sendBufferPool.expireTimeout , the pooled send buffers and push tasks will be cleaned up. 0.3.1 celeborn.client.push.sendBufferPool.expireTimeout 60s Timeout before clean up SendBufferPool. If SendBufferPool is idle for more than this time, the send buffers and push tasks will be cleaned up. 0.3.1 celeborn.client.push.slowStart.initialSleepTime 500ms The initial sleep time if the current max in flight requests is 0 0.3.0 celeborn.client.push.slowStart.maxSleepTime 2s If celeborn.client.push.limit.strategy is set to SLOWSTART, push side will take a sleep strategy for each batch of requests, this controls the max sleep time if the max in flight requests limit is 1 for a long time 0.3.0 celeborn.client.push.sort.randomizePartitionId.enabled false Whether to randomize partitionId in push sorter. If true, partitionId will be randomized when sort data to avoid skew when push to worker 0.3.0 celeborn.client.push.splitPartition.threads 8 Thread number to process shuffle split request in shuffle client. 0.3.0 celeborn.client.push.stageEnd.timeout <value of celeborn.<module>.io.connectionTimeout> Timeout for waiting StageEnd. During this process, there are celeborn.client.requestCommitFiles.maxRetries times for retry opportunities for committing files and 1 times for releasing slots request. User can customize this value according to your setting. By default, the value is the max timeout value celeborn.<module>.io.connectionTimeout . 0.3.0 celeborn.client.push.takeTaskMaxWaitAttempts 1 Max wait times if no task available to push to worker. 0.3.0 celeborn.client.push.takeTaskWaitInterval 50ms Wait interval if no task available to push to worker. 0.3.0 celeborn.client.push.timeout 120s Timeout for a task to push data rpc message. This value should better be more than twice of celeborn.<module>.push.timeoutCheck.interval 0.3.0 celeborn.client.readLocalShuffleFile.enabled false Enable read local shuffle file for clusters that co-deployed with yarn node manager. 0.3.1 celeborn.client.readLocalShuffleFile.threads 4 Threads count for read local shuffle file. 0.3.1 celeborn.client.registerShuffle.maxRetries 3 Max retry times for client to register shuffle. 0.3.0 celeborn.client.registerShuffle.retryWait 3s Wait time before next retry if register shuffle failed. 0.3.0 celeborn.client.requestCommitFiles.maxRetries 4 Max retry times for requestCommitFiles RPC. 0.3.0 celeborn.client.reserveSlots.maxRetries 3 Max retry times for client to reserve slots. 0.3.0 celeborn.client.reserveSlots.rackaware.enabled false Whether need to place different replicates on different racks when allocating slots. 0.3.1 celeborn.client.reserveSlots.retryWait 3s Wait time before next retry if reserve slots failed. 0.3.0 celeborn.client.rpc.cache.concurrencyLevel 32 The number of write locks to update rpc cache. 0.3.0 celeborn.client.rpc.cache.expireTime 15s The time before a cache item is removed. 0.3.0 celeborn.client.rpc.cache.size 256 The max cache items count for rpc cache. 0.3.0 celeborn.client.rpc.getReducerFileGroup.askTimeout <value of celeborn.<module>.io.connectionTimeout> Timeout for ask operations during getting reducer file group information. During this process, there are celeborn.client.requestCommitFiles.maxRetries times for retry opportunities for committing files and 1 times for releasing slots request. User can customize this value according to your setting. By default, the value is the max timeout value celeborn.<module>.io.connectionTimeout . 0.2.0 celeborn.client.rpc.maxRetries 3 Max RPC retry times in LifecycleManager. 0.3.2 celeborn.client.rpc.registerShuffle.askTimeout <value of celeborn.<module>.io.connectionTimeout> Timeout for ask operations during register shuffle. During this process, there are two times for retry opportunities for requesting slots, one request for establishing a connection with Worker and celeborn.client.reserveSlots.maxRetries times for retry opportunities for reserving slots. User can customize this value according to your setting. By default, the value is the max timeout value celeborn.<module>.io.connectionTimeout . 0.3.0 celeborn.client.rpc.requestPartition.askTimeout <value of celeborn.<module>.io.connectionTimeout> Timeout for ask operations during requesting change partition location, such as reviving or splitting partition. During this process, there are celeborn.client.reserveSlots.maxRetries times for retry opportunities for reserving slots. User can customize this value according to your setting. By default, the value is the max timeout value celeborn.<module>.io.connectionTimeout . 0.2.0 celeborn.client.rpc.reserveSlots.askTimeout <value of celeborn.rpc.askTimeout> Timeout for LifecycleManager request reserve slots. 0.3.0 celeborn.client.rpc.shared.threads 16 Number of shared rpc threads in LifecycleManager. 0.3.2 celeborn.client.shuffle.batchHandleChangePartition.interval 100ms Interval for LifecycleManager to schedule handling change partition requests in batch. 0.3.0 celeborn.client.shuffle.batchHandleChangePartition.threads 8 Threads number for LifecycleManager to handle change partition request in batch. 0.3.0 celeborn.client.shuffle.batchHandleCommitPartition.interval 5s Interval for LifecycleManager to schedule handling commit partition requests in batch. 0.3.0 celeborn.client.shuffle.batchHandleCommitPartition.threads 8 Threads number for LifecycleManager to handle commit partition request in batch. 0.3.0 celeborn.client.shuffle.batchHandleReleasePartition.interval 5s Interval for LifecycleManager to schedule handling release partition requests in batch. 0.3.0 celeborn.client.shuffle.batchHandleReleasePartition.threads 8 Threads number for LifecycleManager to handle release partition request in batch. 0.3.0 celeborn.client.shuffle.compression.codec LZ4 The codec used to compress shuffle data. By default, Celeborn provides three codecs: lz4 , zstd , none . 0.3.0 celeborn.client.shuffle.compression.zstd.level 1 Compression level for Zstd compression codec, its value should be an integer between -5 and 22. Increasing the compression level will result in better compression at the expense of more CPU and memory. 0.3.0 celeborn.client.shuffle.expired.checkInterval 60s Interval for client to check expired shuffles. 0.3.0 celeborn.client.shuffle.manager.port 0 Port used by the LifecycleManager on the Driver. 0.3.0 celeborn.client.shuffle.mapPartition.split.enabled false whether to enable shuffle partition split. Currently, this only applies to MapPartition. 0.3.1 celeborn.client.shuffle.partition.type REDUCE Type of shuffle's partition. 0.3.0 celeborn.client.shuffle.partitionSplit.mode SOFT soft: the shuffle file size might be larger than split threshold. hard: the shuffle file size will be limited to split threshold. 0.3.0 celeborn.client.shuffle.partitionSplit.threshold 1G Shuffle file size threshold, if file size exceeds this, trigger split. 0.3.0 celeborn.client.shuffle.rangeReadFilter.enabled false If a spark application have skewed partition, this value can set to true to improve performance. 0.2.0 celeborn.client.slot.assign.maxWorkers 10000 Max workers that slots of one shuffle can be allocated on. Will choose the smaller positive one from Master side and Client side, see celeborn.master.slot.assign.maxWorkers . 0.3.1 celeborn.client.spark.push.sort.memory.threshold 64m When SortBasedPusher use memory over the threshold, will trigger push data. If the pipeline push feature is enabled ( celeborn.client.spark.push.sort.pipeline.enabled=true ), the SortBasedPusher will trigger a data push when the memory usage exceeds half of the threshold(by default, 32m). 0.3.0 celeborn.client.spark.push.sort.pipeline.enabled false Whether to enable pipelining for sort based shuffle writer. If true, double buffering will be used to pipeline push 0.3.0 celeborn.client.spark.push.unsafeRow.fastWrite.enabled true This is Celeborn's optimization on UnsafeRow for Spark and it's true by default. If you have changed UnsafeRow's memory layout set this to false. 0.2.2 celeborn.client.spark.shuffle.forceFallback.enabled false Whether force fallback shuffle to Spark's default. 0.3.0 celeborn.client.spark.shuffle.forceFallback.numPartitionsThreshold 500000 Celeborn will only accept shuffle of partition number lower than this configuration value. 0.3.0 celeborn.client.spark.shuffle.writer HASH Celeborn supports the following kind of shuffle writers. 1. hash: hash-based shuffle writer works fine when shuffle partition count is normal; 2. sort: sort-based shuffle writer works fine when memory pressure is high or shuffle partition count is huge. 0.3.0 celeborn.master.endpoints <localhost>:9097 Endpoints of master nodes for celeborn client to connect, allowed pattern is: <host1>:<port1>[,<host2>:<port2>]* , e.g. clb1:9097,clb2:9098,clb3:9099 . If the port is omitted, 9097 will be used. 0.2.0 celeborn.storage.availableTypes HDD Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.hdfs.dir <undefined> HDFS base directory for Celeborn to store shuffle data. 0.2.0 Quota Key Default Description Since celeborn.quota.configuration.path <undefined> Quota configuration file path. The file format should be yaml. Quota configuration file template can be found under conf directory. 0.2.0 celeborn.quota.enabled true When true, before registering shuffle, LifecycleManager should check if current user have enough quota space, if cluster don't have enough quota space for current user, fallback to Spark's default shuffle 0.2.0 celeborn.quota.identity.provider org.apache.celeborn.common.identity.DefaultIdentityProvider IdentityProvider class name. Default class is org.apache.celeborn.common.identity.DefaultIdentityProvider . Optional values: org.apache.celeborn.common.identity.HadoopBasedIdentityProvider user name will be obtained by UserGroupInformation.getUserName; org.apache.celeborn.common.identity.DefaultIdentityProvider user name and tenant id are default values or user-specific values. 0.2.0 celeborn.quota.identity.user-specific.tenant default Tenant id if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.quota.identity.user-specific.userName default User name if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.quota.manager org.apache.celeborn.common.quota.DefaultQuotaManager QuotaManger class name. Default class is org.apache.celeborn.common.quota.DefaultQuotaManager . 0.2.0 Network Key Default Description Since celeborn.<module>.fetch.timeoutCheck.interval 5s Interval for checking fetch data timeout. It only support setting to data since it works for shuffle client fetch data and should be configured on client side. 0.3.0 celeborn.<module>.fetch.timeoutCheck.threads 4 Threads num for checking fetch data timeout. It only support setting to data since it works for shuffle client fetch data and should be configured on client side. 0.3.0 celeborn.<module>.heartbeat.interval 60s The heartbeat interval between worker and client. If setting to data , it works for shuffle client push and fetch data and should be configured on client side. If setting to replicate , it works for worker replicate data to peer worker and should be configured on worker side. 0.3.0 celeborn.<module>.io.backLog 0 Requested maximum length of the queue of incoming connections. Default 0 for no backlog. celeborn.<module>.io.clientThreads 0 Number of threads used in the client thread pool. Default to 0, which is 2x#cores. celeborn.<module>.io.connectTimeout <value of celeborn.network.connect.timeout> Socket connect timeout. celeborn.<module>.io.connectionTimeout <value of celeborn.network.timeout> Connection active timeout. celeborn.<module>.io.enableVerboseMetrics false Whether to track Netty memory detailed metrics. If true, the detailed metrics of Netty PoolByteBufAllocator will be gotten, otherwise only general memory usage will be tracked. celeborn.<module>.io.lazyFD true Whether to initialize FileDescriptor lazily or not. If true, file descriptors are created only when data is going to be transferred. This can reduce the number of open files. celeborn.<module>.io.maxRetries 3 Max number of times we will try IO exceptions (such as connection timeouts) per request. If set to 0, we will not do any retries. celeborn.<module>.io.mode NIO Netty EventLoopGroup backend, available options: NIO, EPOLL. celeborn.<module>.io.numConnectionsPerPeer 2 Number of concurrent connections between two nodes. celeborn.<module>.io.preferDirectBufs true If true, we will prefer allocating off-heap byte buffers within Netty. celeborn.<module>.io.receiveBuffer 0b Receive buffer size (SO_RCVBUF). Note: the optimal size for receive buffer and send buffer should be latency * network_bandwidth. Assuming latency = 1ms, network_bandwidth = 10Gbps buffer size should be ~ 1.25MB. 0.2.0 celeborn.<module>.io.retryWait 5s Time that we will wait in order to perform a retry after an IOException. Only relevant if maxIORetries > 0. 0.2.0 celeborn.<module>.io.sendBuffer 0b Send buffer size (SO_SNDBUF). 0.2.0 celeborn.<module>.io.serverThreads 0 Number of threads used in the server thread pool. Default to 0, which is 2x#cores. celeborn.<module>.push.timeoutCheck.interval 5s Interval for checking push data timeout. If setting to data , it works for shuffle client push data and should be configured on client side. If setting to replicate , it works for worker replicate data to peer worker and should be configured on worker side. 0.3.0 celeborn.<module>.push.timeoutCheck.threads 4 Threads num for checking push data timeout. If setting to data , it works for shuffle client push data and should be configured on client side. If setting to replicate , it works for worker replicate data to peer worker and should be configured on worker side. 0.3.0 celeborn.<role>.rpc.dispatcher.threads <value of celeborn.rpc.dispatcher.threads> Threads number of message dispatcher event loop for roles celeborn.io.maxDefaultNettyThreads 64 Max default netty threads 0.3.2 celeborn.network.bind.preferIpAddress true When ture , prefer to use IP address, otherwise FQDN. This configuration only takes effects when the bind hostname is not set explicitly, in such case, Celeborn will find the first non-loopback address to bind. 0.3.0 celeborn.network.connect.timeout 10s Default socket connect timeout. 0.2.0 celeborn.network.memory.allocator.numArenas <undefined> Number of arenas for pooled memory allocator. Default value is Runtime.getRuntime.availableProcessors, min value is 2. 0.3.0 celeborn.network.memory.allocator.verbose.metric false Weather to enable verbose metric for pooled allocator. 0.3.0 celeborn.network.timeout 240s Default timeout for network operations. 0.2.0 celeborn.port.maxRetries 1 When port is occupied, we will retry for max retry times. 0.2.0 celeborn.rpc.askTimeout 60s Timeout for RPC ask operations. It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes 0.2.0 celeborn.rpc.connect.threads 64 0.2.0 celeborn.rpc.dispatcher.threads 0 Threads number of message dispatcher event loop. Default to 0, which is availableCore. 0.3.0 celeborn.rpc.io.threads <undefined> Netty IO thread number of NettyRpcEnv to handle RPC request. The default threads number is the number of runtime available processors. 0.2.0 celeborn.rpc.lookupTimeout 30s Timeout for RPC lookup operations. 0.2.0 celeborn.shuffle.io.maxChunksBeingTransferred <undefined> The max number of chunks allowed to be transferred at the same time on shuffle service. Note that new incoming connections will be closed when the max number is hit. The client will retry according to the shuffle retry configs (see celeborn.<module>.io.maxRetries and celeborn.<module>.io.retryWait ), if those limits are reached the task will fail with fetch failure. 0.2.0 Columnar Shuffle Key Default Description Since celeborn.columnarShuffle.batch.size 10000 Vector batch size for columnar shuffle. 0.3.0 celeborn.columnarShuffle.codegen.enabled false Whether to use codegen for columnar-based shuffle. 0.3.0 celeborn.columnarShuffle.enabled false Whether to enable columnar-based shuffle. 0.2.0 celeborn.columnarShuffle.encoding.dictionary.enabled false Whether to use dictionary encoding for columnar-based shuffle data. 0.3.0 celeborn.columnarShuffle.encoding.dictionary.maxFactor 0.3 Max factor for dictionary size. The max dictionary size is min(32.0 KiB, celeborn.columnarShuffle.batch.size * celeborn.columnar.shuffle.encoding.dictionary.maxFactor) . 0.3.0 celeborn.columnarShuffle.offHeap.enabled false Whether to use off heap columnar vector. 0.3.0 Metrics Below metrics configuration both work for master and worker. Key Default Description Since celeborn.metrics.app.topDiskUsage.count 50 Size for top items about top disk usage applications list. 0.2.0 celeborn.metrics.app.topDiskUsage.interval 10min Time length for a window about top disk usage application list. 0.2.0 celeborn.metrics.app.topDiskUsage.windowSize 24 Window size about top disk usage application list. 0.2.0 celeborn.metrics.capacity 4096 The maximum number of metrics which a source can use to generate output strings. 0.2.0 celeborn.metrics.collectPerfCritical.enabled false It controls whether to collect metrics which may affect performance. When enable, Celeborn collects them. 0.2.0 celeborn.metrics.conf <undefined> Custom metrics configuration file path. Default use metrics.properties in classpath. 0.3.0 celeborn.metrics.enabled true When true, enable metrics system. 0.2.0 celeborn.metrics.extraLabels If default metric labels are not enough, extra metric labels can be customized. Labels' pattern is: <label1_key>=<label1_value>[,<label2_key>=<label2_value>]* ; e.g. env=prod,version=1 0.3.0 celeborn.metrics.master.prometheus.host <localhost> Master's Prometheus host. 0.3.0 celeborn.metrics.master.prometheus.port 9098 Master's Prometheus port. 0.3.0 celeborn.metrics.sample.rate 1.0 It controls if Celeborn collect timer metrics for some operations. Its value should be in [0.0, 1.0]. 0.2.0 celeborn.metrics.timer.slidingWindow.size 4096 The sliding window size of timer metric. 0.2.0 celeborn.metrics.worker.pauseSpentTime.forceAppend.threshold 10 Force append worker pause spent time even if worker still in pause serving state.Help user can find worker pause spent time increase, when worker always been pause state. celeborn.metrics.worker.prometheus.host <localhost> Worker's Prometheus host. 0.3.0 celeborn.metrics.worker.prometheus.port 9096 Worker's Prometheus port. 0.3.0 metrics.properties *.sink.csv.class = org.apache.celeborn.common.metrics.sink.CsvSink *.sink.prometheusServlet.class = org.apache.celeborn.common.metrics.sink.PrometheusServlet Environment Variables Recommend configuring in conf/celeborn-env.sh . Key Default Description CELEBORN_HOME $(cd \"`dirname \"$0\"`\"/..; pwd) CELEBORN_CONF_DIR ${CELEBORN_CONF_DIR:-\"${CELEBORN_HOME}/conf\"} CELEBORN_MASTER_MEMORY 1 GB CELEBORN_WORKER_MEMORY 1 GB CELEBORN_WORKER_OFFHEAP_MEMORY 1 GB CELEBORN_MASTER_JAVA_OPTS CELEBORN_WORKER_JAVA_OPTS CELEBORN_PID_DIR ${CELEBORN_HOME}/pids CELEBORN_LOG_DIR ${CELEBORN_HOME}/logs CELEBORN_SSH_OPTS -o StrictHostKeyChecking=no CELEBORN_SLEEP Waiting time for start-all and stop-all operations CELEBORN_PREFER_JEMALLOC set true to enable jemalloc memory allocator CELEBORN_JEMALLOC_PATH jemalloc library path Tuning Assume we have a cluster described as below: 5 Celeborn Workers with 20 GB off-heap memory and 10 disks. As we need to reserve 20% off-heap memory for netty, so we could assume 16 GB off-heap memory can be used for flush buffers. If spark.celeborn.client.push.buffer.max.size is 64 KB, we can have in-flight requests up to 1310720. If you have 8192 mapper tasks, you could set spark.celeborn.client.push.maxReqsInFlight=160 to gain performance improvements. If celeborn.worker.flusher.buffer.size is 256 KB, we can have total slots up to 327680 slots. Rack Awareness Celeborn can be rack-aware by setting celeborn.client.reserveSlots.rackware.enabled to true on client side. Shuffle partition block replica placement will use rack awareness for fault tolerance by placing one shuffle partition replica on a different rack. This provides data availability in the event of a network switch failure or partition within the cluster. Celeborn master daemons obtain the rack id of the cluster workers by invoking either an external script or Java class as specified by configuration files. Using either the Java class or external script for topology, output must adhere to the java org.apache.hadoop.net.DNSToSwitchMapping interface. The interface expects a one-to-one correspondence to be maintained and the topology information in the format of /myrack/myhost , where / is the topology delimiter, myrack is the rack identifier, and myhost is the individual host. Assuming a single /24 subnet per rack, one could use the format of /192.168.100.0/192.168.100.5 as a unique rack-host topology mapping. To use the Java class for topology mapping, the class name is specified by the celeborn.hadoop.net.topology.node.switch.mapping.impl parameter in the master configuration file. An example, NetworkTopology.java , is included with the Celeborn distribution and can be customized by the Celeborn administrator. Using a Java class instead of an external script has a performance benefit in that Celeborn doesn't need to fork an external process when a new worker node registers itself. If implementing an external script, it will be specified with the celeborn.hadoop.net.topology.script.file.name parameter in the master side configuration files. Unlike the Java class, the external topology script is not included with the Celeborn distribution and is provided by the administrator. Celeborn will send multiple IP addresses to ARGV when forking the topology script. The number of IP addresses sent to the topology script is controlled with celeborn.hadoop.net.topology.script.number.args and defaults to 100. If celeborn.hadoop.net.topology.script.number.args was changed to 1, a topology script would get forked for each IP submitted by workers. If celeborn.hadoop.net.topology.script.file.name or celeborn.hadoop.net.topology.node.switch.mapping.impl is not set, the rack id /default-rack is returned for any passed IP address. While this behavior appears desirable, it can cause issues with shuffle partition block replication as default behavior is to write one replicated block off rack and is unable to do so as there is only a single rack named /default-rack . Example can refer to Hadoop Rack Awareness since Celeborn use hadoop's code about rack-aware. Worker Recover Status After Restart ShuffleClient records the shuffle partition location's host, service port, and filename, to support workers recovering reading existing shuffle data after worker restart, during worker shutdown, workers should store the meta about reading shuffle partition files in LevelDB, and restore the meta after restarting workers, also workers should keep a stable service port to support ShuffleClient retry reading data. Users should set celeborn.worker.graceful.shutdown.enabled to true and set below service port with stable port to support worker recover status. celeborn.worker.rpc.port celeborn.worker.fetch.port celeborn.worker.push.port celeborn.worker.replicate.port","title":"Configuration"},{"location":"configuration/#configuration-guide","text":"This documentation contains Celeborn configuration details and a tuning guide.","title":"Configuration Guide"},{"location":"configuration/#important-configurations","text":"","title":"Important Configurations"},{"location":"configuration/#environment-variables","text":"CELEBORN_WORKER_MEMORY=4g CELEBORN_WORKER_OFFHEAP_MEMORY=24g Celeborn workers tend to improve performance by using off-heap buffers. Off-heap memory requirement can be estimated as below: numDirs = `celeborn.worker.storage.dirs` # the amount of directory will be used by Celeborn storage bufferSize = `celeborn.worker.flusher.buffer.size` # the amount of memory will be used by a single flush buffer off-heap-memory = bufferSize * estimatedTasks * 2 + network memory For example, if a Celeborn worker has 10 storage directories or disks and the buffer size is set to 256 KiB. The necessary off-heap memory is 10 GiB. Network memory will be consumed when netty reads from a TCP channel, there will need some extra memory. Empirically, Celeborn worker off-heap memory should be set to (numDirs * bufferSize * 1.2) .","title":"Environment Variables"},{"location":"configuration/#all-configurations","text":"","title":"All Configurations"},{"location":"configuration/#master","text":"Key Default Description Since celeborn.master.estimatedPartitionSize.initialSize 64mb Initial partition size for estimation, it will change according to runtime stats. 0.3.0 celeborn.master.estimatedPartitionSize.update.initialDelay 5min Initial delay time before start updating partition size for estimation. 0.3.0 celeborn.master.estimatedPartitionSize.update.interval 10min Interval of updating partition size for estimation. 0.3.0 celeborn.master.hdfs.expireDirs.timeout 1h The timeout for a expire dirs to be deleted on HDFS. 0.3.0 celeborn.master.heartbeat.application.timeout 300s Application heartbeat timeout. 0.3.0 celeborn.master.heartbeat.worker.timeout 120s Worker heartbeat timeout. 0.3.0 celeborn.master.host <localhost> Hostname for master to bind. 0.2.0 celeborn.master.port 9097 Port for master to bind. 0.2.0 celeborn.master.slot.assign.extraSlots 2 Extra slots number when master assign slots. 0.3.0 celeborn.master.slot.assign.loadAware.diskGroupGradient 0.1 This value means how many more workload will be placed into a faster disk group than a slower group. 0.3.0 celeborn.master.slot.assign.loadAware.fetchTimeWeight 1.0 Weight of average fetch time when calculating ordering in load-aware assignment strategy 0.3.0 celeborn.master.slot.assign.loadAware.flushTimeWeight 0.0 Weight of average flush time when calculating ordering in load-aware assignment strategy 0.3.0 celeborn.master.slot.assign.loadAware.numDiskGroups 5 This configuration is a guidance for load-aware slot allocation algorithm. This value is control how many disk groups will be created. 0.3.0 celeborn.master.slot.assign.maxWorkers 10000 Max workers that slots of one shuffle can be allocated on. Will choose the smaller positive one from Master side and Client side, see celeborn.client.slot.assign.maxWorkers . 0.3.1 celeborn.master.slot.assign.policy ROUNDROBIN Policy for master to assign slots, Celeborn supports two types of policy: roundrobin and loadaware. Loadaware policy will be ignored when HDFS is enabled in celeborn.storage.activeTypes 0.3.0 celeborn.master.userResourceConsumption.update.interval 30s Time length for a window about compute user resource consumption. 0.3.0 celeborn.master.workerUnavailableInfo.expireTimeout 1800s Worker unavailable info would be cleared when the retention period is expired 0.3.1 celeborn.storage.availableTypes HDD Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.hdfs.dir <undefined> HDFS base directory for Celeborn to store shuffle data. 0.2.0 celeborn.storage.hdfs.kerberos.keytab <undefined> Kerberos keytab file path for HDFS storage connection. 0.3.2 celeborn.storage.hdfs.kerberos.principal <undefined> Kerberos principal for HDFS storage connection. 0.3.2 Apart from these, the following properties are also available for enable master HA:","title":"Master"},{"location":"configuration/#master-ha","text":"Key Default Description Since celeborn.master.ha.enabled false When true, master nodes run as Raft cluster mode. 0.3.0 celeborn.master.ha.node.<id>.host <required> Host to bind of master node in HA mode. 0.3.0 celeborn.master.ha.node.<id>.port 9097 Port to bind of master node in HA mode. 0.3.0 celeborn.master.ha.node.<id>.ratis.port 9872 Ratis port to bind of master node in HA mode. 0.3.0 celeborn.master.ha.ratis.raft.rpc.type netty RPC type for Ratis, available options: netty, grpc. 0.3.0 celeborn.master.ha.ratis.raft.server.storage.dir /tmp/ratis 0.3.0","title":"Master HA"},{"location":"configuration/#worker","text":"Key Default Description Since celeborn.master.endpoints <localhost>:9097 Endpoints of master nodes for celeborn client to connect, allowed pattern is: <host1>:<port1>[,<host2>:<port2>]* , e.g. clb1:9097,clb2:9098,clb3:9099 . If the port is omitted, 9097 will be used. 0.2.0 celeborn.master.estimatedPartitionSize.minSize 8mb Ignore partition size smaller than this configuration of partition size for estimation. 0.3.0 celeborn.shuffle.chunk.size 8m Max chunk size of reducer's merged shuffle data. For example, if a reducer's shuffle data is 128M and the data will need 16 fetch chunk requests to fetch. 0.2.0 celeborn.storage.availableTypes HDD Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.hdfs.dir <undefined> HDFS base directory for Celeborn to store shuffle data. 0.2.0 celeborn.storage.hdfs.kerberos.keytab <undefined> Kerberos keytab file path for HDFS storage connection. 0.3.2 celeborn.storage.hdfs.kerberos.principal <undefined> Kerberos principal for HDFS storage connection. 0.3.2 celeborn.worker.activeConnection.max <undefined> If the number of active connections on a worker exceeds this configuration value, the worker will be marked as high-load in the heartbeat report, and the master will not include that node in the response of RequestSlots. 0.3.1 celeborn.worker.bufferStream.threadsPerMountpoint 8 Threads count for read buffer per mount point. 0.3.0 celeborn.worker.clean.threads 64 Thread number of worker to clean up expired shuffle keys. 0.3.2 celeborn.worker.closeIdleConnections false Whether worker will close idle connections. 0.2.0 celeborn.worker.commitFiles.threads 32 Thread number of worker to commit shuffle data files asynchronously. It's recommended to set at least 128 when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.commitFiles.timeout 120s Timeout for a Celeborn worker to commit files of a shuffle. It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.congestionControl.check.interval 10ms Interval of worker checks congestion if celeborn.worker.congestionControl.enabled is true. 0.3.2 celeborn.worker.congestionControl.enabled false Whether to enable congestion control or not. 0.3.0 celeborn.worker.congestionControl.high.watermark <undefined> If the total bytes in disk buffer exceeds this configure, will start to congestusers whose produce rate is higher than the potential average consume rate. The congestion will stop if the produce rate is lower or equal to the average consume rate, or the total pending bytes lower than celeborn.worker.congestionControl.low.watermark 0.3.0 celeborn.worker.congestionControl.low.watermark <undefined> Will stop congest users if the total pending bytes of disk buffer is lower than this configuration 0.3.0 celeborn.worker.congestionControl.sample.time.window 10s The worker holds a time sliding list to calculate users' produce/consume rate 0.3.0 celeborn.worker.congestionControl.user.inactive.interval 10min How long will consider this user is inactive if it doesn't send data 0.3.0 celeborn.worker.decommission.checkInterval 30s The wait interval of checking whether all the shuffle expired during worker decommission 0.4.0 celeborn.worker.decommission.forceExitTimeout 6h The wait time of waiting for all the shuffle expire during worker decommission. 0.4.0 celeborn.worker.directMemoryRatioForMemoryShuffleStorage 0.0 Max ratio of direct memory to store shuffle data 0.2.0 celeborn.worker.directMemoryRatioForReadBuffer 0.1 Max ratio of direct memory for read buffer 0.2.0 celeborn.worker.directMemoryRatioToPauseReceive 0.85 If direct memory usage reaches this limit, the worker will stop to receive data from Celeborn shuffle clients. 0.2.0 celeborn.worker.directMemoryRatioToPauseReplicate 0.95 If direct memory usage reaches this limit, the worker will stop to receive replication data from other workers. This value should be higher than celeborn.worker.directMemoryRatioToPauseReceive. 0.2.0 celeborn.worker.directMemoryRatioToResume 0.7 If direct memory usage is less than this limit, worker will resume. 0.2.0 celeborn.worker.disk.clean.threads 4 Thread number of worker to clean up directories of expired shuffle keys on disk. 0.3.2 celeborn.worker.fetch.heartbeat.enabled false enable the heartbeat from worker to client when fetching data 0.3.0 celeborn.worker.fetch.io.threads <undefined> Netty IO thread number of worker to handle client fetch data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.fetch.port 0 Server port for Worker to receive fetch data request from ShuffleClient. 0.2.0 celeborn.worker.flusher.buffer.size 256k Size of buffer used by a single flusher. 0.2.0 celeborn.worker.flusher.diskTime.slidingWindow.size 20 The size of sliding windows used to calculate statistics about flushed time and count. 0.3.0 celeborn.worker.flusher.hdd.threads 1 Flusher's thread count per disk used for write data to HDD disks. 0.2.0 celeborn.worker.flusher.hdfs.buffer.size 4m Size of buffer used by a HDFS flusher. 0.3.0 celeborn.worker.flusher.hdfs.threads 8 Flusher's thread count used for write data to HDFS. 0.2.0 celeborn.worker.flusher.shutdownTimeout 3s Timeout for a flusher to shutdown. 0.2.0 celeborn.worker.flusher.ssd.threads 16 Flusher's thread count per disk used for write data to SSD disks. 0.2.0 celeborn.worker.flusher.threads 16 Flusher's thread count per disk for unknown-type disks. 0.2.0 celeborn.worker.graceful.shutdown.checkSlotsFinished.interval 1s The wait interval of checking whether all released slots to be committed or destroyed during worker graceful shutdown 0.2.0 celeborn.worker.graceful.shutdown.checkSlotsFinished.timeout 480s The wait time of waiting for the released slots to be committed or destroyed during worker graceful shutdown. 0.2.0 celeborn.worker.graceful.shutdown.enabled false When true, during worker shutdown, the worker will wait for all released slots to be committed or destroyed. 0.2.0 celeborn.worker.graceful.shutdown.partitionSorter.shutdownTimeout 120s The wait time of waiting for sorting partition files during worker graceful shutdown. 0.2.0 celeborn.worker.graceful.shutdown.recoverPath <tmp>/recover The path to store levelDB. 0.2.0 celeborn.worker.graceful.shutdown.saveCommittedFileInfo.interval 5s Interval for a Celeborn worker to flush committed file infos into Level DB. 0.3.1 celeborn.worker.graceful.shutdown.saveCommittedFileInfo.sync false Whether to call sync method to save committed file infos into Level DB to handle OS crash. 0.3.1 celeborn.worker.graceful.shutdown.timeout 600s The worker's graceful shutdown timeout time. 0.2.0 celeborn.worker.monitor.disk.check.interval 30s Intervals between device monitor to check disk. 0.3.0 celeborn.worker.monitor.disk.check.timeout 30s Timeout time for worker check device status. 0.3.0 celeborn.worker.monitor.disk.checklist readwrite,diskusage Monitor type for disk, available items are: iohang, readwrite and diskusage. 0.2.0 celeborn.worker.monitor.disk.enabled true When true, worker will monitor device and report to master. 0.3.0 celeborn.worker.monitor.disk.notifyError.expireTimeout 10m The expire timeout of non-critical device error. Only notify critical error when the number of non-critical errors for a period of time exceeds threshold. 0.3.0 celeborn.worker.monitor.disk.notifyError.threshold 64 Device monitor will only notify critical error once the accumulated valid non-critical error number exceeding this threshold. 0.3.0 celeborn.worker.monitor.disk.sys.block.dir /sys/block The directory where linux file block information is stored. 0.2.0 celeborn.worker.monitor.memory.check.interval 10ms Interval of worker direct memory checking. 0.3.0 celeborn.worker.monitor.memory.report.interval 10s Interval of worker direct memory tracker reporting to log. 0.3.0 celeborn.worker.monitor.memory.trimChannelWaitInterval 1s Wait time after worker trigger channel to trim cache. 0.3.0 celeborn.worker.monitor.memory.trimFlushWaitInterval 1s Wait time after worker trigger StorageManger to flush data. 0.3.0 celeborn.worker.partition.initial.readBuffersMax 1024 Max number of initial read buffers 0.3.0 celeborn.worker.partition.initial.readBuffersMin 1 Min number of initial read buffers 0.3.0 celeborn.worker.partitionSorter.directMemoryRatioThreshold 0.1 Max ratio of partition sorter's memory for sorting, when reserved memory is higher than max partition sorter memory, partition sorter will stop sorting. 0.2.0 celeborn.worker.push.heartbeat.enabled false enable the heartbeat from worker to client when pushing data 0.3.0 celeborn.worker.push.io.threads <undefined> Netty IO thread number of worker to handle client push data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.push.port 0 Server port for Worker to receive push data request from ShuffleClient. 0.2.0 celeborn.worker.readBuffer.allocationWait 50ms The time to wait when buffer dispatcher can not allocate a buffer. 0.3.0 celeborn.worker.readBuffer.target.changeThreshold 1mb The target ratio for pre read memory usage. 0.3.0 celeborn.worker.readBuffer.target.ratio 0.9 The target ratio for read ahead buffer's memory usage. 0.3.0 celeborn.worker.readBuffer.target.updateInterval 100ms The interval for memory manager to calculate new read buffer's target memory. 0.3.0 celeborn.worker.readBuffer.toTriggerReadMin 32 Min buffers count for map data partition to trigger read. 0.3.0 celeborn.worker.register.timeout 180s Worker register timeout. 0.2.0 celeborn.worker.replicate.fastFail.duration 60s If a replicate request not replied during the duration, worker will mark the replicate data request as failed.It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes . 0.2.0 celeborn.worker.replicate.io.threads <undefined> Netty IO thread number of worker to replicate shuffle data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.replicate.port 0 Server port for Worker to receive replicate data request from other Workers. 0.2.0 celeborn.worker.replicate.randomConnection.enabled true Whether worker will create random connection to peer when replicate data. When false, worker tend to reuse the same cached TransportClient to a specific replicate worker; when true, worker tend to use different cached TransportClient. Netty will use the same thread to serve the same connection, so with more connections replicate server can leverage more netty threads 0.2.1 celeborn.worker.replicate.threads 64 Thread number of worker to replicate shuffle data. 0.2.0 celeborn.worker.rpc.port 0 Server port for Worker to receive RPC request. 0.2.0 celeborn.worker.shuffle.partitionSplit.enabled true enable the partition split on worker side 0.3.0 celeborn.worker.shuffle.partitionSplit.max 2g Specify the maximum partition size for splitting, and ensure that individual partition files are always smaller than this limit. 0.3.0 celeborn.worker.shuffle.partitionSplit.min 1m Min size for a partition to split 0.3.0 celeborn.worker.sortPartition.reservedMemoryPerPartition 1mb Reserved memory when sorting a shuffle file off-heap. 0.3.0 celeborn.worker.sortPartition.threads <undefined> PartitionSorter's thread counts. It's recommended to set at least 64 when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.sortPartition.timeout 220s Timeout for a shuffle file to sort. 0.3.0 celeborn.worker.storage.checkDirsEmpty.maxRetries 3 The number of retries for a worker to check if the working directory is cleaned up before registering with the master. 0.3.0 celeborn.worker.storage.checkDirsEmpty.timeout 1000ms The wait time per retry for a worker to check if the working directory is cleaned up before registering with the master. 0.3.0 celeborn.worker.storage.dirs <undefined> Directory list to store shuffle data. It's recommended to configure one directory on each disk. Storage size limit can be set for each directory. For the sake of performance, there should be no more than 2 flush threads on the same disk partition if you are using HDD, and should be 8 or more flush threads on the same disk partition if you are using SSD. For example: dir1[:capacity=][:disktype=][:flushthread=],dir2[:capacity=][:disktype=][:flushthread=] 0.2.0 celeborn.worker.storage.disk.reserve.ratio <undefined> Celeborn worker reserved ratio for each disk. The minimum usable size for each disk is the max space between the reserved space and the space calculate via reserved ratio. 0.3.2 celeborn.worker.storage.disk.reserve.size 5G Celeborn worker reserved space for each disk. 0.3.0 celeborn.worker.storage.expireDirs.timeout 1h The timeout for a expire dirs to be deleted on disk. 0.3.2 celeborn.worker.storage.workingDir celeborn-worker/shuffle_data Worker's working dir path name. 0.3.0 celeborn.worker.writer.close.timeout 120s Timeout for a file writer to close 0.2.0 celeborn.worker.writer.create.maxAttempts 3 Retry count for a file writer to create if its creation was failed. 0.2.0","title":"Worker"},{"location":"configuration/#client","text":"Key Default Description Since celeborn.client.application.heartbeatInterval 10s Interval for client to send heartbeat message to master. 0.3.0 celeborn.client.application.unregister.enabled true When true, Celeborn client will inform celeborn master the application is already shutdown during client exit, this allows the cluster to release resources immediately, resulting in resource savings. 0.3.2 celeborn.client.closeIdleConnections true Whether client will close idle connections. 0.3.0 celeborn.client.commitFiles.ignoreExcludedWorker false When true, LifecycleManager will skip workers which are in the excluded list. 0.3.0 celeborn.client.eagerlyCreateInputStream.threads 32 Threads count for streamCreatorPool in CelebornShuffleReader. 0.3.1 celeborn.client.excludePeerWorkerOnFailure.enabled true When true, Celeborn will exclude partition's peer worker on failure when push data to replica failed. 0.3.0 celeborn.client.excludedWorker.expireTimeout 180s Timeout time for LifecycleManager to clear reserved excluded worker. Default to be 1.5 * celeborn.master.heartbeat.worker.timeout to cover worker heartbeat timeout check period 0.3.0 celeborn.client.fetch.dfsReadChunkSize 8m Max chunk size for DfsPartitionReader. 0.3.1 celeborn.client.fetch.excludeWorkerOnFailure.enabled false Whether to enable shuffle client-side fetch exclude workers on failure. 0.3.0 celeborn.client.fetch.excludedWorker.expireTimeout <value of celeborn.client.excludedWorker.expireTimeout> ShuffleClient is a static object, it will be used in the whole lifecycle of Executor,We give a expire time for excluded workers to avoid a transient worker issues. 0.3.0 celeborn.client.fetch.maxReqsInFlight 3 Amount of in-flight chunk fetch request. 0.3.0 celeborn.client.fetch.maxRetriesForEachReplica 3 Max retry times of fetch chunk on each replica 0.3.0 celeborn.client.fetch.timeout 600s Timeout for a task to open stream and fetch chunk. 0.3.0 celeborn.client.flink.compression.enabled true Whether to compress data in Flink plugin. 0.3.0 celeborn.client.flink.inputGate.concurrentReadings 2147483647 Max concurrent reading channels for a input gate. 0.3.0 celeborn.client.flink.inputGate.memory 32m Memory reserved for a input gate. 0.3.0 celeborn.client.flink.inputGate.minMemory 8m Min memory reserved for a input gate. 0.3.0 celeborn.client.flink.inputGate.supportFloatingBuffer true Whether to support floating buffer in Flink input gates. 0.3.0 celeborn.client.flink.resultPartition.memory 64m Memory reserved for a result partition. 0.3.0 celeborn.client.flink.resultPartition.minMemory 8m Min memory reserved for a result partition. 0.3.0 celeborn.client.flink.resultPartition.supportFloatingBuffer true Whether to support floating buffer for result partitions. 0.3.0 celeborn.client.push.buffer.initial.size 8k 0.3.0 celeborn.client.push.buffer.max.size 64k Max size of reducer partition buffer memory for shuffle hash writer. The pushed data will be buffered in memory before sending to Celeborn worker. For performance consideration keep this buffer size higher than 32K. Example: If reducer amount is 2000, buffer size is 64K, then each task will consume up to 64KiB * 2000 = 125MiB heap memory. 0.3.0 celeborn.client.push.excludeWorkerOnFailure.enabled false Whether to enable shuffle client-side push exclude workers on failures. 0.3.0 celeborn.client.push.limit.inFlight.sleepInterval 50ms Sleep interval when check netty in-flight requests to be done. 0.3.0 celeborn.client.push.limit.inFlight.timeout <undefined> Timeout for netty in-flight requests to be done.Default value should be celeborn.client.push.timeout * 2 . 0.3.0 celeborn.client.push.limit.strategy SIMPLE The strategy used to control the push speed. Valid strategies are SIMPLE and SLOWSTART. The SLOWSTART strategy usually works with congestion control mechanism on the worker side. 0.3.0 celeborn.client.push.maxReqsInFlight.perWorker 32 Amount of Netty in-flight requests per worker. Default max memory of in flight requests per worker is celeborn.client.push.maxReqsInFlight.perWorker * celeborn.client.push.buffer.max.size * compression ratio(1 in worst case): 64KiB * 32 = 2MiB. The maximum memory will not exceed celeborn.client.push.maxReqsInFlight.total . 0.3.0 celeborn.client.push.maxReqsInFlight.total 256 Amount of total Netty in-flight requests. The maximum memory is celeborn.client.push.maxReqsInFlight.total * celeborn.client.push.buffer.max.size * compression ratio(1 in worst case): 64KiB * 256 = 16MiB 0.3.0 celeborn.client.push.queue.capacity 512 Push buffer queue size for a task. The maximum memory is celeborn.client.push.buffer.max.size * celeborn.client.push.queue.capacity , default: 64KiB * 512 = 32MiB 0.3.0 celeborn.client.push.replicate.enabled false When true, Celeborn worker will replicate shuffle data to another Celeborn worker asynchronously to ensure the pushed shuffle data won't be lost after the node failure. It's recommended to set false when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.client.push.retry.threads 8 Thread number to process shuffle re-send push data requests. 0.3.0 celeborn.client.push.revive.batchSize 2048 Max number of partitions in one Revive request. 0.3.0 celeborn.client.push.revive.interval 100ms Interval for client to trigger Revive to LifecycleManager. The number of partitions in one Revive request is celeborn.client.push.revive.batchSize . 0.3.0 celeborn.client.push.revive.maxRetries 5 Max retry times for reviving when celeborn push data failed. 0.3.0 celeborn.client.push.sendBufferPool.checkExpireInterval 30s Interval to check expire for send buffer pool. If the pool has been idle for more than celeborn.client.push.sendBufferPool.expireTimeout , the pooled send buffers and push tasks will be cleaned up. 0.3.1 celeborn.client.push.sendBufferPool.expireTimeout 60s Timeout before clean up SendBufferPool. If SendBufferPool is idle for more than this time, the send buffers and push tasks will be cleaned up. 0.3.1 celeborn.client.push.slowStart.initialSleepTime 500ms The initial sleep time if the current max in flight requests is 0 0.3.0 celeborn.client.push.slowStart.maxSleepTime 2s If celeborn.client.push.limit.strategy is set to SLOWSTART, push side will take a sleep strategy for each batch of requests, this controls the max sleep time if the max in flight requests limit is 1 for a long time 0.3.0 celeborn.client.push.sort.randomizePartitionId.enabled false Whether to randomize partitionId in push sorter. If true, partitionId will be randomized when sort data to avoid skew when push to worker 0.3.0 celeborn.client.push.splitPartition.threads 8 Thread number to process shuffle split request in shuffle client. 0.3.0 celeborn.client.push.stageEnd.timeout <value of celeborn.<module>.io.connectionTimeout> Timeout for waiting StageEnd. During this process, there are celeborn.client.requestCommitFiles.maxRetries times for retry opportunities for committing files and 1 times for releasing slots request. User can customize this value according to your setting. By default, the value is the max timeout value celeborn.<module>.io.connectionTimeout . 0.3.0 celeborn.client.push.takeTaskMaxWaitAttempts 1 Max wait times if no task available to push to worker. 0.3.0 celeborn.client.push.takeTaskWaitInterval 50ms Wait interval if no task available to push to worker. 0.3.0 celeborn.client.push.timeout 120s Timeout for a task to push data rpc message. This value should better be more than twice of celeborn.<module>.push.timeoutCheck.interval 0.3.0 celeborn.client.readLocalShuffleFile.enabled false Enable read local shuffle file for clusters that co-deployed with yarn node manager. 0.3.1 celeborn.client.readLocalShuffleFile.threads 4 Threads count for read local shuffle file. 0.3.1 celeborn.client.registerShuffle.maxRetries 3 Max retry times for client to register shuffle. 0.3.0 celeborn.client.registerShuffle.retryWait 3s Wait time before next retry if register shuffle failed. 0.3.0 celeborn.client.requestCommitFiles.maxRetries 4 Max retry times for requestCommitFiles RPC. 0.3.0 celeborn.client.reserveSlots.maxRetries 3 Max retry times for client to reserve slots. 0.3.0 celeborn.client.reserveSlots.rackaware.enabled false Whether need to place different replicates on different racks when allocating slots. 0.3.1 celeborn.client.reserveSlots.retryWait 3s Wait time before next retry if reserve slots failed. 0.3.0 celeborn.client.rpc.cache.concurrencyLevel 32 The number of write locks to update rpc cache. 0.3.0 celeborn.client.rpc.cache.expireTime 15s The time before a cache item is removed. 0.3.0 celeborn.client.rpc.cache.size 256 The max cache items count for rpc cache. 0.3.0 celeborn.client.rpc.getReducerFileGroup.askTimeout <value of celeborn.<module>.io.connectionTimeout> Timeout for ask operations during getting reducer file group information. During this process, there are celeborn.client.requestCommitFiles.maxRetries times for retry opportunities for committing files and 1 times for releasing slots request. User can customize this value according to your setting. By default, the value is the max timeout value celeborn.<module>.io.connectionTimeout . 0.2.0 celeborn.client.rpc.maxRetries 3 Max RPC retry times in LifecycleManager. 0.3.2 celeborn.client.rpc.registerShuffle.askTimeout <value of celeborn.<module>.io.connectionTimeout> Timeout for ask operations during register shuffle. During this process, there are two times for retry opportunities for requesting slots, one request for establishing a connection with Worker and celeborn.client.reserveSlots.maxRetries times for retry opportunities for reserving slots. User can customize this value according to your setting. By default, the value is the max timeout value celeborn.<module>.io.connectionTimeout . 0.3.0 celeborn.client.rpc.requestPartition.askTimeout <value of celeborn.<module>.io.connectionTimeout> Timeout for ask operations during requesting change partition location, such as reviving or splitting partition. During this process, there are celeborn.client.reserveSlots.maxRetries times for retry opportunities for reserving slots. User can customize this value according to your setting. By default, the value is the max timeout value celeborn.<module>.io.connectionTimeout . 0.2.0 celeborn.client.rpc.reserveSlots.askTimeout <value of celeborn.rpc.askTimeout> Timeout for LifecycleManager request reserve slots. 0.3.0 celeborn.client.rpc.shared.threads 16 Number of shared rpc threads in LifecycleManager. 0.3.2 celeborn.client.shuffle.batchHandleChangePartition.interval 100ms Interval for LifecycleManager to schedule handling change partition requests in batch. 0.3.0 celeborn.client.shuffle.batchHandleChangePartition.threads 8 Threads number for LifecycleManager to handle change partition request in batch. 0.3.0 celeborn.client.shuffle.batchHandleCommitPartition.interval 5s Interval for LifecycleManager to schedule handling commit partition requests in batch. 0.3.0 celeborn.client.shuffle.batchHandleCommitPartition.threads 8 Threads number for LifecycleManager to handle commit partition request in batch. 0.3.0 celeborn.client.shuffle.batchHandleReleasePartition.interval 5s Interval for LifecycleManager to schedule handling release partition requests in batch. 0.3.0 celeborn.client.shuffle.batchHandleReleasePartition.threads 8 Threads number for LifecycleManager to handle release partition request in batch. 0.3.0 celeborn.client.shuffle.compression.codec LZ4 The codec used to compress shuffle data. By default, Celeborn provides three codecs: lz4 , zstd , none . 0.3.0 celeborn.client.shuffle.compression.zstd.level 1 Compression level for Zstd compression codec, its value should be an integer between -5 and 22. Increasing the compression level will result in better compression at the expense of more CPU and memory. 0.3.0 celeborn.client.shuffle.expired.checkInterval 60s Interval for client to check expired shuffles. 0.3.0 celeborn.client.shuffle.manager.port 0 Port used by the LifecycleManager on the Driver. 0.3.0 celeborn.client.shuffle.mapPartition.split.enabled false whether to enable shuffle partition split. Currently, this only applies to MapPartition. 0.3.1 celeborn.client.shuffle.partition.type REDUCE Type of shuffle's partition. 0.3.0 celeborn.client.shuffle.partitionSplit.mode SOFT soft: the shuffle file size might be larger than split threshold. hard: the shuffle file size will be limited to split threshold. 0.3.0 celeborn.client.shuffle.partitionSplit.threshold 1G Shuffle file size threshold, if file size exceeds this, trigger split. 0.3.0 celeborn.client.shuffle.rangeReadFilter.enabled false If a spark application have skewed partition, this value can set to true to improve performance. 0.2.0 celeborn.client.slot.assign.maxWorkers 10000 Max workers that slots of one shuffle can be allocated on. Will choose the smaller positive one from Master side and Client side, see celeborn.master.slot.assign.maxWorkers . 0.3.1 celeborn.client.spark.push.sort.memory.threshold 64m When SortBasedPusher use memory over the threshold, will trigger push data. If the pipeline push feature is enabled ( celeborn.client.spark.push.sort.pipeline.enabled=true ), the SortBasedPusher will trigger a data push when the memory usage exceeds half of the threshold(by default, 32m). 0.3.0 celeborn.client.spark.push.sort.pipeline.enabled false Whether to enable pipelining for sort based shuffle writer. If true, double buffering will be used to pipeline push 0.3.0 celeborn.client.spark.push.unsafeRow.fastWrite.enabled true This is Celeborn's optimization on UnsafeRow for Spark and it's true by default. If you have changed UnsafeRow's memory layout set this to false. 0.2.2 celeborn.client.spark.shuffle.forceFallback.enabled false Whether force fallback shuffle to Spark's default. 0.3.0 celeborn.client.spark.shuffle.forceFallback.numPartitionsThreshold 500000 Celeborn will only accept shuffle of partition number lower than this configuration value. 0.3.0 celeborn.client.spark.shuffle.writer HASH Celeborn supports the following kind of shuffle writers. 1. hash: hash-based shuffle writer works fine when shuffle partition count is normal; 2. sort: sort-based shuffle writer works fine when memory pressure is high or shuffle partition count is huge. 0.3.0 celeborn.master.endpoints <localhost>:9097 Endpoints of master nodes for celeborn client to connect, allowed pattern is: <host1>:<port1>[,<host2>:<port2>]* , e.g. clb1:9097,clb2:9098,clb3:9099 . If the port is omitted, 9097 will be used. 0.2.0 celeborn.storage.availableTypes HDD Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.hdfs.dir <undefined> HDFS base directory for Celeborn to store shuffle data. 0.2.0","title":"Client"},{"location":"configuration/#quota","text":"Key Default Description Since celeborn.quota.configuration.path <undefined> Quota configuration file path. The file format should be yaml. Quota configuration file template can be found under conf directory. 0.2.0 celeborn.quota.enabled true When true, before registering shuffle, LifecycleManager should check if current user have enough quota space, if cluster don't have enough quota space for current user, fallback to Spark's default shuffle 0.2.0 celeborn.quota.identity.provider org.apache.celeborn.common.identity.DefaultIdentityProvider IdentityProvider class name. Default class is org.apache.celeborn.common.identity.DefaultIdentityProvider . Optional values: org.apache.celeborn.common.identity.HadoopBasedIdentityProvider user name will be obtained by UserGroupInformation.getUserName; org.apache.celeborn.common.identity.DefaultIdentityProvider user name and tenant id are default values or user-specific values. 0.2.0 celeborn.quota.identity.user-specific.tenant default Tenant id if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.quota.identity.user-specific.userName default User name if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.quota.manager org.apache.celeborn.common.quota.DefaultQuotaManager QuotaManger class name. Default class is org.apache.celeborn.common.quota.DefaultQuotaManager . 0.2.0","title":"Quota"},{"location":"configuration/#network","text":"Key Default Description Since celeborn.<module>.fetch.timeoutCheck.interval 5s Interval for checking fetch data timeout. It only support setting to data since it works for shuffle client fetch data and should be configured on client side. 0.3.0 celeborn.<module>.fetch.timeoutCheck.threads 4 Threads num for checking fetch data timeout. It only support setting to data since it works for shuffle client fetch data and should be configured on client side. 0.3.0 celeborn.<module>.heartbeat.interval 60s The heartbeat interval between worker and client. If setting to data , it works for shuffle client push and fetch data and should be configured on client side. If setting to replicate , it works for worker replicate data to peer worker and should be configured on worker side. 0.3.0 celeborn.<module>.io.backLog 0 Requested maximum length of the queue of incoming connections. Default 0 for no backlog. celeborn.<module>.io.clientThreads 0 Number of threads used in the client thread pool. Default to 0, which is 2x#cores. celeborn.<module>.io.connectTimeout <value of celeborn.network.connect.timeout> Socket connect timeout. celeborn.<module>.io.connectionTimeout <value of celeborn.network.timeout> Connection active timeout. celeborn.<module>.io.enableVerboseMetrics false Whether to track Netty memory detailed metrics. If true, the detailed metrics of Netty PoolByteBufAllocator will be gotten, otherwise only general memory usage will be tracked. celeborn.<module>.io.lazyFD true Whether to initialize FileDescriptor lazily or not. If true, file descriptors are created only when data is going to be transferred. This can reduce the number of open files. celeborn.<module>.io.maxRetries 3 Max number of times we will try IO exceptions (such as connection timeouts) per request. If set to 0, we will not do any retries. celeborn.<module>.io.mode NIO Netty EventLoopGroup backend, available options: NIO, EPOLL. celeborn.<module>.io.numConnectionsPerPeer 2 Number of concurrent connections between two nodes. celeborn.<module>.io.preferDirectBufs true If true, we will prefer allocating off-heap byte buffers within Netty. celeborn.<module>.io.receiveBuffer 0b Receive buffer size (SO_RCVBUF). Note: the optimal size for receive buffer and send buffer should be latency * network_bandwidth. Assuming latency = 1ms, network_bandwidth = 10Gbps buffer size should be ~ 1.25MB. 0.2.0 celeborn.<module>.io.retryWait 5s Time that we will wait in order to perform a retry after an IOException. Only relevant if maxIORetries > 0. 0.2.0 celeborn.<module>.io.sendBuffer 0b Send buffer size (SO_SNDBUF). 0.2.0 celeborn.<module>.io.serverThreads 0 Number of threads used in the server thread pool. Default to 0, which is 2x#cores. celeborn.<module>.push.timeoutCheck.interval 5s Interval for checking push data timeout. If setting to data , it works for shuffle client push data and should be configured on client side. If setting to replicate , it works for worker replicate data to peer worker and should be configured on worker side. 0.3.0 celeborn.<module>.push.timeoutCheck.threads 4 Threads num for checking push data timeout. If setting to data , it works for shuffle client push data and should be configured on client side. If setting to replicate , it works for worker replicate data to peer worker and should be configured on worker side. 0.3.0 celeborn.<role>.rpc.dispatcher.threads <value of celeborn.rpc.dispatcher.threads> Threads number of message dispatcher event loop for roles celeborn.io.maxDefaultNettyThreads 64 Max default netty threads 0.3.2 celeborn.network.bind.preferIpAddress true When ture , prefer to use IP address, otherwise FQDN. This configuration only takes effects when the bind hostname is not set explicitly, in such case, Celeborn will find the first non-loopback address to bind. 0.3.0 celeborn.network.connect.timeout 10s Default socket connect timeout. 0.2.0 celeborn.network.memory.allocator.numArenas <undefined> Number of arenas for pooled memory allocator. Default value is Runtime.getRuntime.availableProcessors, min value is 2. 0.3.0 celeborn.network.memory.allocator.verbose.metric false Weather to enable verbose metric for pooled allocator. 0.3.0 celeborn.network.timeout 240s Default timeout for network operations. 0.2.0 celeborn.port.maxRetries 1 When port is occupied, we will retry for max retry times. 0.2.0 celeborn.rpc.askTimeout 60s Timeout for RPC ask operations. It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes 0.2.0 celeborn.rpc.connect.threads 64 0.2.0 celeborn.rpc.dispatcher.threads 0 Threads number of message dispatcher event loop. Default to 0, which is availableCore. 0.3.0 celeborn.rpc.io.threads <undefined> Netty IO thread number of NettyRpcEnv to handle RPC request. The default threads number is the number of runtime available processors. 0.2.0 celeborn.rpc.lookupTimeout 30s Timeout for RPC lookup operations. 0.2.0 celeborn.shuffle.io.maxChunksBeingTransferred <undefined> The max number of chunks allowed to be transferred at the same time on shuffle service. Note that new incoming connections will be closed when the max number is hit. The client will retry according to the shuffle retry configs (see celeborn.<module>.io.maxRetries and celeborn.<module>.io.retryWait ), if those limits are reached the task will fail with fetch failure. 0.2.0","title":"Network"},{"location":"configuration/#columnar-shuffle","text":"Key Default Description Since celeborn.columnarShuffle.batch.size 10000 Vector batch size for columnar shuffle. 0.3.0 celeborn.columnarShuffle.codegen.enabled false Whether to use codegen for columnar-based shuffle. 0.3.0 celeborn.columnarShuffle.enabled false Whether to enable columnar-based shuffle. 0.2.0 celeborn.columnarShuffle.encoding.dictionary.enabled false Whether to use dictionary encoding for columnar-based shuffle data. 0.3.0 celeborn.columnarShuffle.encoding.dictionary.maxFactor 0.3 Max factor for dictionary size. The max dictionary size is min(32.0 KiB, celeborn.columnarShuffle.batch.size * celeborn.columnar.shuffle.encoding.dictionary.maxFactor) . 0.3.0 celeborn.columnarShuffle.offHeap.enabled false Whether to use off heap columnar vector. 0.3.0","title":"Columnar Shuffle"},{"location":"configuration/#metrics","text":"Below metrics configuration both work for master and worker. Key Default Description Since celeborn.metrics.app.topDiskUsage.count 50 Size for top items about top disk usage applications list. 0.2.0 celeborn.metrics.app.topDiskUsage.interval 10min Time length for a window about top disk usage application list. 0.2.0 celeborn.metrics.app.topDiskUsage.windowSize 24 Window size about top disk usage application list. 0.2.0 celeborn.metrics.capacity 4096 The maximum number of metrics which a source can use to generate output strings. 0.2.0 celeborn.metrics.collectPerfCritical.enabled false It controls whether to collect metrics which may affect performance. When enable, Celeborn collects them. 0.2.0 celeborn.metrics.conf <undefined> Custom metrics configuration file path. Default use metrics.properties in classpath. 0.3.0 celeborn.metrics.enabled true When true, enable metrics system. 0.2.0 celeborn.metrics.extraLabels If default metric labels are not enough, extra metric labels can be customized. Labels' pattern is: <label1_key>=<label1_value>[,<label2_key>=<label2_value>]* ; e.g. env=prod,version=1 0.3.0 celeborn.metrics.master.prometheus.host <localhost> Master's Prometheus host. 0.3.0 celeborn.metrics.master.prometheus.port 9098 Master's Prometheus port. 0.3.0 celeborn.metrics.sample.rate 1.0 It controls if Celeborn collect timer metrics for some operations. Its value should be in [0.0, 1.0]. 0.2.0 celeborn.metrics.timer.slidingWindow.size 4096 The sliding window size of timer metric. 0.2.0 celeborn.metrics.worker.pauseSpentTime.forceAppend.threshold 10 Force append worker pause spent time even if worker still in pause serving state.Help user can find worker pause spent time increase, when worker always been pause state. celeborn.metrics.worker.prometheus.host <localhost> Worker's Prometheus host. 0.3.0 celeborn.metrics.worker.prometheus.port 9096 Worker's Prometheus port. 0.3.0","title":"Metrics"},{"location":"configuration/#metricsproperties","text":"*.sink.csv.class = org.apache.celeborn.common.metrics.sink.CsvSink *.sink.prometheusServlet.class = org.apache.celeborn.common.metrics.sink.PrometheusServlet","title":"metrics.properties"},{"location":"configuration/#environment-variables_1","text":"Recommend configuring in conf/celeborn-env.sh . Key Default Description CELEBORN_HOME $(cd \"`dirname \"$0\"`\"/..; pwd) CELEBORN_CONF_DIR ${CELEBORN_CONF_DIR:-\"${CELEBORN_HOME}/conf\"} CELEBORN_MASTER_MEMORY 1 GB CELEBORN_WORKER_MEMORY 1 GB CELEBORN_WORKER_OFFHEAP_MEMORY 1 GB CELEBORN_MASTER_JAVA_OPTS CELEBORN_WORKER_JAVA_OPTS CELEBORN_PID_DIR ${CELEBORN_HOME}/pids CELEBORN_LOG_DIR ${CELEBORN_HOME}/logs CELEBORN_SSH_OPTS -o StrictHostKeyChecking=no CELEBORN_SLEEP Waiting time for start-all and stop-all operations CELEBORN_PREFER_JEMALLOC set true to enable jemalloc memory allocator CELEBORN_JEMALLOC_PATH jemalloc library path","title":"Environment Variables"},{"location":"configuration/#tuning","text":"Assume we have a cluster described as below: 5 Celeborn Workers with 20 GB off-heap memory and 10 disks. As we need to reserve 20% off-heap memory for netty, so we could assume 16 GB off-heap memory can be used for flush buffers. If spark.celeborn.client.push.buffer.max.size is 64 KB, we can have in-flight requests up to 1310720. If you have 8192 mapper tasks, you could set spark.celeborn.client.push.maxReqsInFlight=160 to gain performance improvements. If celeborn.worker.flusher.buffer.size is 256 KB, we can have total slots up to 327680 slots.","title":"Tuning"},{"location":"configuration/#rack-awareness","text":"Celeborn can be rack-aware by setting celeborn.client.reserveSlots.rackware.enabled to true on client side. Shuffle partition block replica placement will use rack awareness for fault tolerance by placing one shuffle partition replica on a different rack. This provides data availability in the event of a network switch failure or partition within the cluster. Celeborn master daemons obtain the rack id of the cluster workers by invoking either an external script or Java class as specified by configuration files. Using either the Java class or external script for topology, output must adhere to the java org.apache.hadoop.net.DNSToSwitchMapping interface. The interface expects a one-to-one correspondence to be maintained and the topology information in the format of /myrack/myhost , where / is the topology delimiter, myrack is the rack identifier, and myhost is the individual host. Assuming a single /24 subnet per rack, one could use the format of /192.168.100.0/192.168.100.5 as a unique rack-host topology mapping. To use the Java class for topology mapping, the class name is specified by the celeborn.hadoop.net.topology.node.switch.mapping.impl parameter in the master configuration file. An example, NetworkTopology.java , is included with the Celeborn distribution and can be customized by the Celeborn administrator. Using a Java class instead of an external script has a performance benefit in that Celeborn doesn't need to fork an external process when a new worker node registers itself. If implementing an external script, it will be specified with the celeborn.hadoop.net.topology.script.file.name parameter in the master side configuration files. Unlike the Java class, the external topology script is not included with the Celeborn distribution and is provided by the administrator. Celeborn will send multiple IP addresses to ARGV when forking the topology script. The number of IP addresses sent to the topology script is controlled with celeborn.hadoop.net.topology.script.number.args and defaults to 100. If celeborn.hadoop.net.topology.script.number.args was changed to 1, a topology script would get forked for each IP submitted by workers. If celeborn.hadoop.net.topology.script.file.name or celeborn.hadoop.net.topology.node.switch.mapping.impl is not set, the rack id /default-rack is returned for any passed IP address. While this behavior appears desirable, it can cause issues with shuffle partition block replication as default behavior is to write one replicated block off rack and is unable to do so as there is only a single rack named /default-rack . Example can refer to Hadoop Rack Awareness since Celeborn use hadoop's code about rack-aware.","title":"Rack Awareness"},{"location":"configuration/#worker-recover-status-after-restart","text":"ShuffleClient records the shuffle partition location's host, service port, and filename, to support workers recovering reading existing shuffle data after worker restart, during worker shutdown, workers should store the meta about reading shuffle partition files in LevelDB, and restore the meta after restarting workers, also workers should keep a stable service port to support ShuffleClient retry reading data. Users should set celeborn.worker.graceful.shutdown.enabled to true and set below service port with stable port to support worker recover status. celeborn.worker.rpc.port celeborn.worker.fetch.port celeborn.worker.push.port celeborn.worker.replicate.port","title":"Worker Recover Status After Restart"},{"location":"configuration/client/","text":"Key Default Description Since celeborn.client.application.heartbeatInterval 10s Interval for client to send heartbeat message to master. 0.3.0 celeborn.client.application.unregister.enabled true When true, Celeborn client will inform celeborn master the application is already shutdown during client exit, this allows the cluster to release resources immediately, resulting in resource savings. 0.3.2 celeborn.client.closeIdleConnections true Whether client will close idle connections. 0.3.0 celeborn.client.commitFiles.ignoreExcludedWorker false When true, LifecycleManager will skip workers which are in the excluded list. 0.3.0 celeborn.client.eagerlyCreateInputStream.threads 32 Threads count for streamCreatorPool in CelebornShuffleReader. 0.3.1 celeborn.client.excludePeerWorkerOnFailure.enabled true When true, Celeborn will exclude partition's peer worker on failure when push data to replica failed. 0.3.0 celeborn.client.excludedWorker.expireTimeout 180s Timeout time for LifecycleManager to clear reserved excluded worker. Default to be 1.5 * celeborn.master.heartbeat.worker.timeout to cover worker heartbeat timeout check period 0.3.0 celeborn.client.fetch.dfsReadChunkSize 8m Max chunk size for DfsPartitionReader. 0.3.1 celeborn.client.fetch.excludeWorkerOnFailure.enabled false Whether to enable shuffle client-side fetch exclude workers on failure. 0.3.0 celeborn.client.fetch.excludedWorker.expireTimeout <value of celeborn.client.excludedWorker.expireTimeout> ShuffleClient is a static object, it will be used in the whole lifecycle of Executor,We give a expire time for excluded workers to avoid a transient worker issues. 0.3.0 celeborn.client.fetch.maxReqsInFlight 3 Amount of in-flight chunk fetch request. 0.3.0 celeborn.client.fetch.maxRetriesForEachReplica 3 Max retry times of fetch chunk on each replica 0.3.0 celeborn.client.fetch.timeout 600s Timeout for a task to open stream and fetch chunk. 0.3.0 celeborn.client.flink.compression.enabled true Whether to compress data in Flink plugin. 0.3.0 celeborn.client.flink.inputGate.concurrentReadings 2147483647 Max concurrent reading channels for a input gate. 0.3.0 celeborn.client.flink.inputGate.memory 32m Memory reserved for a input gate. 0.3.0 celeborn.client.flink.inputGate.minMemory 8m Min memory reserved for a input gate. 0.3.0 celeborn.client.flink.inputGate.supportFloatingBuffer true Whether to support floating buffer in Flink input gates. 0.3.0 celeborn.client.flink.resultPartition.memory 64m Memory reserved for a result partition. 0.3.0 celeborn.client.flink.resultPartition.minMemory 8m Min memory reserved for a result partition. 0.3.0 celeborn.client.flink.resultPartition.supportFloatingBuffer true Whether to support floating buffer for result partitions. 0.3.0 celeborn.client.push.buffer.initial.size 8k 0.3.0 celeborn.client.push.buffer.max.size 64k Max size of reducer partition buffer memory for shuffle hash writer. The pushed data will be buffered in memory before sending to Celeborn worker. For performance consideration keep this buffer size higher than 32K. Example: If reducer amount is 2000, buffer size is 64K, then each task will consume up to 64KiB * 2000 = 125MiB heap memory. 0.3.0 celeborn.client.push.excludeWorkerOnFailure.enabled false Whether to enable shuffle client-side push exclude workers on failures. 0.3.0 celeborn.client.push.limit.inFlight.sleepInterval 50ms Sleep interval when check netty in-flight requests to be done. 0.3.0 celeborn.client.push.limit.inFlight.timeout <undefined> Timeout for netty in-flight requests to be done.Default value should be celeborn.client.push.timeout * 2 . 0.3.0 celeborn.client.push.limit.strategy SIMPLE The strategy used to control the push speed. Valid strategies are SIMPLE and SLOWSTART. The SLOWSTART strategy usually works with congestion control mechanism on the worker side. 0.3.0 celeborn.client.push.maxReqsInFlight.perWorker 32 Amount of Netty in-flight requests per worker. Default max memory of in flight requests per worker is celeborn.client.push.maxReqsInFlight.perWorker * celeborn.client.push.buffer.max.size * compression ratio(1 in worst case): 64KiB * 32 = 2MiB. The maximum memory will not exceed celeborn.client.push.maxReqsInFlight.total . 0.3.0 celeborn.client.push.maxReqsInFlight.total 256 Amount of total Netty in-flight requests. The maximum memory is celeborn.client.push.maxReqsInFlight.total * celeborn.client.push.buffer.max.size * compression ratio(1 in worst case): 64KiB * 256 = 16MiB 0.3.0 celeborn.client.push.queue.capacity 512 Push buffer queue size for a task. The maximum memory is celeborn.client.push.buffer.max.size * celeborn.client.push.queue.capacity , default: 64KiB * 512 = 32MiB 0.3.0 celeborn.client.push.replicate.enabled false When true, Celeborn worker will replicate shuffle data to another Celeborn worker asynchronously to ensure the pushed shuffle data won't be lost after the node failure. It's recommended to set false when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.client.push.retry.threads 8 Thread number to process shuffle re-send push data requests. 0.3.0 celeborn.client.push.revive.batchSize 2048 Max number of partitions in one Revive request. 0.3.0 celeborn.client.push.revive.interval 100ms Interval for client to trigger Revive to LifecycleManager. The number of partitions in one Revive request is celeborn.client.push.revive.batchSize . 0.3.0 celeborn.client.push.revive.maxRetries 5 Max retry times for reviving when celeborn push data failed. 0.3.0 celeborn.client.push.sendBufferPool.checkExpireInterval 30s Interval to check expire for send buffer pool. If the pool has been idle for more than celeborn.client.push.sendBufferPool.expireTimeout , the pooled send buffers and push tasks will be cleaned up. 0.3.1 celeborn.client.push.sendBufferPool.expireTimeout 60s Timeout before clean up SendBufferPool. If SendBufferPool is idle for more than this time, the send buffers and push tasks will be cleaned up. 0.3.1 celeborn.client.push.slowStart.initialSleepTime 500ms The initial sleep time if the current max in flight requests is 0 0.3.0 celeborn.client.push.slowStart.maxSleepTime 2s If celeborn.client.push.limit.strategy is set to SLOWSTART, push side will take a sleep strategy for each batch of requests, this controls the max sleep time if the max in flight requests limit is 1 for a long time 0.3.0 celeborn.client.push.sort.randomizePartitionId.enabled false Whether to randomize partitionId in push sorter. If true, partitionId will be randomized when sort data to avoid skew when push to worker 0.3.0 celeborn.client.push.splitPartition.threads 8 Thread number to process shuffle split request in shuffle client. 0.3.0 celeborn.client.push.stageEnd.timeout <value of celeborn.<module>.io.connectionTimeout> Timeout for waiting StageEnd. During this process, there are celeborn.client.requestCommitFiles.maxRetries times for retry opportunities for committing files and 1 times for releasing slots request. User can customize this value according to your setting. By default, the value is the max timeout value celeborn.<module>.io.connectionTimeout . 0.3.0 celeborn.client.push.takeTaskMaxWaitAttempts 1 Max wait times if no task available to push to worker. 0.3.0 celeborn.client.push.takeTaskWaitInterval 50ms Wait interval if no task available to push to worker. 0.3.0 celeborn.client.push.timeout 120s Timeout for a task to push data rpc message. This value should better be more than twice of celeborn.<module>.push.timeoutCheck.interval 0.3.0 celeborn.client.readLocalShuffleFile.enabled false Enable read local shuffle file for clusters that co-deployed with yarn node manager. 0.3.1 celeborn.client.readLocalShuffleFile.threads 4 Threads count for read local shuffle file. 0.3.1 celeborn.client.registerShuffle.maxRetries 3 Max retry times for client to register shuffle. 0.3.0 celeborn.client.registerShuffle.retryWait 3s Wait time before next retry if register shuffle failed. 0.3.0 celeborn.client.requestCommitFiles.maxRetries 4 Max retry times for requestCommitFiles RPC. 0.3.0 celeborn.client.reserveSlots.maxRetries 3 Max retry times for client to reserve slots. 0.3.0 celeborn.client.reserveSlots.rackaware.enabled false Whether need to place different replicates on different racks when allocating slots. 0.3.1 celeborn.client.reserveSlots.retryWait 3s Wait time before next retry if reserve slots failed. 0.3.0 celeborn.client.rpc.cache.concurrencyLevel 32 The number of write locks to update rpc cache. 0.3.0 celeborn.client.rpc.cache.expireTime 15s The time before a cache item is removed. 0.3.0 celeborn.client.rpc.cache.size 256 The max cache items count for rpc cache. 0.3.0 celeborn.client.rpc.getReducerFileGroup.askTimeout <value of celeborn.<module>.io.connectionTimeout> Timeout for ask operations during getting reducer file group information. During this process, there are celeborn.client.requestCommitFiles.maxRetries times for retry opportunities for committing files and 1 times for releasing slots request. User can customize this value according to your setting. By default, the value is the max timeout value celeborn.<module>.io.connectionTimeout . 0.2.0 celeborn.client.rpc.maxRetries 3 Max RPC retry times in LifecycleManager. 0.3.2 celeborn.client.rpc.registerShuffle.askTimeout <value of celeborn.<module>.io.connectionTimeout> Timeout for ask operations during register shuffle. During this process, there are two times for retry opportunities for requesting slots, one request for establishing a connection with Worker and celeborn.client.reserveSlots.maxRetries times for retry opportunities for reserving slots. User can customize this value according to your setting. By default, the value is the max timeout value celeborn.<module>.io.connectionTimeout . 0.3.0 celeborn.client.rpc.requestPartition.askTimeout <value of celeborn.<module>.io.connectionTimeout> Timeout for ask operations during requesting change partition location, such as reviving or splitting partition. During this process, there are celeborn.client.reserveSlots.maxRetries times for retry opportunities for reserving slots. User can customize this value according to your setting. By default, the value is the max timeout value celeborn.<module>.io.connectionTimeout . 0.2.0 celeborn.client.rpc.reserveSlots.askTimeout <value of celeborn.rpc.askTimeout> Timeout for LifecycleManager request reserve slots. 0.3.0 celeborn.client.rpc.shared.threads 16 Number of shared rpc threads in LifecycleManager. 0.3.2 celeborn.client.shuffle.batchHandleChangePartition.interval 100ms Interval for LifecycleManager to schedule handling change partition requests in batch. 0.3.0 celeborn.client.shuffle.batchHandleChangePartition.threads 8 Threads number for LifecycleManager to handle change partition request in batch. 0.3.0 celeborn.client.shuffle.batchHandleCommitPartition.interval 5s Interval for LifecycleManager to schedule handling commit partition requests in batch. 0.3.0 celeborn.client.shuffle.batchHandleCommitPartition.threads 8 Threads number for LifecycleManager to handle commit partition request in batch. 0.3.0 celeborn.client.shuffle.batchHandleReleasePartition.interval 5s Interval for LifecycleManager to schedule handling release partition requests in batch. 0.3.0 celeborn.client.shuffle.batchHandleReleasePartition.threads 8 Threads number for LifecycleManager to handle release partition request in batch. 0.3.0 celeborn.client.shuffle.compression.codec LZ4 The codec used to compress shuffle data. By default, Celeborn provides three codecs: lz4 , zstd , none . 0.3.0 celeborn.client.shuffle.compression.zstd.level 1 Compression level for Zstd compression codec, its value should be an integer between -5 and 22. Increasing the compression level will result in better compression at the expense of more CPU and memory. 0.3.0 celeborn.client.shuffle.expired.checkInterval 60s Interval for client to check expired shuffles. 0.3.0 celeborn.client.shuffle.manager.port 0 Port used by the LifecycleManager on the Driver. 0.3.0 celeborn.client.shuffle.mapPartition.split.enabled false whether to enable shuffle partition split. Currently, this only applies to MapPartition. 0.3.1 celeborn.client.shuffle.partition.type REDUCE Type of shuffle's partition. 0.3.0 celeborn.client.shuffle.partitionSplit.mode SOFT soft: the shuffle file size might be larger than split threshold. hard: the shuffle file size will be limited to split threshold. 0.3.0 celeborn.client.shuffle.partitionSplit.threshold 1G Shuffle file size threshold, if file size exceeds this, trigger split. 0.3.0 celeborn.client.shuffle.rangeReadFilter.enabled false If a spark application have skewed partition, this value can set to true to improve performance. 0.2.0 celeborn.client.slot.assign.maxWorkers 10000 Max workers that slots of one shuffle can be allocated on. Will choose the smaller positive one from Master side and Client side, see celeborn.master.slot.assign.maxWorkers . 0.3.1 celeborn.client.spark.push.sort.memory.threshold 64m When SortBasedPusher use memory over the threshold, will trigger push data. If the pipeline push feature is enabled ( celeborn.client.spark.push.sort.pipeline.enabled=true ), the SortBasedPusher will trigger a data push when the memory usage exceeds half of the threshold(by default, 32m). 0.3.0 celeborn.client.spark.push.sort.pipeline.enabled false Whether to enable pipelining for sort based shuffle writer. If true, double buffering will be used to pipeline push 0.3.0 celeborn.client.spark.push.unsafeRow.fastWrite.enabled true This is Celeborn's optimization on UnsafeRow for Spark and it's true by default. If you have changed UnsafeRow's memory layout set this to false. 0.2.2 celeborn.client.spark.shuffle.forceFallback.enabled false Whether force fallback shuffle to Spark's default. 0.3.0 celeborn.client.spark.shuffle.forceFallback.numPartitionsThreshold 500000 Celeborn will only accept shuffle of partition number lower than this configuration value. 0.3.0 celeborn.client.spark.shuffle.writer HASH Celeborn supports the following kind of shuffle writers. 1. hash: hash-based shuffle writer works fine when shuffle partition count is normal; 2. sort: sort-based shuffle writer works fine when memory pressure is high or shuffle partition count is huge. 0.3.0 celeborn.master.endpoints <localhost>:9097 Endpoints of master nodes for celeborn client to connect, allowed pattern is: <host1>:<port1>[,<host2>:<port2>]* , e.g. clb1:9097,clb2:9098,clb3:9099 . If the port is omitted, 9097 will be used. 0.2.0 celeborn.storage.availableTypes HDD Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.hdfs.dir <undefined> HDFS base directory for Celeborn to store shuffle data. 0.2.0","title":"Client"},{"location":"configuration/columnar-shuffle/","text":"Key Default Description Since celeborn.columnarShuffle.batch.size 10000 Vector batch size for columnar shuffle. 0.3.0 celeborn.columnarShuffle.codegen.enabled false Whether to use codegen for columnar-based shuffle. 0.3.0 celeborn.columnarShuffle.enabled false Whether to enable columnar-based shuffle. 0.2.0 celeborn.columnarShuffle.encoding.dictionary.enabled false Whether to use dictionary encoding for columnar-based shuffle data. 0.3.0 celeborn.columnarShuffle.encoding.dictionary.maxFactor 0.3 Max factor for dictionary size. The max dictionary size is min(32.0 KiB, celeborn.columnarShuffle.batch.size * celeborn.columnar.shuffle.encoding.dictionary.maxFactor) . 0.3.0 celeborn.columnarShuffle.offHeap.enabled false Whether to use off heap columnar vector. 0.3.0","title":"Columnar shuffle"},{"location":"configuration/ha/","text":"Key Default Description Since celeborn.master.ha.enabled false When true, master nodes run as Raft cluster mode. 0.3.0 celeborn.master.ha.node.<id>.host <required> Host to bind of master node in HA mode. 0.3.0 celeborn.master.ha.node.<id>.port 9097 Port to bind of master node in HA mode. 0.3.0 celeborn.master.ha.node.<id>.ratis.port 9872 Ratis port to bind of master node in HA mode. 0.3.0 celeborn.master.ha.ratis.raft.rpc.type netty RPC type for Ratis, available options: netty, grpc. 0.3.0 celeborn.master.ha.ratis.raft.server.storage.dir /tmp/ratis 0.3.0","title":"Ha"},{"location":"configuration/master/","text":"Key Default Description Since celeborn.master.estimatedPartitionSize.initialSize 64mb Initial partition size for estimation, it will change according to runtime stats. 0.3.0 celeborn.master.estimatedPartitionSize.update.initialDelay 5min Initial delay time before start updating partition size for estimation. 0.3.0 celeborn.master.estimatedPartitionSize.update.interval 10min Interval of updating partition size for estimation. 0.3.0 celeborn.master.hdfs.expireDirs.timeout 1h The timeout for a expire dirs to be deleted on HDFS. 0.3.0 celeborn.master.heartbeat.application.timeout 300s Application heartbeat timeout. 0.3.0 celeborn.master.heartbeat.worker.timeout 120s Worker heartbeat timeout. 0.3.0 celeborn.master.host <localhost> Hostname for master to bind. 0.2.0 celeborn.master.port 9097 Port for master to bind. 0.2.0 celeborn.master.slot.assign.extraSlots 2 Extra slots number when master assign slots. 0.3.0 celeborn.master.slot.assign.loadAware.diskGroupGradient 0.1 This value means how many more workload will be placed into a faster disk group than a slower group. 0.3.0 celeborn.master.slot.assign.loadAware.fetchTimeWeight 1.0 Weight of average fetch time when calculating ordering in load-aware assignment strategy 0.3.0 celeborn.master.slot.assign.loadAware.flushTimeWeight 0.0 Weight of average flush time when calculating ordering in load-aware assignment strategy 0.3.0 celeborn.master.slot.assign.loadAware.numDiskGroups 5 This configuration is a guidance for load-aware slot allocation algorithm. This value is control how many disk groups will be created. 0.3.0 celeborn.master.slot.assign.maxWorkers 10000 Max workers that slots of one shuffle can be allocated on. Will choose the smaller positive one from Master side and Client side, see celeborn.client.slot.assign.maxWorkers . 0.3.1 celeborn.master.slot.assign.policy ROUNDROBIN Policy for master to assign slots, Celeborn supports two types of policy: roundrobin and loadaware. Loadaware policy will be ignored when HDFS is enabled in celeborn.storage.activeTypes 0.3.0 celeborn.master.userResourceConsumption.update.interval 30s Time length for a window about compute user resource consumption. 0.3.0 celeborn.master.workerUnavailableInfo.expireTimeout 1800s Worker unavailable info would be cleared when the retention period is expired 0.3.1 celeborn.storage.availableTypes HDD Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.hdfs.dir <undefined> HDFS base directory for Celeborn to store shuffle data. 0.2.0 celeborn.storage.hdfs.kerberos.keytab <undefined> Kerberos keytab file path for HDFS storage connection. 0.3.2 celeborn.storage.hdfs.kerberos.principal <undefined> Kerberos principal for HDFS storage connection. 0.3.2","title":"Master"},{"location":"configuration/metrics/","text":"Key Default Description Since celeborn.metrics.app.topDiskUsage.count 50 Size for top items about top disk usage applications list. 0.2.0 celeborn.metrics.app.topDiskUsage.interval 10min Time length for a window about top disk usage application list. 0.2.0 celeborn.metrics.app.topDiskUsage.windowSize 24 Window size about top disk usage application list. 0.2.0 celeborn.metrics.capacity 4096 The maximum number of metrics which a source can use to generate output strings. 0.2.0 celeborn.metrics.collectPerfCritical.enabled false It controls whether to collect metrics which may affect performance. When enable, Celeborn collects them. 0.2.0 celeborn.metrics.conf <undefined> Custom metrics configuration file path. Default use metrics.properties in classpath. 0.3.0 celeborn.metrics.enabled true When true, enable metrics system. 0.2.0 celeborn.metrics.extraLabels If default metric labels are not enough, extra metric labels can be customized. Labels' pattern is: <label1_key>=<label1_value>[,<label2_key>=<label2_value>]* ; e.g. env=prod,version=1 0.3.0 celeborn.metrics.master.prometheus.host <localhost> Master's Prometheus host. 0.3.0 celeborn.metrics.master.prometheus.port 9098 Master's Prometheus port. 0.3.0 celeborn.metrics.sample.rate 1.0 It controls if Celeborn collect timer metrics for some operations. Its value should be in [0.0, 1.0]. 0.2.0 celeborn.metrics.timer.slidingWindow.size 4096 The sliding window size of timer metric. 0.2.0 celeborn.metrics.worker.pauseSpentTime.forceAppend.threshold 10 Force append worker pause spent time even if worker still in pause serving state.Help user can find worker pause spent time increase, when worker always been pause state. celeborn.metrics.worker.prometheus.host <localhost> Worker's Prometheus host. 0.3.0 celeborn.metrics.worker.prometheus.port 9096 Worker's Prometheus port. 0.3.0","title":"Metrics"},{"location":"configuration/network/","text":"Key Default Description Since celeborn.<module>.fetch.timeoutCheck.interval 5s Interval for checking fetch data timeout. It only support setting to data since it works for shuffle client fetch data and should be configured on client side. 0.3.0 celeborn.<module>.fetch.timeoutCheck.threads 4 Threads num for checking fetch data timeout. It only support setting to data since it works for shuffle client fetch data and should be configured on client side. 0.3.0 celeborn.<module>.heartbeat.interval 60s The heartbeat interval between worker and client. If setting to data , it works for shuffle client push and fetch data and should be configured on client side. If setting to replicate , it works for worker replicate data to peer worker and should be configured on worker side. 0.3.0 celeborn.<module>.io.backLog 0 Requested maximum length of the queue of incoming connections. Default 0 for no backlog. celeborn.<module>.io.clientThreads 0 Number of threads used in the client thread pool. Default to 0, which is 2x#cores. celeborn.<module>.io.connectTimeout <value of celeborn.network.connect.timeout> Socket connect timeout. celeborn.<module>.io.connectionTimeout <value of celeborn.network.timeout> Connection active timeout. celeborn.<module>.io.enableVerboseMetrics false Whether to track Netty memory detailed metrics. If true, the detailed metrics of Netty PoolByteBufAllocator will be gotten, otherwise only general memory usage will be tracked. celeborn.<module>.io.lazyFD true Whether to initialize FileDescriptor lazily or not. If true, file descriptors are created only when data is going to be transferred. This can reduce the number of open files. celeborn.<module>.io.maxRetries 3 Max number of times we will try IO exceptions (such as connection timeouts) per request. If set to 0, we will not do any retries. celeborn.<module>.io.mode NIO Netty EventLoopGroup backend, available options: NIO, EPOLL. celeborn.<module>.io.numConnectionsPerPeer 2 Number of concurrent connections between two nodes. celeborn.<module>.io.preferDirectBufs true If true, we will prefer allocating off-heap byte buffers within Netty. celeborn.<module>.io.receiveBuffer 0b Receive buffer size (SO_RCVBUF). Note: the optimal size for receive buffer and send buffer should be latency * network_bandwidth. Assuming latency = 1ms, network_bandwidth = 10Gbps buffer size should be ~ 1.25MB. 0.2.0 celeborn.<module>.io.retryWait 5s Time that we will wait in order to perform a retry after an IOException. Only relevant if maxIORetries > 0. 0.2.0 celeborn.<module>.io.sendBuffer 0b Send buffer size (SO_SNDBUF). 0.2.0 celeborn.<module>.io.serverThreads 0 Number of threads used in the server thread pool. Default to 0, which is 2x#cores. celeborn.<module>.push.timeoutCheck.interval 5s Interval for checking push data timeout. If setting to data , it works for shuffle client push data and should be configured on client side. If setting to replicate , it works for worker replicate data to peer worker and should be configured on worker side. 0.3.0 celeborn.<module>.push.timeoutCheck.threads 4 Threads num for checking push data timeout. If setting to data , it works for shuffle client push data and should be configured on client side. If setting to replicate , it works for worker replicate data to peer worker and should be configured on worker side. 0.3.0 celeborn.<role>.rpc.dispatcher.threads <value of celeborn.rpc.dispatcher.threads> Threads number of message dispatcher event loop for roles celeborn.io.maxDefaultNettyThreads 64 Max default netty threads 0.3.2 celeborn.network.bind.preferIpAddress true When ture , prefer to use IP address, otherwise FQDN. This configuration only takes effects when the bind hostname is not set explicitly, in such case, Celeborn will find the first non-loopback address to bind. 0.3.0 celeborn.network.connect.timeout 10s Default socket connect timeout. 0.2.0 celeborn.network.memory.allocator.numArenas <undefined> Number of arenas for pooled memory allocator. Default value is Runtime.getRuntime.availableProcessors, min value is 2. 0.3.0 celeborn.network.memory.allocator.verbose.metric false Weather to enable verbose metric for pooled allocator. 0.3.0 celeborn.network.timeout 240s Default timeout for network operations. 0.2.0 celeborn.port.maxRetries 1 When port is occupied, we will retry for max retry times. 0.2.0 celeborn.rpc.askTimeout 60s Timeout for RPC ask operations. It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes 0.2.0 celeborn.rpc.connect.threads 64 0.2.0 celeborn.rpc.dispatcher.threads 0 Threads number of message dispatcher event loop. Default to 0, which is availableCore. 0.3.0 celeborn.rpc.io.threads <undefined> Netty IO thread number of NettyRpcEnv to handle RPC request. The default threads number is the number of runtime available processors. 0.2.0 celeborn.rpc.lookupTimeout 30s Timeout for RPC lookup operations. 0.2.0 celeborn.shuffle.io.maxChunksBeingTransferred <undefined> The max number of chunks allowed to be transferred at the same time on shuffle service. Note that new incoming connections will be closed when the max number is hit. The client will retry according to the shuffle retry configs (see celeborn.<module>.io.maxRetries and celeborn.<module>.io.retryWait ), if those limits are reached the task will fail with fetch failure. 0.2.0","title":"Network"},{"location":"configuration/quota/","text":"Key Default Description Since celeborn.quota.configuration.path <undefined> Quota configuration file path. The file format should be yaml. Quota configuration file template can be found under conf directory. 0.2.0 celeborn.quota.enabled true When true, before registering shuffle, LifecycleManager should check if current user have enough quota space, if cluster don't have enough quota space for current user, fallback to Spark's default shuffle 0.2.0 celeborn.quota.identity.provider org.apache.celeborn.common.identity.DefaultIdentityProvider IdentityProvider class name. Default class is org.apache.celeborn.common.identity.DefaultIdentityProvider . Optional values: org.apache.celeborn.common.identity.HadoopBasedIdentityProvider user name will be obtained by UserGroupInformation.getUserName; org.apache.celeborn.common.identity.DefaultIdentityProvider user name and tenant id are default values or user-specific values. 0.2.0 celeborn.quota.identity.user-specific.tenant default Tenant id if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.quota.identity.user-specific.userName default User name if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.quota.manager org.apache.celeborn.common.quota.DefaultQuotaManager QuotaManger class name. Default class is org.apache.celeborn.common.quota.DefaultQuotaManager . 0.2.0","title":"Quota"},{"location":"configuration/worker/","text":"Key Default Description Since celeborn.master.endpoints <localhost>:9097 Endpoints of master nodes for celeborn client to connect, allowed pattern is: <host1>:<port1>[,<host2>:<port2>]* , e.g. clb1:9097,clb2:9098,clb3:9099 . If the port is omitted, 9097 will be used. 0.2.0 celeborn.master.estimatedPartitionSize.minSize 8mb Ignore partition size smaller than this configuration of partition size for estimation. 0.3.0 celeborn.shuffle.chunk.size 8m Max chunk size of reducer's merged shuffle data. For example, if a reducer's shuffle data is 128M and the data will need 16 fetch chunk requests to fetch. 0.2.0 celeborn.storage.availableTypes HDD Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.hdfs.dir <undefined> HDFS base directory for Celeborn to store shuffle data. 0.2.0 celeborn.storage.hdfs.kerberos.keytab <undefined> Kerberos keytab file path for HDFS storage connection. 0.3.2 celeborn.storage.hdfs.kerberos.principal <undefined> Kerberos principal for HDFS storage connection. 0.3.2 celeborn.worker.activeConnection.max <undefined> If the number of active connections on a worker exceeds this configuration value, the worker will be marked as high-load in the heartbeat report, and the master will not include that node in the response of RequestSlots. 0.3.1 celeborn.worker.bufferStream.threadsPerMountpoint 8 Threads count for read buffer per mount point. 0.3.0 celeborn.worker.clean.threads 64 Thread number of worker to clean up expired shuffle keys. 0.3.2 celeborn.worker.closeIdleConnections false Whether worker will close idle connections. 0.2.0 celeborn.worker.commitFiles.threads 32 Thread number of worker to commit shuffle data files asynchronously. It's recommended to set at least 128 when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.commitFiles.timeout 120s Timeout for a Celeborn worker to commit files of a shuffle. It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.congestionControl.check.interval 10ms Interval of worker checks congestion if celeborn.worker.congestionControl.enabled is true. 0.3.2 celeborn.worker.congestionControl.enabled false Whether to enable congestion control or not. 0.3.0 celeborn.worker.congestionControl.high.watermark <undefined> If the total bytes in disk buffer exceeds this configure, will start to congestusers whose produce rate is higher than the potential average consume rate. The congestion will stop if the produce rate is lower or equal to the average consume rate, or the total pending bytes lower than celeborn.worker.congestionControl.low.watermark 0.3.0 celeborn.worker.congestionControl.low.watermark <undefined> Will stop congest users if the total pending bytes of disk buffer is lower than this configuration 0.3.0 celeborn.worker.congestionControl.sample.time.window 10s The worker holds a time sliding list to calculate users' produce/consume rate 0.3.0 celeborn.worker.congestionControl.user.inactive.interval 10min How long will consider this user is inactive if it doesn't send data 0.3.0 celeborn.worker.decommission.checkInterval 30s The wait interval of checking whether all the shuffle expired during worker decommission 0.4.0 celeborn.worker.decommission.forceExitTimeout 6h The wait time of waiting for all the shuffle expire during worker decommission. 0.4.0 celeborn.worker.directMemoryRatioForMemoryShuffleStorage 0.0 Max ratio of direct memory to store shuffle data 0.2.0 celeborn.worker.directMemoryRatioForReadBuffer 0.1 Max ratio of direct memory for read buffer 0.2.0 celeborn.worker.directMemoryRatioToPauseReceive 0.85 If direct memory usage reaches this limit, the worker will stop to receive data from Celeborn shuffle clients. 0.2.0 celeborn.worker.directMemoryRatioToPauseReplicate 0.95 If direct memory usage reaches this limit, the worker will stop to receive replication data from other workers. This value should be higher than celeborn.worker.directMemoryRatioToPauseReceive. 0.2.0 celeborn.worker.directMemoryRatioToResume 0.7 If direct memory usage is less than this limit, worker will resume. 0.2.0 celeborn.worker.disk.clean.threads 4 Thread number of worker to clean up directories of expired shuffle keys on disk. 0.3.2 celeborn.worker.fetch.heartbeat.enabled false enable the heartbeat from worker to client when fetching data 0.3.0 celeborn.worker.fetch.io.threads <undefined> Netty IO thread number of worker to handle client fetch data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.fetch.port 0 Server port for Worker to receive fetch data request from ShuffleClient. 0.2.0 celeborn.worker.flusher.buffer.size 256k Size of buffer used by a single flusher. 0.2.0 celeborn.worker.flusher.diskTime.slidingWindow.size 20 The size of sliding windows used to calculate statistics about flushed time and count. 0.3.0 celeborn.worker.flusher.hdd.threads 1 Flusher's thread count per disk used for write data to HDD disks. 0.2.0 celeborn.worker.flusher.hdfs.buffer.size 4m Size of buffer used by a HDFS flusher. 0.3.0 celeborn.worker.flusher.hdfs.threads 8 Flusher's thread count used for write data to HDFS. 0.2.0 celeborn.worker.flusher.shutdownTimeout 3s Timeout for a flusher to shutdown. 0.2.0 celeborn.worker.flusher.ssd.threads 16 Flusher's thread count per disk used for write data to SSD disks. 0.2.0 celeborn.worker.flusher.threads 16 Flusher's thread count per disk for unknown-type disks. 0.2.0 celeborn.worker.graceful.shutdown.checkSlotsFinished.interval 1s The wait interval of checking whether all released slots to be committed or destroyed during worker graceful shutdown 0.2.0 celeborn.worker.graceful.shutdown.checkSlotsFinished.timeout 480s The wait time of waiting for the released slots to be committed or destroyed during worker graceful shutdown. 0.2.0 celeborn.worker.graceful.shutdown.enabled false When true, during worker shutdown, the worker will wait for all released slots to be committed or destroyed. 0.2.0 celeborn.worker.graceful.shutdown.partitionSorter.shutdownTimeout 120s The wait time of waiting for sorting partition files during worker graceful shutdown. 0.2.0 celeborn.worker.graceful.shutdown.recoverPath <tmp>/recover The path to store levelDB. 0.2.0 celeborn.worker.graceful.shutdown.saveCommittedFileInfo.interval 5s Interval for a Celeborn worker to flush committed file infos into Level DB. 0.3.1 celeborn.worker.graceful.shutdown.saveCommittedFileInfo.sync false Whether to call sync method to save committed file infos into Level DB to handle OS crash. 0.3.1 celeborn.worker.graceful.shutdown.timeout 600s The worker's graceful shutdown timeout time. 0.2.0 celeborn.worker.monitor.disk.check.interval 30s Intervals between device monitor to check disk. 0.3.0 celeborn.worker.monitor.disk.check.timeout 30s Timeout time for worker check device status. 0.3.0 celeborn.worker.monitor.disk.checklist readwrite,diskusage Monitor type for disk, available items are: iohang, readwrite and diskusage. 0.2.0 celeborn.worker.monitor.disk.enabled true When true, worker will monitor device and report to master. 0.3.0 celeborn.worker.monitor.disk.notifyError.expireTimeout 10m The expire timeout of non-critical device error. Only notify critical error when the number of non-critical errors for a period of time exceeds threshold. 0.3.0 celeborn.worker.monitor.disk.notifyError.threshold 64 Device monitor will only notify critical error once the accumulated valid non-critical error number exceeding this threshold. 0.3.0 celeborn.worker.monitor.disk.sys.block.dir /sys/block The directory where linux file block information is stored. 0.2.0 celeborn.worker.monitor.memory.check.interval 10ms Interval of worker direct memory checking. 0.3.0 celeborn.worker.monitor.memory.report.interval 10s Interval of worker direct memory tracker reporting to log. 0.3.0 celeborn.worker.monitor.memory.trimChannelWaitInterval 1s Wait time after worker trigger channel to trim cache. 0.3.0 celeborn.worker.monitor.memory.trimFlushWaitInterval 1s Wait time after worker trigger StorageManger to flush data. 0.3.0 celeborn.worker.partition.initial.readBuffersMax 1024 Max number of initial read buffers 0.3.0 celeborn.worker.partition.initial.readBuffersMin 1 Min number of initial read buffers 0.3.0 celeborn.worker.partitionSorter.directMemoryRatioThreshold 0.1 Max ratio of partition sorter's memory for sorting, when reserved memory is higher than max partition sorter memory, partition sorter will stop sorting. 0.2.0 celeborn.worker.push.heartbeat.enabled false enable the heartbeat from worker to client when pushing data 0.3.0 celeborn.worker.push.io.threads <undefined> Netty IO thread number of worker to handle client push data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.push.port 0 Server port for Worker to receive push data request from ShuffleClient. 0.2.0 celeborn.worker.readBuffer.allocationWait 50ms The time to wait when buffer dispatcher can not allocate a buffer. 0.3.0 celeborn.worker.readBuffer.target.changeThreshold 1mb The target ratio for pre read memory usage. 0.3.0 celeborn.worker.readBuffer.target.ratio 0.9 The target ratio for read ahead buffer's memory usage. 0.3.0 celeborn.worker.readBuffer.target.updateInterval 100ms The interval for memory manager to calculate new read buffer's target memory. 0.3.0 celeborn.worker.readBuffer.toTriggerReadMin 32 Min buffers count for map data partition to trigger read. 0.3.0 celeborn.worker.register.timeout 180s Worker register timeout. 0.2.0 celeborn.worker.replicate.fastFail.duration 60s If a replicate request not replied during the duration, worker will mark the replicate data request as failed.It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes . 0.2.0 celeborn.worker.replicate.io.threads <undefined> Netty IO thread number of worker to replicate shuffle data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.replicate.port 0 Server port for Worker to receive replicate data request from other Workers. 0.2.0 celeborn.worker.replicate.randomConnection.enabled true Whether worker will create random connection to peer when replicate data. When false, worker tend to reuse the same cached TransportClient to a specific replicate worker; when true, worker tend to use different cached TransportClient. Netty will use the same thread to serve the same connection, so with more connections replicate server can leverage more netty threads 0.2.1 celeborn.worker.replicate.threads 64 Thread number of worker to replicate shuffle data. 0.2.0 celeborn.worker.rpc.port 0 Server port for Worker to receive RPC request. 0.2.0 celeborn.worker.shuffle.partitionSplit.enabled true enable the partition split on worker side 0.3.0 celeborn.worker.shuffle.partitionSplit.max 2g Specify the maximum partition size for splitting, and ensure that individual partition files are always smaller than this limit. 0.3.0 celeborn.worker.shuffle.partitionSplit.min 1m Min size for a partition to split 0.3.0 celeborn.worker.sortPartition.reservedMemoryPerPartition 1mb Reserved memory when sorting a shuffle file off-heap. 0.3.0 celeborn.worker.sortPartition.threads <undefined> PartitionSorter's thread counts. It's recommended to set at least 64 when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.sortPartition.timeout 220s Timeout for a shuffle file to sort. 0.3.0 celeborn.worker.storage.checkDirsEmpty.maxRetries 3 The number of retries for a worker to check if the working directory is cleaned up before registering with the master. 0.3.0 celeborn.worker.storage.checkDirsEmpty.timeout 1000ms The wait time per retry for a worker to check if the working directory is cleaned up before registering with the master. 0.3.0 celeborn.worker.storage.dirs <undefined> Directory list to store shuffle data. It's recommended to configure one directory on each disk. Storage size limit can be set for each directory. For the sake of performance, there should be no more than 2 flush threads on the same disk partition if you are using HDD, and should be 8 or more flush threads on the same disk partition if you are using SSD. For example: dir1[:capacity=][:disktype=][:flushthread=],dir2[:capacity=][:disktype=][:flushthread=] 0.2.0 celeborn.worker.storage.disk.reserve.ratio <undefined> Celeborn worker reserved ratio for each disk. The minimum usable size for each disk is the max space between the reserved space and the space calculate via reserved ratio. 0.3.2 celeborn.worker.storage.disk.reserve.size 5G Celeborn worker reserved space for each disk. 0.3.0 celeborn.worker.storage.expireDirs.timeout 1h The timeout for a expire dirs to be deleted on disk. 0.3.2 celeborn.worker.storage.workingDir celeborn-worker/shuffle_data Worker's working dir path name. 0.3.0 celeborn.worker.writer.close.timeout 120s Timeout for a file writer to close 0.2.0 celeborn.worker.writer.create.maxAttempts 3 Retry count for a file writer to create if its creation was failed. 0.2.0","title":"Worker"}]}