{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Quick Start This documentation gives a quick start guide for running Spark/Flink/MapReduce with Apache Celeborn\u2122. Download Celeborn Download the latest Celeborn binary from the Downloading Page . Decompress the binary and set $CELEBORN_HOME . tar -C <DST_DIR> -zxvf apache-celeborn-<VERSION>-bin.tgz export CELEBORN_HOME = <Decompressed path> Configure Logging and Storage Configure Logging cd $CELEBORN_HOME /conf cp log4j2.xml.template log4j2.xml Configure Storage Configure the directory to store shuffle data, for example $CELEBORN_HOME/shuffle . cd $CELEBORN_HOME /conf echo \"celeborn.worker.storage.dirs= $CELEBORN_HOME /shuffle\" > celeborn-defaults.conf Start Celeborn Service Start Master cd $CELEBORN_HOME ./sbin/start-master.sh You should see Master 's ip:port in the log: INFO [main] NettyRpcEnvFactory: Starting RPC Server [MasterSys] on 192.168.2.109:9097 with advisor endpoint 192.168.2.109:9097 Start Worker Use the Master's IP and Port to start Worker: cd $CELEBORN_HOME ./sbin/start-worker.sh celeborn://<Master IP>:<Master Port> You should see the following message in Worker's log: INFO [main] MasterClient: connect to master 192.168.2.109:9097. INFO [main] Worker: Register worker successfully. INFO [main] Worker: Worker started. And also the following message in Master's log: INFO [dispatcher-event-loop-9] Master: Registered worker Host: 192.168.2.109 RpcPort: 57806 PushPort: 57807 FetchPort: 57809 ReplicatePort: 57808 SlotsUsed: 0 LastHeartbeat: 0 HeartbeatElapsedSeconds: xxx Disks: DiskInfo0: xxx UserResourceConsumption: empty WorkerRef: null Start Spark with Celeborn Copy Celeborn Client to Spark's jars Celeborn release binary contains clients for Spark 2.x and Spark 3.x, copy the corresponding client jar into Spark's jars/ directory: cp $CELEBORN_HOME /spark/<Celeborn Client Jar> $SPARK_HOME /jars/ Start spark-shell Set spark.shuffle.manager to Celeborn's ShuffleManager, and turn off spark.shuffle.service.enabled : cd $SPARK_HOME ./bin/spark-shell \\ --conf spark.shuffle.manager = org.apache.spark.shuffle.celeborn.SparkShuffleManager \\ --conf spark.shuffle.service.enabled = false Then run the following test case: spark . sparkContext . parallelize ( 1 to 10 , 10 ) . flatMap ( _ => ( 1 to 100 ). iterator . map ( num => num )) . repartition ( 10 ) . count During the Spark Job, you should see the following message in Celeborn Master's log: Master: Offer slots successfully for 10 reducers of local-1690000152711-0 on 1 workers. And the following message in Celeborn Worker's log: INFO [dispatcher-event-loop-9] Controller: Reserved 10 primary location and 0 replica location for local-1690000152711-0 INFO [dispatcher-event-loop-8] Controller: Start commitFiles for local-1690000152711-0 INFO [async-reply] Controller: CommitFiles for local-1690000152711-0 success with 10 committed primary partitions, 0 empty primary partitions, 0 failed primary partitions, 0 committed replica partitions, 0 empty replica partitions, 0 failed replica partitions. Start Flink with Celeborn Copy Celeborn Client to Flink's lib Celeborn release binary contains clients for Flink 1.14.x, Flink 1.15.x, Flink 1.17.x, Flink 1.18.x and Flink 1.19.x, copy the corresponding client jar into Flink's lib/ directory: cp $CELEBORN_HOME /flink/<Celeborn Client Jar> $FLINK_HOME /lib/ Add Celeborn configuration to Flink's conf Set shuffle-service-factory.class to Celeborn's ShuffleServiceFactory in Flink configuration file: Flink 1.14.x, Flink 1.15.x, Flink 1.17.x, Flink 1.18.x cd $FLINK_HOME vi conf/flink-conf.yaml Flink 1.19.x cd $FLINK_HOME vi conf/config.yaml shuffle-service-factory.class : org.apache.celeborn.plugin.flink.RemoteShuffleServiceFactory execution.batch-shuffle-mode : ALL_EXCHANGES_BLOCKING Note : The config option execution.batch-shuffle-mode should configure as ALL_EXCHANGES_BLOCKING . Then deploy the example word count job to the running cluster: cd $FLINK_HOME ./bin/flink run examples/streaming/WordCount.jar --execution-mode BATCH During the Flink Job, you should see the following message in Celeborn Master's log: Master: Offer slots successfully for 1 reducers of local-1690000152711-0 on 1 workers. And the following message in Celeborn Worker's log: INFO [dispatcher-event-loop-4] Controller: Reserved 1 primary location and 0 replica location for local-1690000152711-0 INFO [dispatcher-event-loop-3] Controller: Start commitFiles for local-1690000152711-0 INFO [async-reply] Controller: CommitFiles for local-1690000152711-0 success with 1 committed primary partitions, 0 empty primary partitions, 0 failed primary partitions, 0 committed replica partitions, 0 empty replica partitions, 0 failed replica partitions. Start MapReduce With Celeborn Copy Celeborn Client to MapReduce's classpath Copy $CELEBORN_HOME/mr/*.jar into mapreduce.application.classpath and yarn.application.classpath . cp $CELEBORN_HOME /mr/<Celeborn Client Jar> <mapreduce.application.classpath> cp $CELEBORN_HOME /mr/<Celeborn Client Jar> <yarn.application.classpath> Restart your yarn cluster. Add Celeborn configuration to MapReduce's conf Modify configurations in ${HADOOP_CONF_DIR}/yarn-site.xml . <configuration> <property> <name> yarn.app.mapreduce.am.job.recovery.enable </name> <value> false </value> </property> <property> <name> yarn.app.mapreduce.am.command-opts </name> <!-- Append 'org.apache.celeborn.mapreduce.v2.app.MRAppMasterWithCeleborn' to this setting --> <value> org.apache.celeborn.mapreduce.v2.app.MRAppMasterWithCeleborn </value> </property> </configuration> Modify configurations in ${HADOOP_CONF_DIR}/mapred-site.xml . <configuration> <property> <name> mapreduce.job.reduce.slowstart.completedmaps </name> <value> 1 </value> </property> <property> <name> mapreduce.celeborn.master.endpoints </name> <!-- Replace placeholder to the real master address --> <value> placeholder </value> </property> <property> <name> mapreduce.job.map.output.collector.class </name> <value> org.apache.hadoop.mapred.CelebornMapOutputCollector </value> </property> <property> <name> mapreduce.job.reduce.shuffle.consumer.plugin.class </name> <value> org.apache.hadoop.mapreduce.task.reduce.CelebornShuffleConsumer </value> </property> </configuration> Note : MRAppMasterWithCeleborn supports setting mapreduce.celeborn.master.endpoints via environment variable CELEBORN_MASTER_ENDPOINTS . Meanwhile, MRAppMasterWithCeleborn disables yarn.app.mapreduce.am.job.recovery.enable and sets mapreduce.job.reduce.slowstart.completedmaps to 1 by default. Then deploy the example word count to the running cluster for verifying whether above configurations are correct. cd $HADOOP_HOME ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount /someinput /someoutput During the MapReduce Job, you should see the following message in Celeborn Master's log: Master: Offer slots successfully for 1 reducers of application_1694674023293_0003-0 on 1 workers. And the following message in Celeborn Worker's log: INFO [dispatcher-event-loop-4] Controller: Reserved 1 primary location and 0 replica location for application_1694674023293_0003-0 INFO [dispatcher-event-loop-3] Controller: Start commitFiles for application_1694674023293_0003-0 INFO [async-reply] Controller: CommitFiles for application_1694674023293_0003-0 success with 1 committed primary partitions, 0 empty primary partitions, 0 failed primary partitions, 0 committed replica partitions, 0 empty replica partitions, 0 failed replica partitions.","title":"QuickStart"},{"location":"#quick-start","text":"This documentation gives a quick start guide for running Spark/Flink/MapReduce with Apache Celeborn\u2122.","title":"Quick Start"},{"location":"#download-celeborn","text":"Download the latest Celeborn binary from the Downloading Page . Decompress the binary and set $CELEBORN_HOME . tar -C <DST_DIR> -zxvf apache-celeborn-<VERSION>-bin.tgz export CELEBORN_HOME = <Decompressed path>","title":"Download Celeborn"},{"location":"#configure-logging-and-storage","text":"","title":"Configure Logging and Storage"},{"location":"#configure-logging","text":"cd $CELEBORN_HOME /conf cp log4j2.xml.template log4j2.xml","title":"Configure Logging"},{"location":"#configure-storage","text":"Configure the directory to store shuffle data, for example $CELEBORN_HOME/shuffle . cd $CELEBORN_HOME /conf echo \"celeborn.worker.storage.dirs= $CELEBORN_HOME /shuffle\" > celeborn-defaults.conf","title":"Configure Storage"},{"location":"#start-celeborn-service","text":"","title":"Start Celeborn Service"},{"location":"#start-master","text":"cd $CELEBORN_HOME ./sbin/start-master.sh You should see Master 's ip:port in the log: INFO [main] NettyRpcEnvFactory: Starting RPC Server [MasterSys] on 192.168.2.109:9097 with advisor endpoint 192.168.2.109:9097","title":"Start Master"},{"location":"#start-worker","text":"Use the Master's IP and Port to start Worker: cd $CELEBORN_HOME ./sbin/start-worker.sh celeborn://<Master IP>:<Master Port> You should see the following message in Worker's log: INFO [main] MasterClient: connect to master 192.168.2.109:9097. INFO [main] Worker: Register worker successfully. INFO [main] Worker: Worker started. And also the following message in Master's log: INFO [dispatcher-event-loop-9] Master: Registered worker Host: 192.168.2.109 RpcPort: 57806 PushPort: 57807 FetchPort: 57809 ReplicatePort: 57808 SlotsUsed: 0 LastHeartbeat: 0 HeartbeatElapsedSeconds: xxx Disks: DiskInfo0: xxx UserResourceConsumption: empty WorkerRef: null","title":"Start Worker"},{"location":"#start-spark-with-celeborn","text":"","title":"Start Spark with Celeborn"},{"location":"#copy-celeborn-client-to-sparks-jars","text":"Celeborn release binary contains clients for Spark 2.x and Spark 3.x, copy the corresponding client jar into Spark's jars/ directory: cp $CELEBORN_HOME /spark/<Celeborn Client Jar> $SPARK_HOME /jars/","title":"Copy Celeborn Client to Spark's jars"},{"location":"#start-spark-shell","text":"Set spark.shuffle.manager to Celeborn's ShuffleManager, and turn off spark.shuffle.service.enabled : cd $SPARK_HOME ./bin/spark-shell \\ --conf spark.shuffle.manager = org.apache.spark.shuffle.celeborn.SparkShuffleManager \\ --conf spark.shuffle.service.enabled = false Then run the following test case: spark . sparkContext . parallelize ( 1 to 10 , 10 ) . flatMap ( _ => ( 1 to 100 ). iterator . map ( num => num )) . repartition ( 10 ) . count During the Spark Job, you should see the following message in Celeborn Master's log: Master: Offer slots successfully for 10 reducers of local-1690000152711-0 on 1 workers. And the following message in Celeborn Worker's log: INFO [dispatcher-event-loop-9] Controller: Reserved 10 primary location and 0 replica location for local-1690000152711-0 INFO [dispatcher-event-loop-8] Controller: Start commitFiles for local-1690000152711-0 INFO [async-reply] Controller: CommitFiles for local-1690000152711-0 success with 10 committed primary partitions, 0 empty primary partitions, 0 failed primary partitions, 0 committed replica partitions, 0 empty replica partitions, 0 failed replica partitions.","title":"Start spark-shell"},{"location":"#start-flink-with-celeborn","text":"","title":"Start Flink with Celeborn"},{"location":"#copy-celeborn-client-to-flinks-lib","text":"Celeborn release binary contains clients for Flink 1.14.x, Flink 1.15.x, Flink 1.17.x, Flink 1.18.x and Flink 1.19.x, copy the corresponding client jar into Flink's lib/ directory: cp $CELEBORN_HOME /flink/<Celeborn Client Jar> $FLINK_HOME /lib/","title":"Copy Celeborn Client to Flink's lib"},{"location":"#add-celeborn-configuration-to-flinks-conf","text":"Set shuffle-service-factory.class to Celeborn's ShuffleServiceFactory in Flink configuration file: Flink 1.14.x, Flink 1.15.x, Flink 1.17.x, Flink 1.18.x cd $FLINK_HOME vi conf/flink-conf.yaml Flink 1.19.x cd $FLINK_HOME vi conf/config.yaml shuffle-service-factory.class : org.apache.celeborn.plugin.flink.RemoteShuffleServiceFactory execution.batch-shuffle-mode : ALL_EXCHANGES_BLOCKING Note : The config option execution.batch-shuffle-mode should configure as ALL_EXCHANGES_BLOCKING . Then deploy the example word count job to the running cluster: cd $FLINK_HOME ./bin/flink run examples/streaming/WordCount.jar --execution-mode BATCH During the Flink Job, you should see the following message in Celeborn Master's log: Master: Offer slots successfully for 1 reducers of local-1690000152711-0 on 1 workers. And the following message in Celeborn Worker's log: INFO [dispatcher-event-loop-4] Controller: Reserved 1 primary location and 0 replica location for local-1690000152711-0 INFO [dispatcher-event-loop-3] Controller: Start commitFiles for local-1690000152711-0 INFO [async-reply] Controller: CommitFiles for local-1690000152711-0 success with 1 committed primary partitions, 0 empty primary partitions, 0 failed primary partitions, 0 committed replica partitions, 0 empty replica partitions, 0 failed replica partitions.","title":"Add Celeborn configuration to Flink's conf"},{"location":"#start-mapreduce-with-celeborn","text":"","title":"Start MapReduce With Celeborn"},{"location":"#copy-celeborn-client-to-mapreduces-classpath","text":"Copy $CELEBORN_HOME/mr/*.jar into mapreduce.application.classpath and yarn.application.classpath . cp $CELEBORN_HOME /mr/<Celeborn Client Jar> <mapreduce.application.classpath> cp $CELEBORN_HOME /mr/<Celeborn Client Jar> <yarn.application.classpath> Restart your yarn cluster.","title":"Copy Celeborn Client to MapReduce's classpath"},{"location":"#add-celeborn-configuration-to-mapreduces-conf","text":"Modify configurations in ${HADOOP_CONF_DIR}/yarn-site.xml . <configuration> <property> <name> yarn.app.mapreduce.am.job.recovery.enable </name> <value> false </value> </property> <property> <name> yarn.app.mapreduce.am.command-opts </name> <!-- Append 'org.apache.celeborn.mapreduce.v2.app.MRAppMasterWithCeleborn' to this setting --> <value> org.apache.celeborn.mapreduce.v2.app.MRAppMasterWithCeleborn </value> </property> </configuration> Modify configurations in ${HADOOP_CONF_DIR}/mapred-site.xml . <configuration> <property> <name> mapreduce.job.reduce.slowstart.completedmaps </name> <value> 1 </value> </property> <property> <name> mapreduce.celeborn.master.endpoints </name> <!-- Replace placeholder to the real master address --> <value> placeholder </value> </property> <property> <name> mapreduce.job.map.output.collector.class </name> <value> org.apache.hadoop.mapred.CelebornMapOutputCollector </value> </property> <property> <name> mapreduce.job.reduce.shuffle.consumer.plugin.class </name> <value> org.apache.hadoop.mapreduce.task.reduce.CelebornShuffleConsumer </value> </property> </configuration> Note : MRAppMasterWithCeleborn supports setting mapreduce.celeborn.master.endpoints via environment variable CELEBORN_MASTER_ENDPOINTS . Meanwhile, MRAppMasterWithCeleborn disables yarn.app.mapreduce.am.job.recovery.enable and sets mapreduce.job.reduce.slowstart.completedmaps to 1 by default. Then deploy the example word count to the running cluster for verifying whether above configurations are correct. cd $HADOOP_HOME ./bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount /someinput /someoutput During the MapReduce Job, you should see the following message in Celeborn Master's log: Master: Offer slots successfully for 1 reducers of application_1694674023293_0003-0 on 1 workers. And the following message in Celeborn Worker's log: INFO [dispatcher-event-loop-4] Controller: Reserved 1 primary location and 0 replica location for application_1694674023293_0003-0 INFO [dispatcher-event-loop-3] Controller: Start commitFiles for application_1694674023293_0003-0 INFO [async-reply] Controller: CommitFiles for application_1694674023293_0003-0 success with 1 committed primary partitions, 0 empty primary partitions, 0 failed primary partitions, 0 committed replica partitions, 0 empty replica partitions, 0 failed replica partitions.","title":"Add Celeborn configuration to MapReduce's conf"},{"location":"celeborn_ratis_shell/","text":"Celeborn Ratis-shell Ratis-shell is the command line interface of Ratis. Celeborn uses Ratis to implement the HA function of the master, Celeborn directly introduces ratis-shell package into the project then it's convenient for Celeborn Admin to operate the master ratis service. Note : Ratis-shell is currently only experimental . The compatibility story is not considered for the time being. Availability Version Available in src tarball? Available in bin tarball? < 0.3.0 No No >= 0.3.0 Yes Yes Setting up the Celeborn ratis-shell Celeborn directly introduces the ratis-shell into the project, users don't need to set up ratis-shell env from ratis repo. User can directly download the Celeborn source tarball from Download and build the Celeborn according to build_and_test or just download the pre-built binary tarball from Download to get the binary package apache-celeborn-<VERSION>-bin.tgz . After getting the binary package apache-celeborn-<VERSION>-bin.tgz : $ tar -C <DST_DIR> -zxvf apache-celeborn-<VERSION>-bin.tgz $ ln -s <DST_DIR>/apache-celeborn-<VERSION>-bin <DST_DIR>/celeborn Export the following environment variable and add the bin directory to the $PATH . $ export CELEBORN_HOME=<DST_DIR>/celeborn $ export PATH=${CELEBORN_HOME}/bin:$PATH The following command can be invoked in order to get the basic usage: $ celeborn-ratis sh Usage: celeborn-ratis sh [ generic options ] [ election [ transfer ] [ stepDown ] [ pause ] [ resume ]] [ group [ info ] [ list ]] [ peer [ add ] [ remove ] [ setPriority ]] [ snapshot [ create ]] [ local [ raftMetaConf ]] generic options The generic options pass values for a given ratis-shell property. It supports the following content: -D* , -X* , -agentlib* , -javaagent* $ celeborn-ratis sh -D<property=value> ... Note: Celeborn HA uses NETTY as the default RPC type, for details please refer to configuration celeborn.master.ha.ratis.raft.rpc.type . But Ratis uses GRPC as the default RPC type. So if the user wants to use Ratis shell to access Ratis cluster which uses NETTY RPC type, the generic option -Draft.rpc.type=NETTY should be set to change the RPC type of Ratis shell to Netty. election The election command manages leader election. It has the following subcommands: transfer , stepDown , pause , resume election transfer Transfer a group leader to the specified server. $ celeborn-ratis sh election transfer -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -address <HOSTNAME:PORT> [-groupid <RAFT_GROUP_ID>] election stepDown Make a group leader of the given group step down its leadership. $ celeborn-ratis sh election stepDown -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] election pause Pause leader election at the specified server. Then, the specified server would not start a leader election. $ celeborn-ratis sh election pause -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -address <HOSTNAME:PORT> [-groupid <RAFT_GROUP_ID>] election resume Resume leader election at the specified server. $ celeborn-ratis sh election resume -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -address <HOSTNAME:PORT> [-groupid <RAFT_GROUP_ID>] group The group command manages ratis groups. It has the following subcommands: info , list group info Display the information of a specific raft group. $ celeborn-ratis sh group info -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] group list Display the group information of a specific raft server $ celeborn-ratis sh group list -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] <[-serverAddress <P0_HOST:P0_PORT>]|[-peerId <peerId0>]> peer The peer command manages ratis cluster peers. It has the following subcommands: add , remove , setPriority peer add Add peers to a ratis group. $ celeborn-ratis sh peer add -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] -address <P4_HOST:P4_PORT,...,PN_HOST:PN_PORT> peer remove Remove peers to from a ratis group. $ celeborn-ratis sh peer remove -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] -address <P0_HOST:P0_PORT,...> peer setPriority Set priority to ratis peers. The priority of ratis peer can affect the leader election, the server with the highest priority will eventually become the leader of the cluster. $ celeborn-ratis sh peer setPriority -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] -addressPriority <P0_HOST:P0_PORT|PRIORITY> snapshot The snapshot command manages ratis snapshot. It has the following subcommands: create snapshot create Trigger the specified server take snapshot. $ celeborn-ratis sh snapshot create -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -peerId <peerId0> [-groupid <RAFT_GROUP_ID>] local The local command is used to process local operation, which no need to connect to ratis server. It has the following subcommands: raftMetaConf local raftMetaConf Generate a new raft-meta.conf file based on original raft-meta.conf and new peers, which is used to move a raft node to a new node. $ celeborn-ratis sh local raftMetaConf -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -path <PARENT_PATH_OF_RAFT_META_CONF>","title":"Ratis Shell"},{"location":"celeborn_ratis_shell/#celeborn-ratis-shell","text":"Ratis-shell is the command line interface of Ratis. Celeborn uses Ratis to implement the HA function of the master, Celeborn directly introduces ratis-shell package into the project then it's convenient for Celeborn Admin to operate the master ratis service. Note : Ratis-shell is currently only experimental . The compatibility story is not considered for the time being.","title":"Celeborn Ratis-shell"},{"location":"celeborn_ratis_shell/#availability","text":"Version Available in src tarball? Available in bin tarball? < 0.3.0 No No >= 0.3.0 Yes Yes","title":"Availability"},{"location":"celeborn_ratis_shell/#setting-up-the-celeborn-ratis-shell","text":"Celeborn directly introduces the ratis-shell into the project, users don't need to set up ratis-shell env from ratis repo. User can directly download the Celeborn source tarball from Download and build the Celeborn according to build_and_test or just download the pre-built binary tarball from Download to get the binary package apache-celeborn-<VERSION>-bin.tgz . After getting the binary package apache-celeborn-<VERSION>-bin.tgz : $ tar -C <DST_DIR> -zxvf apache-celeborn-<VERSION>-bin.tgz $ ln -s <DST_DIR>/apache-celeborn-<VERSION>-bin <DST_DIR>/celeborn Export the following environment variable and add the bin directory to the $PATH . $ export CELEBORN_HOME=<DST_DIR>/celeborn $ export PATH=${CELEBORN_HOME}/bin:$PATH The following command can be invoked in order to get the basic usage: $ celeborn-ratis sh Usage: celeborn-ratis sh [ generic options ] [ election [ transfer ] [ stepDown ] [ pause ] [ resume ]] [ group [ info ] [ list ]] [ peer [ add ] [ remove ] [ setPriority ]] [ snapshot [ create ]] [ local [ raftMetaConf ]]","title":"Setting up the Celeborn ratis-shell"},{"location":"celeborn_ratis_shell/#generic-options","text":"The generic options pass values for a given ratis-shell property. It supports the following content: -D* , -X* , -agentlib* , -javaagent* $ celeborn-ratis sh -D<property=value> ... Note: Celeborn HA uses NETTY as the default RPC type, for details please refer to configuration celeborn.master.ha.ratis.raft.rpc.type . But Ratis uses GRPC as the default RPC type. So if the user wants to use Ratis shell to access Ratis cluster which uses NETTY RPC type, the generic option -Draft.rpc.type=NETTY should be set to change the RPC type of Ratis shell to Netty.","title":"generic options"},{"location":"celeborn_ratis_shell/#election","text":"The election command manages leader election. It has the following subcommands: transfer , stepDown , pause , resume","title":"election"},{"location":"celeborn_ratis_shell/#election-transfer","text":"Transfer a group leader to the specified server. $ celeborn-ratis sh election transfer -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -address <HOSTNAME:PORT> [-groupid <RAFT_GROUP_ID>]","title":"election transfer"},{"location":"celeborn_ratis_shell/#election-stepdown","text":"Make a group leader of the given group step down its leadership. $ celeborn-ratis sh election stepDown -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>]","title":"election stepDown"},{"location":"celeborn_ratis_shell/#election-pause","text":"Pause leader election at the specified server. Then, the specified server would not start a leader election. $ celeborn-ratis sh election pause -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -address <HOSTNAME:PORT> [-groupid <RAFT_GROUP_ID>]","title":"election pause"},{"location":"celeborn_ratis_shell/#election-resume","text":"Resume leader election at the specified server. $ celeborn-ratis sh election resume -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -address <HOSTNAME:PORT> [-groupid <RAFT_GROUP_ID>]","title":"election resume"},{"location":"celeborn_ratis_shell/#group","text":"The group command manages ratis groups. It has the following subcommands: info , list","title":"group"},{"location":"celeborn_ratis_shell/#group-info","text":"Display the information of a specific raft group. $ celeborn-ratis sh group info -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>]","title":"group info"},{"location":"celeborn_ratis_shell/#group-list","text":"Display the group information of a specific raft server $ celeborn-ratis sh group list -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] <[-serverAddress <P0_HOST:P0_PORT>]|[-peerId <peerId0>]>","title":"group list"},{"location":"celeborn_ratis_shell/#peer","text":"The peer command manages ratis cluster peers. It has the following subcommands: add , remove , setPriority","title":"peer"},{"location":"celeborn_ratis_shell/#peer-add","text":"Add peers to a ratis group. $ celeborn-ratis sh peer add -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] -address <P4_HOST:P4_PORT,...,PN_HOST:PN_PORT>","title":"peer add"},{"location":"celeborn_ratis_shell/#peer-remove","text":"Remove peers to from a ratis group. $ celeborn-ratis sh peer remove -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] -address <P0_HOST:P0_PORT,...>","title":"peer remove"},{"location":"celeborn_ratis_shell/#peer-setpriority","text":"Set priority to ratis peers. The priority of ratis peer can affect the leader election, the server with the highest priority will eventually become the leader of the cluster. $ celeborn-ratis sh peer setPriority -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> [-groupid <RAFT_GROUP_ID>] -addressPriority <P0_HOST:P0_PORT|PRIORITY>","title":"peer setPriority"},{"location":"celeborn_ratis_shell/#snapshot","text":"The snapshot command manages ratis snapshot. It has the following subcommands: create","title":"snapshot"},{"location":"celeborn_ratis_shell/#snapshot-create","text":"Trigger the specified server take snapshot. $ celeborn-ratis sh snapshot create -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -peerId <peerId0> [-groupid <RAFT_GROUP_ID>]","title":"snapshot create"},{"location":"celeborn_ratis_shell/#local","text":"The local command is used to process local operation, which no need to connect to ratis server. It has the following subcommands: raftMetaConf","title":"local"},{"location":"celeborn_ratis_shell/#local-raftmetaconf","text":"Generate a new raft-meta.conf file based on original raft-meta.conf and new peers, which is used to move a raft node to a new node. $ celeborn-ratis sh local raftMetaConf -peers <P0_HOST:P0_PORT,P1_HOST:P1_PORT,P2_HOST:P2_PORT> -path <PARENT_PATH_OF_RAFT_META_CONF>","title":"local raftMetaConf"},{"location":"cluster_planning/","text":"Cluster Planning Node Spec Empirical size configs for Celeborn nodes The principle is to try to avoid any hardware(CPU, Memory, Disk Bandwidth/IOPS, Network Bandwidth/PPS)becoming the bottleneck. The goal is to let all the hardware usage be close to each other, for example let the disk IOPS/Bandwidth usage and network usage stay roughly the same so that data will be perfectly pipelined and no back-pressure will be triggered. The goal is hard to reach, and perhaps has a relationship with workload characteristics, and also Celeborn\u2019s configs can have some impact. In our former experience, vCores: memory(GB): Bandwidth( Gbps): Disk IO (KIOps) is better to be 2: 5: 2: 1. We didn\u2019t thoroughly conduct experiments on various configs(it\u2019s hard to do so), so it\u2019s merely a reference. Worker Scale You need to estimate your cluster's max concurrent shuffle size(ES), and get the total usable disk space of a node(NS). The worker count can be (ES * 2 / NS) .","title":"Cluster Planning"},{"location":"cluster_planning/#cluster-planning","text":"","title":"Cluster Planning"},{"location":"cluster_planning/#node-spec","text":"Empirical size configs for Celeborn nodes The principle is to try to avoid any hardware(CPU, Memory, Disk Bandwidth/IOPS, Network Bandwidth/PPS)becoming the bottleneck. The goal is to let all the hardware usage be close to each other, for example let the disk IOPS/Bandwidth usage and network usage stay roughly the same so that data will be perfectly pipelined and no back-pressure will be triggered. The goal is hard to reach, and perhaps has a relationship with workload characteristics, and also Celeborn\u2019s configs can have some impact. In our former experience, vCores: memory(GB): Bandwidth( Gbps): Disk IO (KIOps) is better to be 2: 5: 2: 1. We didn\u2019t thoroughly conduct experiments on various configs(it\u2019s hard to do so), so it\u2019s merely a reference.","title":"Node Spec"},{"location":"cluster_planning/#worker-scale","text":"You need to estimate your cluster's max concurrent shuffle size(ES), and get the total usable disk space of a node(NS). The worker count can be (ES * 2 / NS) .","title":"Worker Scale"},{"location":"deploy/","text":"Deploy Celeborn Unzip the tarball to $CELEBORN_HOME . Modify environment variables in $CELEBORN_HOME/conf/celeborn-env.sh . EXAMPLE: #!/usr/bin/env bash CELEBORN_MASTER_MEMORY = 4g CELEBORN_WORKER_MEMORY = 2g CELEBORN_WORKER_OFFHEAP_MEMORY = 4g 3. Modify configurations in $CELEBORN_HOME/conf/celeborn-defaults.conf . EXAMPLE: single master cluster # used by client and worker to connect to master celeborn.master.endpoints clb-master : 9097 # used by master to bootstrap celeborn.master.host clb-master celeborn.master.port 9097 celeborn.metrics.enabled true celeborn.worker.flusher.buffer.size 256k # If Celeborn workers have local disks and HDFS. Following configs should be added. # If Celeborn workers have local disks, use following config. # Disk type is HDD by default. celeborn.worker.storage.dirs /mnt/disk1 : disktype=SSD,/mnt/disk2:disktype=SSD # If Celeborn workers don't have local disks. You can use HDFS. # Do not set `celeborn.worker.storage.dirs` and use following configs. celeborn.storage.activeTypes HDFS celeborn.worker.sortPartition.threads 64 celeborn.worker.commitFiles.timeout 240s celeborn.worker.commitFiles.threads 128 celeborn.master.slot.assign.policy roundrobin celeborn.rpc.askTimeout 240s celeborn.worker.flusher.hdfs.buffer.size 4m celeborn.storage.hdfs.dir hdfs : //<namenode>/celeborn celeborn.worker.replicate.fastFail.duration 240s # Either principal/keytab or valid TGT cache is required to access kerberized HDFS celeborn.storage.hdfs.kerberos.principal user@REALM celeborn.storage.hdfs.kerberos.keytab /path/to/user.keytab # If your hosts have disk raid or use lvm, set `celeborn.worker.monitor.disk.enabled` to false celeborn.worker.monitor.disk.enabled false EXAMPLE: HA cluster # used by client and worker to connect to master celeborn.master.endpoints clb-1 : 9097,clb-2:9097,clb-3:9097 # used by master nodes to bootstrap, every node should know the topology of whole cluster, for each node, # `celeborn.master.ha.node.id` should be unique, and `celeborn.master.ha.node.<id>.host` is required. celeborn.master.ha.enabled true celeborn.master.ha.node.id 1 celeborn.master.ha.node.1.host clb-1 celeborn.master.ha.node.1.port 9097 celeborn.master.ha.node.1.ratis.port 9872 celeborn.master.ha.node.2.host clb-2 celeborn.master.ha.node.2.port 9097 celeborn.master.ha.node.2.ratis.port 9872 celeborn.master.ha.node.3.host clb-3 celeborn.master.ha.node.3.port 9097 celeborn.master.ha.node.3.ratis.port 9872 celeborn.master.ha.ratis.raft.server.storage.dir /mnt/disk1/celeborn_ratis/ celeborn.metrics.enabled true # If you want to use HDFS as shuffle storage, make sure that flush buffer size is at least 4MB or larger. celeborn.worker.flusher.buffer.size 256k # If Celeborn workers have local disks and HDFS. Following configs should be added. # Celeborn will use local disks until local disk become unavailable to gain the best performance. # Increase Celeborn's off-heap memory if Celeborn write to HDFS. # If Celeborn workers have local disks, use following config. # Disk type is HDD by default. celeborn.worker.storage.dirs /mnt/disk1 : disktype=SSD,/mnt/disk2:disktype=SSD # If Celeborn workers don't have local disks. You can use HDFS. # Do not set `celeborn.worker.storage.dirs` and use following configs. celeborn.storage.activeTypes HDFS celeborn.worker.sortPartition.threads 64 celeborn.worker.commitFiles.timeout 240s celeborn.worker.commitFiles.threads 128 celeborn.master.slot.assign.policy roundrobin celeborn.rpc.askTimeout 240s celeborn.worker.flusher.hdfs.buffer.size 4m celeborn.storage.hdfs.dir hdfs : //<namenode>/celeborn celeborn.worker.replicate.fastFail.duration 240s # If your hosts have disk raid or use lvm, set `celeborn.worker.monitor.disk.enabled` to false celeborn.worker.monitor.disk.enabled false Flink engine related configurations: # If you are using Celeborn for flink, these settings will be needed. celeborn.worker.directMemoryRatioForReadBuffer 0.4 celeborn.worker.directMemoryRatioToResume 0.5 # These setting will affect performance. # If there is enough off-heap memory, you can try to increase read buffers. # Read buffer max memory usage for a data partition is `taskmanager.memory.segment-size * readBuffersMax` celeborn.worker.partition.initial.readBuffersMin 512 celeborn.worker.partition.initial.readBuffersMax 1024 celeborn.worker.readBuffer.allocationWait 10ms Copy Celeborn and configurations to all nodes. Start all services. If you install Celeborn distribution in the same path on every node and your cluster can perform SSH login then you can fill $CELEBORN_HOME/conf/hosts and use $CELEBORN_HOME/sbin/start-all.sh to start all services. If the installation paths are not identical, you will need to start service manually. Start Celeborn master $CELEBORN_HOME/sbin/start-master.sh Start Celeborn worker $CELEBORN_HOME/sbin/start-worker.sh If Celeborn starts success, the output of the Master's log should be like this: 22/10/08 19:29:11,805 INFO [main] Dispatcher: Dispatcher numThreads: 64 22/10/08 19:29:11,875 INFO [main] TransportClientFactory: mode NIO threads 64 22/10/08 19:29:12,057 INFO [main] Utils: Successfully started service 'MasterSys' on port 9097. 22/10/08 19:29:12,113 INFO [main] Master: Metrics system enabled. 22/10/08 19:29:12,125 INFO [main] HttpServer: master: HttpServer started on port 9098. 22/10/08 19:29:12,126 INFO [main] Master: Master started. 22/10/08 19:29:57,842 INFO [dispatcher-event-loop-19] Master: Registered worker Host: 192.168.15.140 RpcPort: 37359 PushPort: 38303 FetchPort: 37569 ReplicatePort: 37093 SlotsUsed: 0() LastHeartbeat: 0 Disks: {/mnt/disk1=DiskInfo(maxSlots: 6679, committed shuffles 0, running applications 0, shuffleAllocations: Map(), mountPoint: /mnt/disk1, usableSpace: 448284381184, avgFlushTime: 0, activeSlots: 0) status: HEALTHY dirs , /mnt/disk3=DiskInfo(maxSlots: 6716, committed shuffles 0, running applications 0, shuffleAllocations: Map(), mountPoint: /mnt/disk3, usableSpace: 450755608576, avgFlushTime: 0, activeSlots: 0) status: HEALTHY dirs , /mnt/disk2=DiskInfo(maxSlots: 6713, committed shuffles 0, running applications 0, shuffleAllocations: Map(), mountPoint: /mnt/disk2, usableSpace: 450532900864, avgFlushTime: 0, activeSlots: 0) status: HEALTHY dirs , /mnt/disk4=DiskInfo(maxSlots: 6712, committed shuffles 0, running applications 0, shuffleAllocations: Map(), mountPoint: /mnt/disk4, usableSpace: 450456805376, avgFlushTime: 0, activeSlots: 0) status: HEALTHY dirs } WorkerRef: null Deploy Spark client Copy $CELEBORN_HOME/spark/*.jar to $SPARK_HOME/jars/ . Spark Configuration To use Celeborn, the following spark configurations should be added. # Shuffle manager class name changed in 0.3.0: # before 0.3.0: `org.apache.spark.shuffle.celeborn.RssShuffleManager` # since 0.3.0: `org.apache.spark.shuffle.celeborn.SparkShuffleManager` spark.shuffle.manager org.apache.spark.shuffle.celeborn.SparkShuffleManager # must use kryo serializer because java serializer do not support relocation spark.serializer org.apache.spark.serializer.KryoSerializer # celeborn master spark.celeborn.master.endpoints clb-1 : 9097,clb-2:9097,clb-3:9097 # This is not necessary if your Spark external shuffle service is Spark 3.1 or newer spark.shuffle.service.enabled false # options: hash, sort # Hash shuffle writer use (partition count) * (celeborn.push.buffer.max.size) * (spark.executor.cores) memory. # Sort shuffle writer uses less memory than hash shuffle writer, if your shuffle partition count is large, try to use sort hash writer. spark.celeborn.client.spark.shuffle.writer hash # We recommend setting `spark.celeborn.client.push.replicate.enabled` to true to enable server-side data replication # If you have only one worker, this setting must be false # If your Celeborn is using HDFS, it's recommended to set this setting to false spark.celeborn.client.push.replicate.enabled true # Support for Spark AQE only tested under Spark 3 # we recommend setting localShuffleReader to false for getting better performance of Celeborn spark.sql.adaptive.localShuffleReader.enabled false # If Celeborn is using HDFS spark.celeborn.storage.hdfs.dir hdfs : //<namenode>/celeborn # we recommend enabling aqe support to gain better performance spark.sql.adaptive.enabled true spark.sql.adaptive.skewJoin.enabled true # Support Spark Dynamic Resource Allocation # Required Spark version >= 3.5.0 spark.shuffle.sort.io.plugin.class org.apache.spark.shuffle.celeborn.CelebornShuffleDataIO # Required Spark version >= 3.4.0, highly recommended to disable spark.dynamicAllocation.shuffleTracking.enabled false # Support ShuffleManager when defined in user jars # Required Spark version < 4.0.0 or without SPARK-45762, highly recommended to false for ShuffleManager in user-defined jar specified by --jars or spark.jars spark.executor.userClassPathFirst false Deploy Flink client Copy $CELEBORN_HOME/flink/*.jar to $FLINK_HOME/lib/ . Flink Configuration To use Celeborn, the following flink configurations should be added. shuffle-service-factory.class : org.apache.celeborn.plugin.flink.RemoteShuffleServiceFactory execution.batch-shuffle-mode : ALL_EXCHANGES_BLOCKING celeborn.master.endpoints : clb-1:9097,clb-2:9097,clb-3:9097 celeborn.client.shuffle.batchHandleReleasePartition.enabled : true celeborn.client.push.maxReqsInFlight : 128 # Network connections between peers celeborn.data.io.numConnectionsPerPeer : 16 # threads number may vary according to your cluster but do not set to 1 celeborn.data.io.threads : 32 celeborn.client.shuffle.batchHandleCommitPartition.threads : 32 celeborn.rpc.dispatcher.numThreads : 32 # Floating buffers may need to change `taskmanager.network.memory.fraction` and `taskmanager.network.memory.max` taskmanager.network.memory.floating-buffers-per-gate : 4096 taskmanager.network.memory.buffers-per-channel : 0 taskmanager.memory.task.off-heap.size : 512m Note : The config option execution.batch-shuffle-mode should configure as ALL_EXCHANGES_BLOCKING . Deploy MapReduce client Copy $CELEBORN_HOME/mr/*.jar into mapreduce.application.classpath and yarn.application.classpath . Meanwhile, configure the following settings in YARN and MapReduce config. -Dyarn.app.mapreduce.am.job.recovery.enable = false -Dmapreduce.job.reduce.slowstart.completedmaps = 1 -Dmapreduce.celeborn.master.endpoints = <master-1-1>:9097 -Dyarn.app.mapreduce.am.command-opts = org.apache.celeborn.mapreduce.v2.app.MRAppMasterWithCeleborn -Dmapreduce.job.map.output.collector.class = org.apache.hadoop.mapred.CelebornMapOutputCollector -Dmapreduce.job.reduce.shuffle.consumer.plugin.class = org.apache.hadoop.mapreduce.task.reduce.CelebornShuffleConsumer Note : MRAppMasterWithCeleborn supports setting mapreduce.celeborn.master.endpoints via environment variable CELEBORN_MASTER_ENDPOINTS . Meanwhile, MRAppMasterWithCeleborn disables yarn.app.mapreduce.am.job.recovery.enable and sets mapreduce.job.reduce.slowstart.completedmaps to 1 by default.","title":"Overview"},{"location":"deploy/#deploy-celeborn","text":"Unzip the tarball to $CELEBORN_HOME . Modify environment variables in $CELEBORN_HOME/conf/celeborn-env.sh . EXAMPLE: #!/usr/bin/env bash CELEBORN_MASTER_MEMORY = 4g CELEBORN_WORKER_MEMORY = 2g CELEBORN_WORKER_OFFHEAP_MEMORY = 4g 3. Modify configurations in $CELEBORN_HOME/conf/celeborn-defaults.conf . EXAMPLE: single master cluster # used by client and worker to connect to master celeborn.master.endpoints clb-master : 9097 # used by master to bootstrap celeborn.master.host clb-master celeborn.master.port 9097 celeborn.metrics.enabled true celeborn.worker.flusher.buffer.size 256k # If Celeborn workers have local disks and HDFS. Following configs should be added. # If Celeborn workers have local disks, use following config. # Disk type is HDD by default. celeborn.worker.storage.dirs /mnt/disk1 : disktype=SSD,/mnt/disk2:disktype=SSD # If Celeborn workers don't have local disks. You can use HDFS. # Do not set `celeborn.worker.storage.dirs` and use following configs. celeborn.storage.activeTypes HDFS celeborn.worker.sortPartition.threads 64 celeborn.worker.commitFiles.timeout 240s celeborn.worker.commitFiles.threads 128 celeborn.master.slot.assign.policy roundrobin celeborn.rpc.askTimeout 240s celeborn.worker.flusher.hdfs.buffer.size 4m celeborn.storage.hdfs.dir hdfs : //<namenode>/celeborn celeborn.worker.replicate.fastFail.duration 240s # Either principal/keytab or valid TGT cache is required to access kerberized HDFS celeborn.storage.hdfs.kerberos.principal user@REALM celeborn.storage.hdfs.kerberos.keytab /path/to/user.keytab # If your hosts have disk raid or use lvm, set `celeborn.worker.monitor.disk.enabled` to false celeborn.worker.monitor.disk.enabled false EXAMPLE: HA cluster # used by client and worker to connect to master celeborn.master.endpoints clb-1 : 9097,clb-2:9097,clb-3:9097 # used by master nodes to bootstrap, every node should know the topology of whole cluster, for each node, # `celeborn.master.ha.node.id` should be unique, and `celeborn.master.ha.node.<id>.host` is required. celeborn.master.ha.enabled true celeborn.master.ha.node.id 1 celeborn.master.ha.node.1.host clb-1 celeborn.master.ha.node.1.port 9097 celeborn.master.ha.node.1.ratis.port 9872 celeborn.master.ha.node.2.host clb-2 celeborn.master.ha.node.2.port 9097 celeborn.master.ha.node.2.ratis.port 9872 celeborn.master.ha.node.3.host clb-3 celeborn.master.ha.node.3.port 9097 celeborn.master.ha.node.3.ratis.port 9872 celeborn.master.ha.ratis.raft.server.storage.dir /mnt/disk1/celeborn_ratis/ celeborn.metrics.enabled true # If you want to use HDFS as shuffle storage, make sure that flush buffer size is at least 4MB or larger. celeborn.worker.flusher.buffer.size 256k # If Celeborn workers have local disks and HDFS. Following configs should be added. # Celeborn will use local disks until local disk become unavailable to gain the best performance. # Increase Celeborn's off-heap memory if Celeborn write to HDFS. # If Celeborn workers have local disks, use following config. # Disk type is HDD by default. celeborn.worker.storage.dirs /mnt/disk1 : disktype=SSD,/mnt/disk2:disktype=SSD # If Celeborn workers don't have local disks. You can use HDFS. # Do not set `celeborn.worker.storage.dirs` and use following configs. celeborn.storage.activeTypes HDFS celeborn.worker.sortPartition.threads 64 celeborn.worker.commitFiles.timeout 240s celeborn.worker.commitFiles.threads 128 celeborn.master.slot.assign.policy roundrobin celeborn.rpc.askTimeout 240s celeborn.worker.flusher.hdfs.buffer.size 4m celeborn.storage.hdfs.dir hdfs : //<namenode>/celeborn celeborn.worker.replicate.fastFail.duration 240s # If your hosts have disk raid or use lvm, set `celeborn.worker.monitor.disk.enabled` to false celeborn.worker.monitor.disk.enabled false Flink engine related configurations: # If you are using Celeborn for flink, these settings will be needed. celeborn.worker.directMemoryRatioForReadBuffer 0.4 celeborn.worker.directMemoryRatioToResume 0.5 # These setting will affect performance. # If there is enough off-heap memory, you can try to increase read buffers. # Read buffer max memory usage for a data partition is `taskmanager.memory.segment-size * readBuffersMax` celeborn.worker.partition.initial.readBuffersMin 512 celeborn.worker.partition.initial.readBuffersMax 1024 celeborn.worker.readBuffer.allocationWait 10ms Copy Celeborn and configurations to all nodes. Start all services. If you install Celeborn distribution in the same path on every node and your cluster can perform SSH login then you can fill $CELEBORN_HOME/conf/hosts and use $CELEBORN_HOME/sbin/start-all.sh to start all services. If the installation paths are not identical, you will need to start service manually. Start Celeborn master $CELEBORN_HOME/sbin/start-master.sh Start Celeborn worker $CELEBORN_HOME/sbin/start-worker.sh If Celeborn starts success, the output of the Master's log should be like this: 22/10/08 19:29:11,805 INFO [main] Dispatcher: Dispatcher numThreads: 64 22/10/08 19:29:11,875 INFO [main] TransportClientFactory: mode NIO threads 64 22/10/08 19:29:12,057 INFO [main] Utils: Successfully started service 'MasterSys' on port 9097. 22/10/08 19:29:12,113 INFO [main] Master: Metrics system enabled. 22/10/08 19:29:12,125 INFO [main] HttpServer: master: HttpServer started on port 9098. 22/10/08 19:29:12,126 INFO [main] Master: Master started. 22/10/08 19:29:57,842 INFO [dispatcher-event-loop-19] Master: Registered worker Host: 192.168.15.140 RpcPort: 37359 PushPort: 38303 FetchPort: 37569 ReplicatePort: 37093 SlotsUsed: 0() LastHeartbeat: 0 Disks: {/mnt/disk1=DiskInfo(maxSlots: 6679, committed shuffles 0, running applications 0, shuffleAllocations: Map(), mountPoint: /mnt/disk1, usableSpace: 448284381184, avgFlushTime: 0, activeSlots: 0) status: HEALTHY dirs , /mnt/disk3=DiskInfo(maxSlots: 6716, committed shuffles 0, running applications 0, shuffleAllocations: Map(), mountPoint: /mnt/disk3, usableSpace: 450755608576, avgFlushTime: 0, activeSlots: 0) status: HEALTHY dirs , /mnt/disk2=DiskInfo(maxSlots: 6713, committed shuffles 0, running applications 0, shuffleAllocations: Map(), mountPoint: /mnt/disk2, usableSpace: 450532900864, avgFlushTime: 0, activeSlots: 0) status: HEALTHY dirs , /mnt/disk4=DiskInfo(maxSlots: 6712, committed shuffles 0, running applications 0, shuffleAllocations: Map(), mountPoint: /mnt/disk4, usableSpace: 450456805376, avgFlushTime: 0, activeSlots: 0) status: HEALTHY dirs } WorkerRef: null","title":"Deploy Celeborn"},{"location":"deploy/#deploy-spark-client","text":"Copy $CELEBORN_HOME/spark/*.jar to $SPARK_HOME/jars/ .","title":"Deploy Spark client"},{"location":"deploy/#spark-configuration","text":"To use Celeborn, the following spark configurations should be added. # Shuffle manager class name changed in 0.3.0: # before 0.3.0: `org.apache.spark.shuffle.celeborn.RssShuffleManager` # since 0.3.0: `org.apache.spark.shuffle.celeborn.SparkShuffleManager` spark.shuffle.manager org.apache.spark.shuffle.celeborn.SparkShuffleManager # must use kryo serializer because java serializer do not support relocation spark.serializer org.apache.spark.serializer.KryoSerializer # celeborn master spark.celeborn.master.endpoints clb-1 : 9097,clb-2:9097,clb-3:9097 # This is not necessary if your Spark external shuffle service is Spark 3.1 or newer spark.shuffle.service.enabled false # options: hash, sort # Hash shuffle writer use (partition count) * (celeborn.push.buffer.max.size) * (spark.executor.cores) memory. # Sort shuffle writer uses less memory than hash shuffle writer, if your shuffle partition count is large, try to use sort hash writer. spark.celeborn.client.spark.shuffle.writer hash # We recommend setting `spark.celeborn.client.push.replicate.enabled` to true to enable server-side data replication # If you have only one worker, this setting must be false # If your Celeborn is using HDFS, it's recommended to set this setting to false spark.celeborn.client.push.replicate.enabled true # Support for Spark AQE only tested under Spark 3 # we recommend setting localShuffleReader to false for getting better performance of Celeborn spark.sql.adaptive.localShuffleReader.enabled false # If Celeborn is using HDFS spark.celeborn.storage.hdfs.dir hdfs : //<namenode>/celeborn # we recommend enabling aqe support to gain better performance spark.sql.adaptive.enabled true spark.sql.adaptive.skewJoin.enabled true # Support Spark Dynamic Resource Allocation # Required Spark version >= 3.5.0 spark.shuffle.sort.io.plugin.class org.apache.spark.shuffle.celeborn.CelebornShuffleDataIO # Required Spark version >= 3.4.0, highly recommended to disable spark.dynamicAllocation.shuffleTracking.enabled false # Support ShuffleManager when defined in user jars # Required Spark version < 4.0.0 or without SPARK-45762, highly recommended to false for ShuffleManager in user-defined jar specified by --jars or spark.jars spark.executor.userClassPathFirst false","title":"Spark Configuration"},{"location":"deploy/#deploy-flink-client","text":"Copy $CELEBORN_HOME/flink/*.jar to $FLINK_HOME/lib/ .","title":"Deploy Flink client"},{"location":"deploy/#flink-configuration","text":"To use Celeborn, the following flink configurations should be added. shuffle-service-factory.class : org.apache.celeborn.plugin.flink.RemoteShuffleServiceFactory execution.batch-shuffle-mode : ALL_EXCHANGES_BLOCKING celeborn.master.endpoints : clb-1:9097,clb-2:9097,clb-3:9097 celeborn.client.shuffle.batchHandleReleasePartition.enabled : true celeborn.client.push.maxReqsInFlight : 128 # Network connections between peers celeborn.data.io.numConnectionsPerPeer : 16 # threads number may vary according to your cluster but do not set to 1 celeborn.data.io.threads : 32 celeborn.client.shuffle.batchHandleCommitPartition.threads : 32 celeborn.rpc.dispatcher.numThreads : 32 # Floating buffers may need to change `taskmanager.network.memory.fraction` and `taskmanager.network.memory.max` taskmanager.network.memory.floating-buffers-per-gate : 4096 taskmanager.network.memory.buffers-per-channel : 0 taskmanager.memory.task.off-heap.size : 512m Note : The config option execution.batch-shuffle-mode should configure as ALL_EXCHANGES_BLOCKING .","title":"Flink Configuration"},{"location":"deploy/#deploy-mapreduce-client","text":"Copy $CELEBORN_HOME/mr/*.jar into mapreduce.application.classpath and yarn.application.classpath . Meanwhile, configure the following settings in YARN and MapReduce config. -Dyarn.app.mapreduce.am.job.recovery.enable = false -Dmapreduce.job.reduce.slowstart.completedmaps = 1 -Dmapreduce.celeborn.master.endpoints = <master-1-1>:9097 -Dyarn.app.mapreduce.am.command-opts = org.apache.celeborn.mapreduce.v2.app.MRAppMasterWithCeleborn -Dmapreduce.job.map.output.collector.class = org.apache.hadoop.mapred.CelebornMapOutputCollector -Dmapreduce.job.reduce.shuffle.consumer.plugin.class = org.apache.hadoop.mapreduce.task.reduce.CelebornShuffleConsumer Note : MRAppMasterWithCeleborn supports setting mapreduce.celeborn.master.endpoints via environment variable CELEBORN_MASTER_ENDPOINTS . Meanwhile, MRAppMasterWithCeleborn disables yarn.app.mapreduce.am.job.recovery.enable and sets mapreduce.job.reduce.slowstart.completedmaps to 1 by default.","title":"Deploy MapReduce client"},{"location":"deploy_on_k8s/","text":"Deploy Celeborn on Kubernetes Celeborn currently supports rapid deployment by using helm. Before Deploy You should have a Running Kubernetes Cluster. You should understand simple Kubernetes deploy related, e.g. Kubernetes Resources . You have enough permissions to create resources . Installed Helm . Deploy 1. Get Celeborn Binary Package You can find released version of Celeborn on Downloading Page . Of course, you can build binary package from master branch or your own branch by using ./build/make-distribution.sh in source code. Notice: Celeborn supports automatic builds on linux aarch64 platform via aarch64 profile. aarch64 profile requires glibc version 3.4.21. There is potential problematic frame C [libc.so.6+0x8412a] for other glibc version like 2.x etc. Anyway, you should unzip and into binary package. 2. Modify Celeborn Configurations Notice: Celeborn Charts Template Files is in the experimental instability stage, the subsequent optimization will be adjusted. The configuration in ./charts/celeborn/values.yaml you should focus on modifying is: image repository - Get images from which repository image tag - Which version of image to use masterReplicas - Number of celeborn master replicas workerReplicas - Number of celeborn worker replicas volumes - How and where to mount volumes (For more information, Volumes ) [Optional] Build Celeborn Docker Image Maybe you want to make your own celeborn docker image, you can use docker build . -f docker/Dockerfile in Celeborn Binary. 3. Helm Install Celeborn Charts More details in Helm Install cd ./charts/celeborn helm install celeborn -n <namespace> . 4. Check Celeborn After the above operation, you should be able to find the corresponding Celeborn Master/Worker by kubectl get pods -n <namespace> Etc. NAME READY STATUS RESTARTS AGE celeborn-master-0 1/1 Running 0 1m ... celeborn-worker-0 1/1 Running 0 1m ... Given that Celeborn Master/Worker Pod takes time to start, you can see the following phenomenon: ** server can't find celeborn-master-0.celeborn-master-svc.default.svc.cluster.local: NXDOMAIN waiting for master Server: 172.17.0.10 Address: 172.17.0.10#53 ... Name: celeborn-master-0.celeborn-master-svc.default.svc.cluster.local Address: 10.225.139.80 Server: 172.17.0.10 Address: 172.17.0.10#53 starting org.apache.celeborn.service.deploy.master.Master, logging to /opt/celeborn/logs/celeborn--org.apache.celeborn.service.deploy.master.Master-1-celeborn-master-0.out ... 23/03/23 14:10:56,081 INFO [main] RaftServer: 0: start RPC server 23/03/23 14:10:56,132 INFO [nioEventLoopGroup-2-1] LoggingHandler: [id: 0x83032bf1] REGISTERED 23/03/23 14:10:56,132 INFO [nioEventLoopGroup-2-1] LoggingHandler: [id: 0x83032bf1] BIND: 0.0.0.0/0.0.0.0:9872 23/03/23 14:10:56,134 INFO [nioEventLoopGroup-2-1] LoggingHandler: [id: 0x83032bf1, L:/0:0:0:0:0:0:0:0:9872] ACTIVE 23/03/23 14:10:56,135 INFO [JvmPauseMonitor0] JvmPauseMonitor: JvmPauseMonitor-0: Started 23/03/23 14:10:56,208 INFO [main] Master: Metrics system enabled. 23/03/23 14:10:56,216 INFO [main] HttpServer: master: HttpServer started on port 9098. 23/03/23 14:10:56,216 INFO [main] Master: Master started. 5. Access Celeborn Service The Celeborn Master/Worker nodes deployed via official Helm charts run as StatefulSet , it can be accessed through Pod IP or Stable Network ID (DNS name) , in above case, the Master/Worker nodes can be accessed through: celeborn-master-0.celeborn-master-svc.default.svc.cluster.local` ... celeborn-worker-0.celeborn-worker-svc.default.svc.cluster.local` ... After a restart, the StatefulSet Pod IP changes but the DNS name remains, this is important for rolling upgrade. When bind address is not set explicitly, Celeborn worker is going to find the first non-loopback address to bind. By default, it uses IP address both for address binding and registering, that causes the Master and Client use the IP address to access the Worker, it's problematic after Worker restart as explained above, especially when Graceful Shutdown is enabled. You may want to set celeborn.network.bind.preferIpAddress=false to address such issue. Note that, depends on your Kubernetes network infrastructure, this may cause pressure on DNS service or other network issues compared with using IP address directly. 6. Build Celeborn Client Here, without going into detail on how to configure Spark/Flink/MapReduce to find celeborn master/worker, mention the key configuration: spark.celeborn.master.endpoints: celeborn-master-0.celeborn-master-svc.<namespace>:9097,celeborn-master-1.celeborn-master-svc.<namespace>:9097,celeborn-master-2.celeborn-master-svc.<namespace>:9097 You can find why config endpoints such way in Kubernetes DNS for Service And Pods Notice: You should ensure that Spark/Flink/MapReduce can find the Celeborn Master/Worker via IP or the Kubernetes DNS mentioned above","title":"Kubernetes"},{"location":"deploy_on_k8s/#deploy-celeborn-on-kubernetes","text":"Celeborn currently supports rapid deployment by using helm.","title":"Deploy Celeborn on Kubernetes"},{"location":"deploy_on_k8s/#before-deploy","text":"You should have a Running Kubernetes Cluster. You should understand simple Kubernetes deploy related, e.g. Kubernetes Resources . You have enough permissions to create resources . Installed Helm .","title":"Before Deploy"},{"location":"deploy_on_k8s/#deploy","text":"","title":"Deploy"},{"location":"deploy_on_k8s/#1-get-celeborn-binary-package","text":"You can find released version of Celeborn on Downloading Page . Of course, you can build binary package from master branch or your own branch by using ./build/make-distribution.sh in source code. Notice: Celeborn supports automatic builds on linux aarch64 platform via aarch64 profile. aarch64 profile requires glibc version 3.4.21. There is potential problematic frame C [libc.so.6+0x8412a] for other glibc version like 2.x etc. Anyway, you should unzip and into binary package.","title":"1. Get Celeborn Binary Package"},{"location":"deploy_on_k8s/#2-modify-celeborn-configurations","text":"Notice: Celeborn Charts Template Files is in the experimental instability stage, the subsequent optimization will be adjusted. The configuration in ./charts/celeborn/values.yaml you should focus on modifying is: image repository - Get images from which repository image tag - Which version of image to use masterReplicas - Number of celeborn master replicas workerReplicas - Number of celeborn worker replicas volumes - How and where to mount volumes (For more information, Volumes )","title":"2. Modify Celeborn Configurations"},{"location":"deploy_on_k8s/#optional-build-celeborn-docker-image","text":"Maybe you want to make your own celeborn docker image, you can use docker build . -f docker/Dockerfile in Celeborn Binary.","title":"[Optional] Build Celeborn Docker Image"},{"location":"deploy_on_k8s/#3-helm-install-celeborn-charts","text":"More details in Helm Install cd ./charts/celeborn helm install celeborn -n <namespace> .","title":"3. Helm Install Celeborn Charts"},{"location":"deploy_on_k8s/#4-check-celeborn","text":"After the above operation, you should be able to find the corresponding Celeborn Master/Worker by kubectl get pods -n <namespace> Etc. NAME READY STATUS RESTARTS AGE celeborn-master-0 1/1 Running 0 1m ... celeborn-worker-0 1/1 Running 0 1m ... Given that Celeborn Master/Worker Pod takes time to start, you can see the following phenomenon: ** server can't find celeborn-master-0.celeborn-master-svc.default.svc.cluster.local: NXDOMAIN waiting for master Server: 172.17.0.10 Address: 172.17.0.10#53 ... Name: celeborn-master-0.celeborn-master-svc.default.svc.cluster.local Address: 10.225.139.80 Server: 172.17.0.10 Address: 172.17.0.10#53 starting org.apache.celeborn.service.deploy.master.Master, logging to /opt/celeborn/logs/celeborn--org.apache.celeborn.service.deploy.master.Master-1-celeborn-master-0.out ... 23/03/23 14:10:56,081 INFO [main] RaftServer: 0: start RPC server 23/03/23 14:10:56,132 INFO [nioEventLoopGroup-2-1] LoggingHandler: [id: 0x83032bf1] REGISTERED 23/03/23 14:10:56,132 INFO [nioEventLoopGroup-2-1] LoggingHandler: [id: 0x83032bf1] BIND: 0.0.0.0/0.0.0.0:9872 23/03/23 14:10:56,134 INFO [nioEventLoopGroup-2-1] LoggingHandler: [id: 0x83032bf1, L:/0:0:0:0:0:0:0:0:9872] ACTIVE 23/03/23 14:10:56,135 INFO [JvmPauseMonitor0] JvmPauseMonitor: JvmPauseMonitor-0: Started 23/03/23 14:10:56,208 INFO [main] Master: Metrics system enabled. 23/03/23 14:10:56,216 INFO [main] HttpServer: master: HttpServer started on port 9098. 23/03/23 14:10:56,216 INFO [main] Master: Master started.","title":"4. Check Celeborn"},{"location":"deploy_on_k8s/#5-access-celeborn-service","text":"The Celeborn Master/Worker nodes deployed via official Helm charts run as StatefulSet , it can be accessed through Pod IP or Stable Network ID (DNS name) , in above case, the Master/Worker nodes can be accessed through: celeborn-master-0.celeborn-master-svc.default.svc.cluster.local` ... celeborn-worker-0.celeborn-worker-svc.default.svc.cluster.local` ... After a restart, the StatefulSet Pod IP changes but the DNS name remains, this is important for rolling upgrade. When bind address is not set explicitly, Celeborn worker is going to find the first non-loopback address to bind. By default, it uses IP address both for address binding and registering, that causes the Master and Client use the IP address to access the Worker, it's problematic after Worker restart as explained above, especially when Graceful Shutdown is enabled. You may want to set celeborn.network.bind.preferIpAddress=false to address such issue. Note that, depends on your Kubernetes network infrastructure, this may cause pressure on DNS service or other network issues compared with using IP address directly.","title":"5. Access Celeborn Service"},{"location":"deploy_on_k8s/#6-build-celeborn-client","text":"Here, without going into detail on how to configure Spark/Flink/MapReduce to find celeborn master/worker, mention the key configuration: spark.celeborn.master.endpoints: celeborn-master-0.celeborn-master-svc.<namespace>:9097,celeborn-master-1.celeborn-master-svc.<namespace>:9097,celeborn-master-2.celeborn-master-svc.<namespace>:9097 You can find why config endpoints such way in Kubernetes DNS for Service And Pods Notice: You should ensure that Spark/Flink/MapReduce can find the Celeborn Master/Worker via IP or the Kubernetes DNS mentioned above","title":"6. Build Celeborn Client"},{"location":"migration/","text":"Migration Guide Upgrading from 0.5.0 to 0.5.1 Since 0.5.1, Celeborn master REST API /exclude request uses media type application/x-www-form-urlencoded instead of text/plain . Since 0.5.1, Celeborn master REST API /sendWorkerEvent request uses POST method and the parameters type and workers use form parameters instead, and uses media type application/x-www-form-urlencoded instead of text/plain . Since 0.5.1, Celeborn worker REST API /exit request uses media type application/x-www-form-urlencoded instead of text/plain . Upgrading from 0.4 to 0.5 Since 0.5.0, Celeborn master metrics LostWorkers is renamed as LostWorkerCount . Since 0.5.0, Celeborn worker metrics ChunkStreamCount is renamed as ActiveChunkStreamCount . Since 0.5.0, Celeborn worker metrics CreditStreamCount is renamed as ActiveCreditStreamCount . Since 0.5.0, Celeborn configurations support new tag isDynamic to represent whether the configuration is dynamic config. Since 0.5.0, Celeborn changed the default value of celeborn.worker.graceful.shutdown.recoverDbBackend from LEVELDB to ROCKSDB , which means Celeborn will use RocksDB store for recovery backend. To restore the behavior before Celeborn 0.5, you can set celeborn.worker.graceful.shutdown.recoverDbBackend to LEVELDB . Since 0.5.0, Celeborn deprecate celeborn.quota.configuration.path . Please use celeborn.dynamicConfig.store.fs.path instead. Since 0.5.0, Celeborn client removes configuration celeborn.client.push.splitPartition.threads , celeborn.client.flink.inputGate.minMemory and celeborn.client.flink.resultPartition.minMemory . Since 0.5.0, Celeborn deprecate celeborn.client.spark.shuffle.forceFallback.enabled . Please use celeborn.client.spark.shuffle.fallback.policy instead. Since 0.5.0, Celeborn master REST API /exclude uses POST method and the parameters add and remove use form parameters instead. Since 0.5.0, Celeborn worker REST API /exit uses POST method and the parameter type uses form parameter instead. Upgrading from 0.4.0 to 0.4.1 Since 0.4.1, Celeborn master adds a limit to the estimated partition size used for computing worker slots. This size is now constrained within the range specified by celeborn.master.estimatedPartitionSize.minSize and celeborn.master.estimatedPartitionSize.maxSize . Since 0.4.1, Celeborn changed the fallback configuration of celeborn.client.rpc.getReducerFileGroup.askTimeout , celeborn.client.rpc.registerShuffle.askTimeout and celeborn.client.rpc.requestPartition.askTimeout from celeborn.<module>.io.connectionTimeout to celeborn.rpc.askTimeout . Upgrading from 0.3 to 0.4 Since 0.4.0, Celeborn won't be compatible with Celeborn client that versions below 0.3.0. Note that: It's strongly recommended to use the same version of Client and Celeborn Master/Worker in production. Since 0.4.0, Celeborn won't support org.apache.spark.shuffle.celeborn.RssShuffleManager . Since 0.4.0, Celeborn changed the default value of celeborn.<module>.io.numConnectionsPerPeer from 2 to 1 . Since 0.4.0, Celeborn has changed the names of the prometheus master and worker configuration as shown in the table below: Key Before v0.4.0 Key After v0.4.0 celeborn.metrics.master.prometheus.host celeborn.master.http.host celeborn.metrics.master.prometheus.port celeborn.master.http.port celeborn.metrics.worker.prometheus.host celeborn.worker.http.host celeborn.metrics.worker.prometheus.port celeborn.worker.http.port Since 0.4.0, Celeborn deprecate celeborn.worker.storage.baseDir.prefix and celeborn.worker.storage.baseDir.number . Please use celeborn.worker.storage.dirs instead. Since 0.4.0, Celeborn deprecate celeborn.storage.activeTypes . Please use celeborn.storage.availableTypes instead. Since 0.4.0, Celeborn worker removes configuration celeborn.worker.userResourceConsumption.update.interval . Since 0.4.0, Celeborn master metrics PartitionWritten is renamed as ActiveShuffleSize . Since 0.4.0, Celeborn master metrics PartitionFileCount is renamed as ActiveShuffleFileCount . Upgrading from 0.3.1 to 0.3.2 Since 0.3.1, Celeborn changed the default value of raft.client.rpc.request.timeout from 3s to 10s . Since 0.3.1, Celeborn changed the default value of raft.client.rpc.watch.request.timeout from 10s to 20s . Upgrading from 0.3.0 to 0.3.1 Since 0.3.1, Celeborn changed the default value of celeborn.worker.directMemoryRatioToResume from 0.5 to 0.7 . Since 0.3.1, Celeborn changed the default value of celeborn.worker.monitor.disk.check.interval from 60 to 30 . Since 0.3.1, name of JVM metrics changed, see details at CELEBORN-1007. Upgrading from 0.2 to 0.3 Celeborn 0.2 Client is compatible with 0.3 Master/Server, it allows to upgrade Master/Worker first then Client. Note that: It's strongly recommended to use the same version of Client and Celeborn Master/Worker in production. Since 0.3.0, the support of deprecated configurations rss.* is removed. All configurations listed in 0.2.1 docs still take effect, but some of those are deprecated too, please read the bootstrap logs and follow the suggestion to migrate to the new configuration. From 0.3.0 on the default value for celeborn.client.push.replicate.enabled is changed from true to false , users who want replication on should explicitly enable replication. For example, to enable replication for Spark users should add the spark config when submitting job: spark.celeborn.client.push.replicate.enabled=true From 0.3.0 on the default value for celeborn.worker.storage.workingDir is changed from hadoop/rss-worker/shuffle_data to celeborn-worker/shuffle_data , users who want to use origin working dir path should set this configuration. Since 0.3.0, configuration namespace celeborn.ha.master is deprecated, and will be removed in the future versions. All configurations celeborn.ha.master.* should migrate to celeborn.master.ha.* . Since 0.3.0, environment variables CELEBORN_MASTER_HOST and CELEBORN_MASTER_PORT are removed. Instead CELEBORN_LOCAL_HOSTNAME works on both master and worker, which takes high priority than configurations defined in properties file. Since 0.3.0, the Celeborn Master URL schema is changed from rss:// to celeborn:// , for users who start Worker by sbin/start-worker.sh rss://<master-host>:<master-port> , should migrate to sbin/start-worker.sh celeborn://<master-host>:<master-port> . Since 0.3.0, Celeborn supports overriding Hadoop configuration( core-site.xml , hdfs-site.xml , etc.) from Celeborn configuration with the additional prefix celeborn.hadoop. . On Spark client side, user should set Hadoop configuration like spark.celeborn.hadoop.foo=bar , note that spark.hadoop.foo=bar does not take effect; on Flink client and Celeborn Master/Worker side, user should set like celeborn.hadoop.foo=bar . Since 0.3.0, Celeborn master metrics BlacklistedWorkerCount is renamed as ExcludedWorkerCount . Since 0.3.0, Celeborn master http request url /blacklistedWorkers is renamed as /excludedWorkers . Since 0.3.0, introduces a terminology update for Celeborn worker data replication, replacing the previous master/slave terminology with primary/replica . In alignment with this change, corresponding metrics keywords have been adjusted. The following table presents a comprehensive overview of the changes: Key Before v0.3.0 Key After v0.3.0 MasterPushDataTime PrimaryPushDataTime MasterPushDataHandshakeTime PrimaryPushDataHandshakeTime MasterRegionStartTime PrimaryRegionStartTime MasterRegionFinishTime PrimaryRegionFinishTime SlavePushDataTime ReplicaPushDataTime SlavePushDataHandshakeTime ReplicaPushDataHandshakeTime SlaveRegionStartTime ReplicaRegionStartTime SlaveRegionFinishTime ReplicaRegionFinishTime Since 0.3.0, Celeborn's spark shuffle manager change from org.apache.spark.shuffle.celeborn.RssShuffleManager to org.apache.spark.shuffle.celeborn.SparkShuffleManager . User can set spark property spark.shuffle.manager to org.apache.spark.shuffle.celeborn.SparkShuffleManager to use Celeborn remote shuffle service. In 0.3.0, Celeborn still support org.apache.spark.shuffle.celeborn.RssShuffleManager , it will be removed in 0.4.0.","title":"Migration Guide"},{"location":"migration/#migration-guide","text":"","title":"Migration Guide"},{"location":"migration/#upgrading-from-050-to-051","text":"Since 0.5.1, Celeborn master REST API /exclude request uses media type application/x-www-form-urlencoded instead of text/plain . Since 0.5.1, Celeborn master REST API /sendWorkerEvent request uses POST method and the parameters type and workers use form parameters instead, and uses media type application/x-www-form-urlencoded instead of text/plain . Since 0.5.1, Celeborn worker REST API /exit request uses media type application/x-www-form-urlencoded instead of text/plain .","title":"Upgrading from 0.5.0 to 0.5.1"},{"location":"migration/#upgrading-from-04-to-05","text":"Since 0.5.0, Celeborn master metrics LostWorkers is renamed as LostWorkerCount . Since 0.5.0, Celeborn worker metrics ChunkStreamCount is renamed as ActiveChunkStreamCount . Since 0.5.0, Celeborn worker metrics CreditStreamCount is renamed as ActiveCreditStreamCount . Since 0.5.0, Celeborn configurations support new tag isDynamic to represent whether the configuration is dynamic config. Since 0.5.0, Celeborn changed the default value of celeborn.worker.graceful.shutdown.recoverDbBackend from LEVELDB to ROCKSDB , which means Celeborn will use RocksDB store for recovery backend. To restore the behavior before Celeborn 0.5, you can set celeborn.worker.graceful.shutdown.recoverDbBackend to LEVELDB . Since 0.5.0, Celeborn deprecate celeborn.quota.configuration.path . Please use celeborn.dynamicConfig.store.fs.path instead. Since 0.5.0, Celeborn client removes configuration celeborn.client.push.splitPartition.threads , celeborn.client.flink.inputGate.minMemory and celeborn.client.flink.resultPartition.minMemory . Since 0.5.0, Celeborn deprecate celeborn.client.spark.shuffle.forceFallback.enabled . Please use celeborn.client.spark.shuffle.fallback.policy instead. Since 0.5.0, Celeborn master REST API /exclude uses POST method and the parameters add and remove use form parameters instead. Since 0.5.0, Celeborn worker REST API /exit uses POST method and the parameter type uses form parameter instead.","title":"Upgrading from 0.4 to 0.5"},{"location":"migration/#upgrading-from-040-to-041","text":"Since 0.4.1, Celeborn master adds a limit to the estimated partition size used for computing worker slots. This size is now constrained within the range specified by celeborn.master.estimatedPartitionSize.minSize and celeborn.master.estimatedPartitionSize.maxSize . Since 0.4.1, Celeborn changed the fallback configuration of celeborn.client.rpc.getReducerFileGroup.askTimeout , celeborn.client.rpc.registerShuffle.askTimeout and celeborn.client.rpc.requestPartition.askTimeout from celeborn.<module>.io.connectionTimeout to celeborn.rpc.askTimeout .","title":"Upgrading from 0.4.0 to 0.4.1"},{"location":"migration/#upgrading-from-03-to-04","text":"Since 0.4.0, Celeborn won't be compatible with Celeborn client that versions below 0.3.0. Note that: It's strongly recommended to use the same version of Client and Celeborn Master/Worker in production. Since 0.4.0, Celeborn won't support org.apache.spark.shuffle.celeborn.RssShuffleManager . Since 0.4.0, Celeborn changed the default value of celeborn.<module>.io.numConnectionsPerPeer from 2 to 1 . Since 0.4.0, Celeborn has changed the names of the prometheus master and worker configuration as shown in the table below: Key Before v0.4.0 Key After v0.4.0 celeborn.metrics.master.prometheus.host celeborn.master.http.host celeborn.metrics.master.prometheus.port celeborn.master.http.port celeborn.metrics.worker.prometheus.host celeborn.worker.http.host celeborn.metrics.worker.prometheus.port celeborn.worker.http.port Since 0.4.0, Celeborn deprecate celeborn.worker.storage.baseDir.prefix and celeborn.worker.storage.baseDir.number . Please use celeborn.worker.storage.dirs instead. Since 0.4.0, Celeborn deprecate celeborn.storage.activeTypes . Please use celeborn.storage.availableTypes instead. Since 0.4.0, Celeborn worker removes configuration celeborn.worker.userResourceConsumption.update.interval . Since 0.4.0, Celeborn master metrics PartitionWritten is renamed as ActiveShuffleSize . Since 0.4.0, Celeborn master metrics PartitionFileCount is renamed as ActiveShuffleFileCount .","title":"Upgrading from 0.3 to 0.4"},{"location":"migration/#upgrading-from-031-to-032","text":"Since 0.3.1, Celeborn changed the default value of raft.client.rpc.request.timeout from 3s to 10s . Since 0.3.1, Celeborn changed the default value of raft.client.rpc.watch.request.timeout from 10s to 20s .","title":"Upgrading from 0.3.1 to 0.3.2"},{"location":"migration/#upgrading-from-030-to-031","text":"Since 0.3.1, Celeborn changed the default value of celeborn.worker.directMemoryRatioToResume from 0.5 to 0.7 . Since 0.3.1, Celeborn changed the default value of celeborn.worker.monitor.disk.check.interval from 60 to 30 . Since 0.3.1, name of JVM metrics changed, see details at CELEBORN-1007.","title":"Upgrading from 0.3.0 to 0.3.1"},{"location":"migration/#upgrading-from-02-to-03","text":"Celeborn 0.2 Client is compatible with 0.3 Master/Server, it allows to upgrade Master/Worker first then Client. Note that: It's strongly recommended to use the same version of Client and Celeborn Master/Worker in production. Since 0.3.0, the support of deprecated configurations rss.* is removed. All configurations listed in 0.2.1 docs still take effect, but some of those are deprecated too, please read the bootstrap logs and follow the suggestion to migrate to the new configuration. From 0.3.0 on the default value for celeborn.client.push.replicate.enabled is changed from true to false , users who want replication on should explicitly enable replication. For example, to enable replication for Spark users should add the spark config when submitting job: spark.celeborn.client.push.replicate.enabled=true From 0.3.0 on the default value for celeborn.worker.storage.workingDir is changed from hadoop/rss-worker/shuffle_data to celeborn-worker/shuffle_data , users who want to use origin working dir path should set this configuration. Since 0.3.0, configuration namespace celeborn.ha.master is deprecated, and will be removed in the future versions. All configurations celeborn.ha.master.* should migrate to celeborn.master.ha.* . Since 0.3.0, environment variables CELEBORN_MASTER_HOST and CELEBORN_MASTER_PORT are removed. Instead CELEBORN_LOCAL_HOSTNAME works on both master and worker, which takes high priority than configurations defined in properties file. Since 0.3.0, the Celeborn Master URL schema is changed from rss:// to celeborn:// , for users who start Worker by sbin/start-worker.sh rss://<master-host>:<master-port> , should migrate to sbin/start-worker.sh celeborn://<master-host>:<master-port> . Since 0.3.0, Celeborn supports overriding Hadoop configuration( core-site.xml , hdfs-site.xml , etc.) from Celeborn configuration with the additional prefix celeborn.hadoop. . On Spark client side, user should set Hadoop configuration like spark.celeborn.hadoop.foo=bar , note that spark.hadoop.foo=bar does not take effect; on Flink client and Celeborn Master/Worker side, user should set like celeborn.hadoop.foo=bar . Since 0.3.0, Celeborn master metrics BlacklistedWorkerCount is renamed as ExcludedWorkerCount . Since 0.3.0, Celeborn master http request url /blacklistedWorkers is renamed as /excludedWorkers . Since 0.3.0, introduces a terminology update for Celeborn worker data replication, replacing the previous master/slave terminology with primary/replica . In alignment with this change, corresponding metrics keywords have been adjusted. The following table presents a comprehensive overview of the changes: Key Before v0.3.0 Key After v0.3.0 MasterPushDataTime PrimaryPushDataTime MasterPushDataHandshakeTime PrimaryPushDataHandshakeTime MasterRegionStartTime PrimaryRegionStartTime MasterRegionFinishTime PrimaryRegionFinishTime SlavePushDataTime ReplicaPushDataTime SlavePushDataHandshakeTime ReplicaPushDataHandshakeTime SlaveRegionStartTime ReplicaRegionStartTime SlaveRegionFinishTime ReplicaRegionFinishTime Since 0.3.0, Celeborn's spark shuffle manager change from org.apache.spark.shuffle.celeborn.RssShuffleManager to org.apache.spark.shuffle.celeborn.SparkShuffleManager . User can set spark property spark.shuffle.manager to org.apache.spark.shuffle.celeborn.SparkShuffleManager to use Celeborn remote shuffle service. In 0.3.0, Celeborn still support org.apache.spark.shuffle.celeborn.RssShuffleManager , it will be removed in 0.4.0.","title":"Upgrading from 0.2 to 0.3"},{"location":"monitoring/","text":"Monitoring There are two ways to monitor Celeborn cluster: Prometheus metrics and REST API. Metrics Celeborn has a configurable metrics system based on the Dropwizard Metrics Library . This allows users to report Celeborn metrics to a variety of sinks including HTTP, JMX, CSV files and prometheus servlet. The metrics are generated by sources embedded in the Celeborn code base. They provide instrumentation for specific activities and Celeborn components. The metrics system is configured via a configuration file that Celeborn expects to be present at $CELEBORN_HOME/conf/metrics.properties . A custom file location can be specified via the celeborn.metrics.conf configuration property . Instead of using the configuration file, a set of configuration parameters with prefix celeborn.metrics.conf. can be used. Celeborn's metrics are divided into two instances corresponding to Celeborn components. The following instances are currently supported: master : The Celeborn cluster master process. worker : The Celeborn cluster worker process. Each instance can report to zero or more sinks . Sinks are contained in the org.apache.celeborn.common.metrics.sink package: CSVSink : Exports metrics data to CSV files at regular intervals. PrometheusServlet : Adds a servlet within the existing Celeborn REST API to serve metrics data in Prometheus format. GraphiteSink : Sends metrics to a Graphite node. The syntax of the metrics configuration file and the parameters available for each sink are defined in an example configuration file, $CELEBORN_HOME/conf/metrics.properties.template . When using Celeborn configuration parameters instead of the metrics configuration file, the relevant parameter names are composed by the prefix celeborn.metrics.conf. followed by the configuration details, i.e. the parameters take the following form: celeborn.metrics.conf.[instance|*].sink.[sink_name].[parameter_name] . This example shows a list of Celeborn configuration parameters for a CSV sink: \"celeborn.metrics.conf.*.sink.csv.class\"=\"org.apache.celeborn.common.metrics.sink.CsvSink\" \"celeborn.metrics.conf.*.sink.csv.period\"=\"1\" \"celeborn.metrics.conf.*.sink.csv.unit\"=minutes \"celeborn.metrics.conf.*.sink.csv.directory\"=/tmp/ Default values of the Celeborn metrics configuration are as follows: *.sink.prometheusServlet.class=org.apache.celeborn.common.metrics.sink.PrometheusServlet Additional sources can be configured using the metrics configuration file or the configuration parameter celeborn.metrics.conf.[component_name].source.jvm.class=[source_name] . At present the no source is the available optional source. For example the following configuration parameter activates the Example source: \"celeborn.metrics.conf.*.source.jvm.class\"=\"org.apache.celeborn.common.metrics.source.ExampleSource\" Available metrics providers Metrics used by Celeborn are of multiple types: gauge, counter, histogram, meter and timer, see Dropwizard library documentation for details . The following list of components and metrics reports the name and some details about the available metrics, grouped per component instance and source namespace. The most common time of metrics used in Celeborn instrumentation are gauges and counters. Counters can be recognized as they have the .count suffix. Timers, meters and histograms are annotated in the list, the rest of the list elements are metrics of type gauge. The large majority of metrics are active as soon as their parent component instance is configured, some metrics require also to be enabled via an additional configuration parameter, the details are reported in the list. Master These metrics are exposed by Celeborn master. namespace=master RegisteredShuffleCount DeviceCelebornFreeBytes DeviceCelebornTotalBytes RunningApplicationCount ActiveShuffleSize The active shuffle size of workers. ActiveShuffleFileCount The active shuffle file count of workers. WorkerCount LostWorkerCount ExcludedWorkerCount ShutdownWorkerCount IsActiveMaster PartitionSize The size of estimated shuffle partition. OfferSlotsTime The time for masters to handle RequestSlots request when registering shuffle. namespace=CPU JVMCPUTime namespace=system LastMinuteSystemLoad The average system load for the last minute. AvailableProcessors namespace=JVM This source provides information on JVM metrics using the Dropwizard/Codahale Metric Sets for JVM instrumentation and in particular the metric sets BufferPoolMetricSet, GarbageCollectorMetricSet and MemoryUsageGaugeSet. namespace=ResourceConsumption notes: This metrics data is generated for each user and they are identified using a metric tag. This metrics also include subResourceConsumptions generated for each application of user and they are identified using applicationId tag. diskFileCount diskBytesWritten hdfsFileCount hdfsBytesWritten namespace=ThreadPool notes: This metrics data is generated for each thread pool and they are identified using a metric tag by thread pool name. active_thread_count pending_task_count pool_size core_pool_size maximum_pool_size largest_pool_size is_terminating is_terminated is_shutdown thread_count thread_is_terminated_count thread_is_shutdown_count Worker These metrics are exposed by Celeborn worker. namespace=worker RegisteredShuffleCount RunningApplicationCount ActiveShuffleSize The active shuffle size of a worker including master replica and slave replica. ActiveShuffleFileCount The active shuffle file count of a worker including master replica and slave replica. OpenStreamTime The time for a worker to process openStream RPC and return StreamHandle. FetchChunkTime The time for a worker to fetch a chunk which is 8MB by default from a reduced partition. ActiveChunkStreamCount Active stream count for reduce partition reading streams. OpenStreamSuccessCount OpenStreamFailCount FetchChunkSuccessCount FetchChunkFailCount PrimaryPushDataTime The time for a worker to handle a pushData RPC sent from a celeborn client. ReplicaPushDataTime The time for a worker to handle a pushData RPC sent from a celeborn worker by replicating. WriteDataHardSplitCount WriteDataSuccessCount WriteDataFailCount ReplicateDataFailCount ReplicateDataWriteFailCount ReplicateDataCreateConnectionFailCount ReplicateDataConnectionExceptionCount ReplicateDataFailNonCriticalCauseCount ReplicateDataTimeoutCount PushDataHandshakeFailCount RegionStartFailCount RegionFinishFailCount PrimaryPushDataHandshakeTime ReplicaPushDataHandshakeTime PrimaryRegionStartTime ReplicaRegionStartTime PrimaryRegionFinishTime ReplicaRegionFinishTime PausePushDataTime The time for a worker to stop receiving pushData from clients because of back pressure. PausePushDataAndReplicateTime The time for a worker to stop receiving pushData from clients and other workers because of back pressure. PausePushData The count for a worker to stop receiving pushData from clients because of back pressure. PausePushDataAndReplicate The count for a worker to stop receiving pushData from clients and other workers because of back pressure. TakeBufferTime The time for a worker to take out a buffer from a disk flusher. FlushDataTime The time for a worker to write a buffer which is 256KB by default to storage. CommitFilesTime The time for a worker to flush buffers and close files related to specified shuffle. SlotsAllocated ActiveSlotsCount The number of slots currently being used in a worker ReserveSlotsTime ActiveConnectionCount NettyMemory The total amount of off-heap memory used by celeborn worker. SortTime The time for a worker to sort a shuffle file. SortMemory The memory used by sorting shuffle files. SortingFiles SortedFiles SortedFileSize DiskBuffer The memory occupied by pushData and pushMergedData which should be written to disk. BufferStreamReadBuffer The memory used by credit stream read buffer. ReadBufferDispatcherRequestsLength The queue size of read buffer allocation requests. ReadBufferAllocatedCount Allocated read buffer count. ActiveCreditStreamCount Active stream count for map partition reading streams. ActiveMapPartitionCount CleanTaskQueueSize CleanExpiredShuffleKeysTime The time for a worker to clean up shuffle data of expired shuffle keys. DeviceOSFreeBytes DeviceOSTotalBytes DeviceCelebornFreeBytes DeviceCelebornTotalBytes PotentialConsumeSpeed UserProduceSpeed WorkerConsumeSpeed push_server_usedHeapMemory push_server_usedDirectMemory push_server_numAllocations push_server_numTinyAllocations push_server_numSmallAllocations push_server_numNormalAllocations push_server_numHugeAllocations push_server_numDeallocations push_server_numTinyDeallocations push_server_numSmallDeallocations push_server_numNormalDeallocations push_server_numHugeDeallocations push_server_numActiveAllocations push_server_numActiveTinyAllocations push_server_numActiveSmallAllocations push_server_numActiveNormalAllocations push_server_numActiveHugeAllocations push_server_numActiveBytes replicate_server_usedHeapMemory replicate_server_usedDirectMemory replicate_server_numAllocations replicate_server_numTinyAllocations replicate_server_numSmallAllocations replicate_server_numNormalAllocations replicate_server_numHugeAllocations replicate_server_numDeallocations replicate_server_numTinyDeallocations replicate_server_numSmallDeallocations replicate_server_numNormalDeallocations replicate_server_numHugeDeallocations replicate_server_numActiveAllocations replicate_server_numActiveTinyAllocations replicate_server_numActiveSmallAllocations replicate_server_numActiveNormalAllocations replicate_server_numActiveHugeAllocations replicate_server_numActiveBytes fetch_server_usedHeapMemory fetch_server_usedDirectMemory fetch_server_numAllocations fetch_server_numTinyAllocations fetch_server_numSmallAllocations fetch_server_numNormalAllocations fetch_server_numHugeAllocations fetch_server_numDeallocations fetch_server_numTinyDeallocations fetch_server_numSmallDeallocations fetch_server_numNormalDeallocations fetch_server_numHugeDeallocations fetch_server_numActiveAllocations fetch_server_numActiveTinyAllocations fetch_server_numActiveSmallAllocations fetch_server_numActiveNormalAllocations fetch_server_numActiveHugeAllocations fetch_server_numActiveBytes namespace=CPU JVMCPUTime namespace=system LastMinuteSystemLoad Returns the system load average for the last minute. AvailableProcessors namespace=JVM This source provides information on JVM metrics using the Dropwizard/Codahale Metric Sets for JVM instrumentation and in particular the metric sets BufferPoolMetricSet, GarbageCollectorMetricSet and MemoryUsageGaugeSet. namespace=ResourceConsumption notes: This metrics data is generated for each user and they are identified using a metric tag. This metrics also include subResourceConsumptions generated for each application of user and they are identified using applicationId tag. diskFileCount diskBytesWritten hdfsFileCount hdfsBytesWritten namespace=ThreadPool notes: This metrics data is generated for each thread pool and they are identified using a metric tag by thread pool name. active_thread_count pending_task_count pool_size core_pool_size maximum_pool_size largest_pool_size is_terminating is_terminated is_shutdown Note: The Netty DirectArenaMetrics named like push/fetch/replicate_server_numXX are not exposed by default, nor in Grafana dashboard. If there is a need, you can enable celeborn.network.memory.allocator.verbose.metric to expose these metrics. REST API In addition to viewing the metrics, Celeborn also support REST API. This gives developers an easy way to create new visualizations and monitoring tools for Celeborn and also easy for users to get the running status of the service. The REST API is available for both master and worker. The endpoints are mounted at host:port . For example, for the master, they would typically be accessible at http://<master-http-host>:<master-http-port><path> , and for the worker, at http://<worker-http-host>:<worker-http-port><path> . The configuration of <master-http-host> , <master-http-port> , <worker-http-host> , <worker-http--port> as below: Key Default Description Since celeborn.master.http.host 0.0.0.0 Master's http host. 0.4.0 celeborn.master.http.port 9098 Master's http port. 0.4.0 celeborn.worker.http.host 0.0.0.0 Worker's http host. 0.4.0 celeborn.worker.http.port 9096 Worker's http port. 0.4.0 Available API providers API path listed as below: Master Path Method Parameters Meaning /applications GET List all running application's ids of the cluster. /conf GET List the conf setting of the master. /excludedWorkers GET List all excluded workers of the master. /help GET List the available API providers of the master. /hostnames GET List all running application's LifecycleManager's hostnames of the cluster. /listDynamicConfigs GET level=${LEVEL} tenant=${TENANT} name=${NAME} List the dynamic configs of the master. The parameter level specifies the config level of dynamic configs. The parameter tenant specifies the tenant id of TENANT or TENANT_USER level. The parameter name specifies the user name of TENANT_USER level. Meanwhile, either none or all of the parameter tenant and name are specified for TENANT_USER level. /listTopDiskUsedApps GET List the top disk usage application ids. It will return the top disk usage application ids for the cluster. /lostWorkers GET List all lost workers of the master. /masterGroupInfo GET List master group information of the service. It will list all master's LEADER, FOLLOWER information. /metrics/prometheus GET List the metrics data in prometheus format of the master. The url path is defined by configure celeborn.metrics.prometheus.path . /shuffles GET List all running shuffle keys of the service. It will return all running shuffle's key of the cluster. /shutdownWorkers GET List all shutdown workers of the master. /threadDump GET List the current thread dump of the master. /workerEventInfo GET List all worker event information of the master. /workerInfo GET List worker information of the service. It will list all registered workers' information. /exclude POST add=${ADD_WORKERS} remove=${REMOVE_WORKERS} Excluded workers of the master add or remove the worker manually given worker id. The parameter add or remove specifies the excluded workers to add or remove, which value is separated by commas. /sendWorkerEvent POST type=${EVENT_TYPE} workers=${WORKERS} For Master(Leader) can send worker event to manager workers. Legal type s are 'None', 'Immediately', 'Decommission', 'DecommissionThenIdle', 'Graceful', 'Recommission', and the parameter workers is separated by commas. Worker Path Method Parameters Meaning /applications GET List all running application's ids of the worker. It only return application ids running in that worker. /conf GET List the conf setting of the worker. /help GET List the available API providers of the worker. /isRegistered GET Show if the worker is registered to the master success. /isShutdown GET Show if the worker is during the process of shutdown. /listDynamicConfigs GET level=${LEVEL} tenant=${TENANT} name=${NAME} List the dynamic configs of the worker. The parameter level specifies the config level of dynamic configs. The parameter tenant specifies the tenant id of TENANT or TENANT_USER level. The parameter name specifies the user name of TENANT_USER level. Meanwhile, either none or all of the parameter tenant and name are specified for TENANT_USER level. /listPartitionLocationInfo GET List all the living PartitionLocation information in that worker. /listTopDiskUsedApps GET List the top disk usage application ids. It only return application ids running in that worker. /metrics/prometheus GET List the metrics data in prometheus format of the worker. The url path is defined by configure celeborn.metrics.prometheus.path . /shuffles GET List all the running shuffle keys of the worker. It only return keys of shuffles running in that worker. /threadDump GET List the current thread dump of the worker. /unavailablePeers GET List the unavailable peers of the worker, this always means the worker connect to the peer failed. /workerInfo GET List the worker information of the worker. /exit POST type=${EXIT_TYPE} Trigger this worker to exit. Legal type s are 'Decommission', 'Graceful' and 'Immediately'.","title":"Monitoring"},{"location":"monitoring/#monitoring","text":"There are two ways to monitor Celeborn cluster: Prometheus metrics and REST API.","title":"Monitoring"},{"location":"monitoring/#metrics","text":"Celeborn has a configurable metrics system based on the Dropwizard Metrics Library . This allows users to report Celeborn metrics to a variety of sinks including HTTP, JMX, CSV files and prometheus servlet. The metrics are generated by sources embedded in the Celeborn code base. They provide instrumentation for specific activities and Celeborn components. The metrics system is configured via a configuration file that Celeborn expects to be present at $CELEBORN_HOME/conf/metrics.properties . A custom file location can be specified via the celeborn.metrics.conf configuration property . Instead of using the configuration file, a set of configuration parameters with prefix celeborn.metrics.conf. can be used. Celeborn's metrics are divided into two instances corresponding to Celeborn components. The following instances are currently supported: master : The Celeborn cluster master process. worker : The Celeborn cluster worker process. Each instance can report to zero or more sinks . Sinks are contained in the org.apache.celeborn.common.metrics.sink package: CSVSink : Exports metrics data to CSV files at regular intervals. PrometheusServlet : Adds a servlet within the existing Celeborn REST API to serve metrics data in Prometheus format. GraphiteSink : Sends metrics to a Graphite node. The syntax of the metrics configuration file and the parameters available for each sink are defined in an example configuration file, $CELEBORN_HOME/conf/metrics.properties.template . When using Celeborn configuration parameters instead of the metrics configuration file, the relevant parameter names are composed by the prefix celeborn.metrics.conf. followed by the configuration details, i.e. the parameters take the following form: celeborn.metrics.conf.[instance|*].sink.[sink_name].[parameter_name] . This example shows a list of Celeborn configuration parameters for a CSV sink: \"celeborn.metrics.conf.*.sink.csv.class\"=\"org.apache.celeborn.common.metrics.sink.CsvSink\" \"celeborn.metrics.conf.*.sink.csv.period\"=\"1\" \"celeborn.metrics.conf.*.sink.csv.unit\"=minutes \"celeborn.metrics.conf.*.sink.csv.directory\"=/tmp/ Default values of the Celeborn metrics configuration are as follows: *.sink.prometheusServlet.class=org.apache.celeborn.common.metrics.sink.PrometheusServlet Additional sources can be configured using the metrics configuration file or the configuration parameter celeborn.metrics.conf.[component_name].source.jvm.class=[source_name] . At present the no source is the available optional source. For example the following configuration parameter activates the Example source: \"celeborn.metrics.conf.*.source.jvm.class\"=\"org.apache.celeborn.common.metrics.source.ExampleSource\"","title":"Metrics"},{"location":"monitoring/#available-metrics-providers","text":"Metrics used by Celeborn are of multiple types: gauge, counter, histogram, meter and timer, see Dropwizard library documentation for details . The following list of components and metrics reports the name and some details about the available metrics, grouped per component instance and source namespace. The most common time of metrics used in Celeborn instrumentation are gauges and counters. Counters can be recognized as they have the .count suffix. Timers, meters and histograms are annotated in the list, the rest of the list elements are metrics of type gauge. The large majority of metrics are active as soon as their parent component instance is configured, some metrics require also to be enabled via an additional configuration parameter, the details are reported in the list.","title":"Available metrics providers"},{"location":"monitoring/#master","text":"These metrics are exposed by Celeborn master. namespace=master RegisteredShuffleCount DeviceCelebornFreeBytes DeviceCelebornTotalBytes RunningApplicationCount ActiveShuffleSize The active shuffle size of workers. ActiveShuffleFileCount The active shuffle file count of workers. WorkerCount LostWorkerCount ExcludedWorkerCount ShutdownWorkerCount IsActiveMaster PartitionSize The size of estimated shuffle partition. OfferSlotsTime The time for masters to handle RequestSlots request when registering shuffle. namespace=CPU JVMCPUTime namespace=system LastMinuteSystemLoad The average system load for the last minute. AvailableProcessors namespace=JVM This source provides information on JVM metrics using the Dropwizard/Codahale Metric Sets for JVM instrumentation and in particular the metric sets BufferPoolMetricSet, GarbageCollectorMetricSet and MemoryUsageGaugeSet. namespace=ResourceConsumption notes: This metrics data is generated for each user and they are identified using a metric tag. This metrics also include subResourceConsumptions generated for each application of user and they are identified using applicationId tag. diskFileCount diskBytesWritten hdfsFileCount hdfsBytesWritten namespace=ThreadPool notes: This metrics data is generated for each thread pool and they are identified using a metric tag by thread pool name. active_thread_count pending_task_count pool_size core_pool_size maximum_pool_size largest_pool_size is_terminating is_terminated is_shutdown thread_count thread_is_terminated_count thread_is_shutdown_count","title":"Master"},{"location":"monitoring/#worker","text":"These metrics are exposed by Celeborn worker. namespace=worker RegisteredShuffleCount RunningApplicationCount ActiveShuffleSize The active shuffle size of a worker including master replica and slave replica. ActiveShuffleFileCount The active shuffle file count of a worker including master replica and slave replica. OpenStreamTime The time for a worker to process openStream RPC and return StreamHandle. FetchChunkTime The time for a worker to fetch a chunk which is 8MB by default from a reduced partition. ActiveChunkStreamCount Active stream count for reduce partition reading streams. OpenStreamSuccessCount OpenStreamFailCount FetchChunkSuccessCount FetchChunkFailCount PrimaryPushDataTime The time for a worker to handle a pushData RPC sent from a celeborn client. ReplicaPushDataTime The time for a worker to handle a pushData RPC sent from a celeborn worker by replicating. WriteDataHardSplitCount WriteDataSuccessCount WriteDataFailCount ReplicateDataFailCount ReplicateDataWriteFailCount ReplicateDataCreateConnectionFailCount ReplicateDataConnectionExceptionCount ReplicateDataFailNonCriticalCauseCount ReplicateDataTimeoutCount PushDataHandshakeFailCount RegionStartFailCount RegionFinishFailCount PrimaryPushDataHandshakeTime ReplicaPushDataHandshakeTime PrimaryRegionStartTime ReplicaRegionStartTime PrimaryRegionFinishTime ReplicaRegionFinishTime PausePushDataTime The time for a worker to stop receiving pushData from clients because of back pressure. PausePushDataAndReplicateTime The time for a worker to stop receiving pushData from clients and other workers because of back pressure. PausePushData The count for a worker to stop receiving pushData from clients because of back pressure. PausePushDataAndReplicate The count for a worker to stop receiving pushData from clients and other workers because of back pressure. TakeBufferTime The time for a worker to take out a buffer from a disk flusher. FlushDataTime The time for a worker to write a buffer which is 256KB by default to storage. CommitFilesTime The time for a worker to flush buffers and close files related to specified shuffle. SlotsAllocated ActiveSlotsCount The number of slots currently being used in a worker ReserveSlotsTime ActiveConnectionCount NettyMemory The total amount of off-heap memory used by celeborn worker. SortTime The time for a worker to sort a shuffle file. SortMemory The memory used by sorting shuffle files. SortingFiles SortedFiles SortedFileSize DiskBuffer The memory occupied by pushData and pushMergedData which should be written to disk. BufferStreamReadBuffer The memory used by credit stream read buffer. ReadBufferDispatcherRequestsLength The queue size of read buffer allocation requests. ReadBufferAllocatedCount Allocated read buffer count. ActiveCreditStreamCount Active stream count for map partition reading streams. ActiveMapPartitionCount CleanTaskQueueSize CleanExpiredShuffleKeysTime The time for a worker to clean up shuffle data of expired shuffle keys. DeviceOSFreeBytes DeviceOSTotalBytes DeviceCelebornFreeBytes DeviceCelebornTotalBytes PotentialConsumeSpeed UserProduceSpeed WorkerConsumeSpeed push_server_usedHeapMemory push_server_usedDirectMemory push_server_numAllocations push_server_numTinyAllocations push_server_numSmallAllocations push_server_numNormalAllocations push_server_numHugeAllocations push_server_numDeallocations push_server_numTinyDeallocations push_server_numSmallDeallocations push_server_numNormalDeallocations push_server_numHugeDeallocations push_server_numActiveAllocations push_server_numActiveTinyAllocations push_server_numActiveSmallAllocations push_server_numActiveNormalAllocations push_server_numActiveHugeAllocations push_server_numActiveBytes replicate_server_usedHeapMemory replicate_server_usedDirectMemory replicate_server_numAllocations replicate_server_numTinyAllocations replicate_server_numSmallAllocations replicate_server_numNormalAllocations replicate_server_numHugeAllocations replicate_server_numDeallocations replicate_server_numTinyDeallocations replicate_server_numSmallDeallocations replicate_server_numNormalDeallocations replicate_server_numHugeDeallocations replicate_server_numActiveAllocations replicate_server_numActiveTinyAllocations replicate_server_numActiveSmallAllocations replicate_server_numActiveNormalAllocations replicate_server_numActiveHugeAllocations replicate_server_numActiveBytes fetch_server_usedHeapMemory fetch_server_usedDirectMemory fetch_server_numAllocations fetch_server_numTinyAllocations fetch_server_numSmallAllocations fetch_server_numNormalAllocations fetch_server_numHugeAllocations fetch_server_numDeallocations fetch_server_numTinyDeallocations fetch_server_numSmallDeallocations fetch_server_numNormalDeallocations fetch_server_numHugeDeallocations fetch_server_numActiveAllocations fetch_server_numActiveTinyAllocations fetch_server_numActiveSmallAllocations fetch_server_numActiveNormalAllocations fetch_server_numActiveHugeAllocations fetch_server_numActiveBytes namespace=CPU JVMCPUTime namespace=system LastMinuteSystemLoad Returns the system load average for the last minute. AvailableProcessors namespace=JVM This source provides information on JVM metrics using the Dropwizard/Codahale Metric Sets for JVM instrumentation and in particular the metric sets BufferPoolMetricSet, GarbageCollectorMetricSet and MemoryUsageGaugeSet. namespace=ResourceConsumption notes: This metrics data is generated for each user and they are identified using a metric tag. This metrics also include subResourceConsumptions generated for each application of user and they are identified using applicationId tag. diskFileCount diskBytesWritten hdfsFileCount hdfsBytesWritten namespace=ThreadPool notes: This metrics data is generated for each thread pool and they are identified using a metric tag by thread pool name. active_thread_count pending_task_count pool_size core_pool_size maximum_pool_size largest_pool_size is_terminating is_terminated is_shutdown Note: The Netty DirectArenaMetrics named like push/fetch/replicate_server_numXX are not exposed by default, nor in Grafana dashboard. If there is a need, you can enable celeborn.network.memory.allocator.verbose.metric to expose these metrics.","title":"Worker"},{"location":"monitoring/#rest-api","text":"In addition to viewing the metrics, Celeborn also support REST API. This gives developers an easy way to create new visualizations and monitoring tools for Celeborn and also easy for users to get the running status of the service. The REST API is available for both master and worker. The endpoints are mounted at host:port . For example, for the master, they would typically be accessible at http://<master-http-host>:<master-http-port><path> , and for the worker, at http://<worker-http-host>:<worker-http-port><path> . The configuration of <master-http-host> , <master-http-port> , <worker-http-host> , <worker-http--port> as below: Key Default Description Since celeborn.master.http.host 0.0.0.0 Master's http host. 0.4.0 celeborn.master.http.port 9098 Master's http port. 0.4.0 celeborn.worker.http.host 0.0.0.0 Worker's http host. 0.4.0 celeborn.worker.http.port 9096 Worker's http port. 0.4.0","title":"REST API"},{"location":"monitoring/#available-api-providers","text":"API path listed as below:","title":"Available API providers"},{"location":"monitoring/#master_1","text":"Path Method Parameters Meaning /applications GET List all running application's ids of the cluster. /conf GET List the conf setting of the master. /excludedWorkers GET List all excluded workers of the master. /help GET List the available API providers of the master. /hostnames GET List all running application's LifecycleManager's hostnames of the cluster. /listDynamicConfigs GET level=${LEVEL} tenant=${TENANT} name=${NAME} List the dynamic configs of the master. The parameter level specifies the config level of dynamic configs. The parameter tenant specifies the tenant id of TENANT or TENANT_USER level. The parameter name specifies the user name of TENANT_USER level. Meanwhile, either none or all of the parameter tenant and name are specified for TENANT_USER level. /listTopDiskUsedApps GET List the top disk usage application ids. It will return the top disk usage application ids for the cluster. /lostWorkers GET List all lost workers of the master. /masterGroupInfo GET List master group information of the service. It will list all master's LEADER, FOLLOWER information. /metrics/prometheus GET List the metrics data in prometheus format of the master. The url path is defined by configure celeborn.metrics.prometheus.path . /shuffles GET List all running shuffle keys of the service. It will return all running shuffle's key of the cluster. /shutdownWorkers GET List all shutdown workers of the master. /threadDump GET List the current thread dump of the master. /workerEventInfo GET List all worker event information of the master. /workerInfo GET List worker information of the service. It will list all registered workers' information. /exclude POST add=${ADD_WORKERS} remove=${REMOVE_WORKERS} Excluded workers of the master add or remove the worker manually given worker id. The parameter add or remove specifies the excluded workers to add or remove, which value is separated by commas. /sendWorkerEvent POST type=${EVENT_TYPE} workers=${WORKERS} For Master(Leader) can send worker event to manager workers. Legal type s are 'None', 'Immediately', 'Decommission', 'DecommissionThenIdle', 'Graceful', 'Recommission', and the parameter workers is separated by commas.","title":"Master"},{"location":"monitoring/#worker_1","text":"Path Method Parameters Meaning /applications GET List all running application's ids of the worker. It only return application ids running in that worker. /conf GET List the conf setting of the worker. /help GET List the available API providers of the worker. /isRegistered GET Show if the worker is registered to the master success. /isShutdown GET Show if the worker is during the process of shutdown. /listDynamicConfigs GET level=${LEVEL} tenant=${TENANT} name=${NAME} List the dynamic configs of the worker. The parameter level specifies the config level of dynamic configs. The parameter tenant specifies the tenant id of TENANT or TENANT_USER level. The parameter name specifies the user name of TENANT_USER level. Meanwhile, either none or all of the parameter tenant and name are specified for TENANT_USER level. /listPartitionLocationInfo GET List all the living PartitionLocation information in that worker. /listTopDiskUsedApps GET List the top disk usage application ids. It only return application ids running in that worker. /metrics/prometheus GET List the metrics data in prometheus format of the worker. The url path is defined by configure celeborn.metrics.prometheus.path . /shuffles GET List all the running shuffle keys of the worker. It only return keys of shuffles running in that worker. /threadDump GET List the current thread dump of the worker. /unavailablePeers GET List the unavailable peers of the worker, this always means the worker connect to the peer failed. /workerInfo GET List the worker information of the worker. /exit POST type=${EXIT_TYPE} Trigger this worker to exit. Legal type s are 'Decommission', 'Graceful' and 'Immediately'.","title":"Worker"},{"location":"quota_management/","text":"Quota Management Celeborn offers administrators flexibility by allowing them to set quotas for individual users and providing a system-level default quota for users without a specific named quota. This feature ensures control and customization in managing system quotas. When celeborn.quota.enabled is set to true, the Master enforces quota limits using the QuotaManager . Similarly, if this setting is enabled on the client side, the LifecycleManager will ask the Master to verify whether the current user has sufficient quota before shuffle registration. Should there be insufficient quota, the LifecycleManager will revert to using Spark's default shuffle service. Quota Indicators Celeborn supports fine-grained quota management, including four indicators: celeborn.quota.tenant.diskBytesWritten : Maximum allowed size of disk write files, of which default value Long.MAX_VALUE . celeborn.quota.tenant.diskFileCount : Maximum allowed number of disk write files, of which default value is Long.MAX_VALUE . celeborn.quota.tenant.hdfsBytesWritten : Maximum allowed size of HDFS write files, of which default value Long.MAX_VALUE . celeborn.quota.tenant.hdfsFileCount : Maximum allowed number of HDFS write files, of which default value is Long.MAX_VALUE . User Identifier The LifecycleManager will request the Master to check the quota for the current user defined by user setting. Users can set celeborn.quota.identity.provider to choose an identity provider. Celeborn support the following types at present: org.apache.celeborn.common.identity.HadoopBasedIdentityProvider : The username will be obtained by UserGroupInformation.getUserName() , tenant id will be default. org.apache.celeborn.common.identity.DefaultIdentityProvider : The username and tenant id are default values or user-specific values set by celeborn.quota.identity.user-specific.tenant and celeborn.quota.identity.user-specific.userName . By default, Celeborn uses org.apache.celeborn.common.identity.DefaultIdentityProvider . Users can also implement their own identity provider by inheriting the org.apache.celeborn.common.identity.IdentityProvider interface. QuotaManager QuotaManager supports to check whether quota is available and manage quota configurations for Master . QuotaManager uses the dynamic config service to store quota settings. For example, there are some quota configurations as follows: The quota for user tenant_01.Jerry is diskBytesWritten: 100G diskFileCount: 10000 hdfsBytesWritten: 10G diskFileCount: Long.MAX_VALUE The quota for tenant id tenant_01 is diskBytesWritten: 10G diskFileCount: 1000 hdfsBytesWritten: 10G diskFileCount: Long.MAX_VALUE The quota for system default is diskBytesWritten: 1G diskFileCount: 100 hdfsBytesWritten: 1G diskFileCount: Long.MAX_VALUE FileSystem Store Backend This backend reads quota settings from a user-specified dynamic config file. For more information on using the database store backend, refer to filesystem config service . Here's an example quota setting YAML file of above quota examples: - level : SYSTEM config : celeborn.quota.tenant.diskBytesWritten : 1G celeborn.quota.tenant.diskFileCount : 100 celeborn.quota.tenant.hdfsBytesWritten : 1G - tenantId : tenant_01 level : TENANT config : celeborn.quota.tenant.diskBytesWritten : 10G celeborn.quota.tenant.diskFileCount : 1000 celeborn.quota.tenant.hdfsBytesWritten : 10G users : - name : Jerry config : celeborn.quota.tenant.diskBytesWritten : 100G celeborn.quota.tenant.diskFileCount : 10000 Database Store Backend This backend reads quota settings from a user-specified database. For more information on using the database store backend, refer to database config service . Here's an example quota setting sql of above quota examples: # SYSTEM level configuration INSERT INTO ` celeborn_cluster_system_config ` ( ` id ` , ` cluster_id ` , ` config_key ` , ` config_value ` , ` type ` , ` gmt_create ` , ` gmt_modify ` ) VALUES ( 1 , 1 , 'celeborn.quota.tenant.diskBytesWritten' , '1G' , 'QUOTA' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ), ( 2 , 1 , 'celeborn.quota.tenant.diskFileCount' , '100' , 'QUOTA' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ), ( 3 , 1 , 'celeborn.quota.tenant.hdfsBytesWritten' , '1G' , 'QUOTA' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ); # TENANT / TENANT_USER level configuration INSERT INTO ` celeborn_cluster_tenant_config ` ( ` id ` , ` cluster_id ` , ` tenant_id ` , ` level ` , ` name ` , ` config_key ` , ` config_value ` , ` type ` , ` gmt_create ` , ` gmt_modify ` ) VALUES ( 1 , 1 , 'tenant_01' , 'TENANT' , '' , 'celeborn.quota.tenant.diskBytesWritten' , '10G' , 'master' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ), ( 2 , 1 , 'tenant_01' , 'TENANT' , '' , 'celeborn.quota.tenant.diskFileCount' , '1000' , 'master' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ), ( 3 , 1 , 'tenant_01' , 'TENANT' , '' , 'celeborn.quota.tenant.hdfsBytesWritten' , '10G' , 'master' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ), ( 4 , 1 , 'tenant_01' , 'TENANT_USER' , 'Jerry' , 'celeborn.quota.tenant.diskBytesWritten' , '100G' , 'master' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ), ( 5 , 1 , 'tenant_01' , 'TENANT_USER' , 'Jerry' , 'celeborn.quota.tenant.diskFileCount' , '10000' , 'master' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' );","title":"Quota Management"},{"location":"quota_management/#quota-management","text":"Celeborn offers administrators flexibility by allowing them to set quotas for individual users and providing a system-level default quota for users without a specific named quota. This feature ensures control and customization in managing system quotas. When celeborn.quota.enabled is set to true, the Master enforces quota limits using the QuotaManager . Similarly, if this setting is enabled on the client side, the LifecycleManager will ask the Master to verify whether the current user has sufficient quota before shuffle registration. Should there be insufficient quota, the LifecycleManager will revert to using Spark's default shuffle service.","title":"Quota Management"},{"location":"quota_management/#quota-indicators","text":"Celeborn supports fine-grained quota management, including four indicators: celeborn.quota.tenant.diskBytesWritten : Maximum allowed size of disk write files, of which default value Long.MAX_VALUE . celeborn.quota.tenant.diskFileCount : Maximum allowed number of disk write files, of which default value is Long.MAX_VALUE . celeborn.quota.tenant.hdfsBytesWritten : Maximum allowed size of HDFS write files, of which default value Long.MAX_VALUE . celeborn.quota.tenant.hdfsFileCount : Maximum allowed number of HDFS write files, of which default value is Long.MAX_VALUE .","title":"Quota Indicators"},{"location":"quota_management/#user-identifier","text":"The LifecycleManager will request the Master to check the quota for the current user defined by user setting. Users can set celeborn.quota.identity.provider to choose an identity provider. Celeborn support the following types at present: org.apache.celeborn.common.identity.HadoopBasedIdentityProvider : The username will be obtained by UserGroupInformation.getUserName() , tenant id will be default. org.apache.celeborn.common.identity.DefaultIdentityProvider : The username and tenant id are default values or user-specific values set by celeborn.quota.identity.user-specific.tenant and celeborn.quota.identity.user-specific.userName . By default, Celeborn uses org.apache.celeborn.common.identity.DefaultIdentityProvider . Users can also implement their own identity provider by inheriting the org.apache.celeborn.common.identity.IdentityProvider interface.","title":"User Identifier"},{"location":"quota_management/#quotamanager","text":"QuotaManager supports to check whether quota is available and manage quota configurations for Master . QuotaManager uses the dynamic config service to store quota settings. For example, there are some quota configurations as follows: The quota for user tenant_01.Jerry is diskBytesWritten: 100G diskFileCount: 10000 hdfsBytesWritten: 10G diskFileCount: Long.MAX_VALUE The quota for tenant id tenant_01 is diskBytesWritten: 10G diskFileCount: 1000 hdfsBytesWritten: 10G diskFileCount: Long.MAX_VALUE The quota for system default is diskBytesWritten: 1G diskFileCount: 100 hdfsBytesWritten: 1G diskFileCount: Long.MAX_VALUE","title":"QuotaManager"},{"location":"quota_management/#filesystem-store-backend","text":"This backend reads quota settings from a user-specified dynamic config file. For more information on using the database store backend, refer to filesystem config service . Here's an example quota setting YAML file of above quota examples: - level : SYSTEM config : celeborn.quota.tenant.diskBytesWritten : 1G celeborn.quota.tenant.diskFileCount : 100 celeborn.quota.tenant.hdfsBytesWritten : 1G - tenantId : tenant_01 level : TENANT config : celeborn.quota.tenant.diskBytesWritten : 10G celeborn.quota.tenant.diskFileCount : 1000 celeborn.quota.tenant.hdfsBytesWritten : 10G users : - name : Jerry config : celeborn.quota.tenant.diskBytesWritten : 100G celeborn.quota.tenant.diskFileCount : 10000","title":"FileSystem Store Backend"},{"location":"quota_management/#database-store-backend","text":"This backend reads quota settings from a user-specified database. For more information on using the database store backend, refer to database config service . Here's an example quota setting sql of above quota examples: # SYSTEM level configuration INSERT INTO ` celeborn_cluster_system_config ` ( ` id ` , ` cluster_id ` , ` config_key ` , ` config_value ` , ` type ` , ` gmt_create ` , ` gmt_modify ` ) VALUES ( 1 , 1 , 'celeborn.quota.tenant.diskBytesWritten' , '1G' , 'QUOTA' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ), ( 2 , 1 , 'celeborn.quota.tenant.diskFileCount' , '100' , 'QUOTA' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ), ( 3 , 1 , 'celeborn.quota.tenant.hdfsBytesWritten' , '1G' , 'QUOTA' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ); # TENANT / TENANT_USER level configuration INSERT INTO ` celeborn_cluster_tenant_config ` ( ` id ` , ` cluster_id ` , ` tenant_id ` , ` level ` , ` name ` , ` config_key ` , ` config_value ` , ` type ` , ` gmt_create ` , ` gmt_modify ` ) VALUES ( 1 , 1 , 'tenant_01' , 'TENANT' , '' , 'celeborn.quota.tenant.diskBytesWritten' , '10G' , 'master' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ), ( 2 , 1 , 'tenant_01' , 'TENANT' , '' , 'celeborn.quota.tenant.diskFileCount' , '1000' , 'master' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ), ( 3 , 1 , 'tenant_01' , 'TENANT' , '' , 'celeborn.quota.tenant.hdfsBytesWritten' , '10G' , 'master' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ), ( 4 , 1 , 'tenant_01' , 'TENANT_USER' , 'Jerry' , 'celeborn.quota.tenant.diskBytesWritten' , '100G' , 'master' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ), ( 5 , 1 , 'tenant_01' , 'TENANT_USER' , 'Jerry' , 'celeborn.quota.tenant.diskFileCount' , '10000' , 'master' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' );","title":"Database Store Backend"},{"location":"security/","text":"Introduction Celeborn supports both SASL (Simple Authentication and Security Layer) based authentication and TLS (Transport Layer Security) based over the wire encryption. Both are disabled by default, and require to be explicitly enabled. Celeborn can use TLS to encrypt data transmitted over the network, provide privacy and data integrity. It also facilitates validating the identity of the server to mitigate the risk of man-in-the-middle attacks and ensure trusted communication. SASL is leveraged by Celeborn to authenticate requests from an application - it ensures clients can only mutate or access state and data that belongs to them. Note: SSL and TLS are used interchangeably in this document. Network encryption with TLS When enabled, Celeborn leverages TLS to provide over the wire encryption. Celeborn has different transport namespaces, and each can be independently configured for TLS. The full list of all configurations which apply for ssl are listed in network configurations - and namespaced under celeborn.ssl . The various transport modules which can be configured are: Module Parent Module Description rpc_app rpc Configure control plane RPC environment used by Celeborn within the application. For backward compatibility, supports fallback to rpc parent module for missing configuration. Note, this is for RPC environment - see below for other transport modules rpc_service rpc Configure control plane RPC environment when communicating with Celeborn service hosts. This includes all RPC communication from application to Celeborn Master/Workers, as well as between Celeborn masters/workers themselves. For backward compatibility, supports fallback to rpc parent module for missing configuration. As with rpc_app , this is only for RPC environment see below for other transport modules. rpc - Fallback parent transport module for rpc_app and rpc_service . It is advisible to use the specific transport modules while configuring - rpc exists primarily for backward compatibility push - Configure transport module for handling data push at Celeborn workers fetch - Configure transport module for handling data fetch at Celeborn workers data - Configure transport module for handling data push and fetch at Celeborn apps replicate - Configure transport module for handling data replication between Celeborn workers When SSL is enabled for rpc_service , Raft communication between masters are secured only when celeborn.master.ha.ratis.raft.rpc.type is set to grpc . Note that celeborn.ssl , without any module , can be used to set SSL default values which applies to all modules. Also note that data module at application side, maps to push and fetch at worker - hence, for SSL configuration, worker configuration for push and fetch should be compatible with each other and with data at application side. Example configuration Master/Worker configuration for TLS # TLS configuration celeborn.ssl.rpc_service.enabled true # Location of the java keystore which contains the private key and certificate for the master/worker. celeborn.ssl.rpc_service.keyStore /mnt/disk1/celeborn/conf/ssl/server.jks celeborn.ssl.rpc_service.keyStorePassword password # Location of the java truststore which contains the CA certs which can validate the master/worker certificate celeborn.ssl.rpc_service.trustStore /mnt/disk1/celeborn/conf/ssl/truststore.jks celeborn.ssl.rpc_service.trustStorePassword changeit Application configuration for TLS # TLS # Configure rpc_app to enable ssl between lifecyclemanager and executors spark.celeborn.ssl.rpc_app.enabled true # Use auto ssl to generate a self-signed certificate at lifecyclemanager, to secure network communication. spark.celeborn.ssl.rpc_app.autoSslEnabled true # Secure communication with celeborn servers spark.celeborn.ssl.rpc_service.enabled true # trust store with CA certs, to verify the celeborn service certificate spark.celeborn.ssl.rpc_service.trustStore /etc/ssl/certs/truststore.jks spark.celeborn.ssl.rpc_service.trustStorePassword changeit Authentication Celeborn supports authentication to prevent unauthorized access or modifications to an application's data. When enabled, lifecyclemanager registers the application with Celeborn service, and negotiates a shared secret. All further connections, from the application to Celeborn servers, will first be authenticated - and only an authorized connection can read/modify data for a registered application. The shared secret , which is generated as part of application registration, is used to authenticate all subsequent connections. Even though Celeborn does not transmit the secret, in the clear, as part of authentication - it still sends it in the clear during registration - and so enabling TLS for rpc_service and rpc_app transport modules is recommended (see above on how). Note: SASL requires use of internal port . Property Name Default Description celeborn.auth.enabled false Enables Authentication celeborn.internal.port.enabled false Enable internal port for Celeborn services. This must be enabled when authentication is enabled. Only server components communicate with each other on the internal port, while applications continue to use the regular ports. celeborn.master.internal.endpoints None Analogous to celeborn.master.endpoints , but with internal ports instead. celeborn.master.ha.node.<id>.internal.port None Analogous to celeborn.master.ha.node.<id>.port , but for internal ports instead Example configuration Master/Worker configuration for authentication # Enable authentication celeborn.auth.enabled true # Rest of the changes are to enable internal port # Enable internal port celeborn.internal.port.enabled true celeborn.master.endpoints clb-1 : 9097,clb-2:9097,clb-3:9097 # Configure internal master endpoint, in addition to master endpoint celeborn.master.internal.endpoints clb-1 : 19097,clb-2:19097,clb-3:19097 celeborn.master.host clb-master celeborn.master.port 9097 # internal port, matches internal endpoint above celeborn.master.internal.port 19097 celeborn.master.ha.enabled true celeborn.master.ha.node.id 1 celeborn.master.ha.node.1.host clb-1 celeborn.master.ha.node.1.port 9097 # Ensure that internal.port is configured for HA mode as well celeborn.master.ha.node.1.internal.port 19097 celeborn.master.ha.node.1.ratis.port 9872 celeborn.master.ha.node.2.host clb-2 celeborn.master.ha.node.2.port 9097 celeborn.master.ha.node.2.internal.port 19097 celeborn.master.ha.node.2.ratis.port 9872 celeborn.master.ha.node.3.host clb-3 celeborn.master.ha.node.3.port 9097 celeborn.master.ha.node.3.internal.port 19097 celeborn.master.ha.node.3.ratis.port 9872 Application configuration for Authentication spark.celeborn.auth.enabled true","title":"Security"},{"location":"security/#introduction","text":"Celeborn supports both SASL (Simple Authentication and Security Layer) based authentication and TLS (Transport Layer Security) based over the wire encryption. Both are disabled by default, and require to be explicitly enabled. Celeborn can use TLS to encrypt data transmitted over the network, provide privacy and data integrity. It also facilitates validating the identity of the server to mitigate the risk of man-in-the-middle attacks and ensure trusted communication. SASL is leveraged by Celeborn to authenticate requests from an application - it ensures clients can only mutate or access state and data that belongs to them. Note: SSL and TLS are used interchangeably in this document.","title":"Introduction"},{"location":"security/#network-encryption-with-tls","text":"When enabled, Celeborn leverages TLS to provide over the wire encryption. Celeborn has different transport namespaces, and each can be independently configured for TLS. The full list of all configurations which apply for ssl are listed in network configurations - and namespaced under celeborn.ssl . The various transport modules which can be configured are: Module Parent Module Description rpc_app rpc Configure control plane RPC environment used by Celeborn within the application. For backward compatibility, supports fallback to rpc parent module for missing configuration. Note, this is for RPC environment - see below for other transport modules rpc_service rpc Configure control plane RPC environment when communicating with Celeborn service hosts. This includes all RPC communication from application to Celeborn Master/Workers, as well as between Celeborn masters/workers themselves. For backward compatibility, supports fallback to rpc parent module for missing configuration. As with rpc_app , this is only for RPC environment see below for other transport modules. rpc - Fallback parent transport module for rpc_app and rpc_service . It is advisible to use the specific transport modules while configuring - rpc exists primarily for backward compatibility push - Configure transport module for handling data push at Celeborn workers fetch - Configure transport module for handling data fetch at Celeborn workers data - Configure transport module for handling data push and fetch at Celeborn apps replicate - Configure transport module for handling data replication between Celeborn workers When SSL is enabled for rpc_service , Raft communication between masters are secured only when celeborn.master.ha.ratis.raft.rpc.type is set to grpc . Note that celeborn.ssl , without any module , can be used to set SSL default values which applies to all modules. Also note that data module at application side, maps to push and fetch at worker - hence, for SSL configuration, worker configuration for push and fetch should be compatible with each other and with data at application side.","title":"Network encryption with TLS"},{"location":"security/#example-configuration","text":"","title":"Example configuration"},{"location":"security/#masterworker-configuration-for-tls","text":"# TLS configuration celeborn.ssl.rpc_service.enabled true # Location of the java keystore which contains the private key and certificate for the master/worker. celeborn.ssl.rpc_service.keyStore /mnt/disk1/celeborn/conf/ssl/server.jks celeborn.ssl.rpc_service.keyStorePassword password # Location of the java truststore which contains the CA certs which can validate the master/worker certificate celeborn.ssl.rpc_service.trustStore /mnt/disk1/celeborn/conf/ssl/truststore.jks celeborn.ssl.rpc_service.trustStorePassword changeit","title":"Master/Worker configuration for TLS"},{"location":"security/#application-configuration-for-tls","text":"# TLS # Configure rpc_app to enable ssl between lifecyclemanager and executors spark.celeborn.ssl.rpc_app.enabled true # Use auto ssl to generate a self-signed certificate at lifecyclemanager, to secure network communication. spark.celeborn.ssl.rpc_app.autoSslEnabled true # Secure communication with celeborn servers spark.celeborn.ssl.rpc_service.enabled true # trust store with CA certs, to verify the celeborn service certificate spark.celeborn.ssl.rpc_service.trustStore /etc/ssl/certs/truststore.jks spark.celeborn.ssl.rpc_service.trustStorePassword changeit","title":"Application configuration for TLS"},{"location":"security/#authentication","text":"Celeborn supports authentication to prevent unauthorized access or modifications to an application's data. When enabled, lifecyclemanager registers the application with Celeborn service, and negotiates a shared secret. All further connections, from the application to Celeborn servers, will first be authenticated - and only an authorized connection can read/modify data for a registered application. The shared secret , which is generated as part of application registration, is used to authenticate all subsequent connections. Even though Celeborn does not transmit the secret, in the clear, as part of authentication - it still sends it in the clear during registration - and so enabling TLS for rpc_service and rpc_app transport modules is recommended (see above on how). Note: SASL requires use of internal port . Property Name Default Description celeborn.auth.enabled false Enables Authentication celeborn.internal.port.enabled false Enable internal port for Celeborn services. This must be enabled when authentication is enabled. Only server components communicate with each other on the internal port, while applications continue to use the regular ports. celeborn.master.internal.endpoints None Analogous to celeborn.master.endpoints , but with internal ports instead. celeborn.master.ha.node.<id>.internal.port None Analogous to celeborn.master.ha.node.<id>.port , but for internal ports instead","title":"Authentication"},{"location":"security/#example-configuration_1","text":"","title":"Example configuration"},{"location":"security/#masterworker-configuration-for-authentication","text":"# Enable authentication celeborn.auth.enabled true # Rest of the changes are to enable internal port # Enable internal port celeborn.internal.port.enabled true celeborn.master.endpoints clb-1 : 9097,clb-2:9097,clb-3:9097 # Configure internal master endpoint, in addition to master endpoint celeborn.master.internal.endpoints clb-1 : 19097,clb-2:19097,clb-3:19097 celeborn.master.host clb-master celeborn.master.port 9097 # internal port, matches internal endpoint above celeborn.master.internal.port 19097 celeborn.master.ha.enabled true celeborn.master.ha.node.id 1 celeborn.master.ha.node.1.host clb-1 celeborn.master.ha.node.1.port 9097 # Ensure that internal.port is configured for HA mode as well celeborn.master.ha.node.1.internal.port 19097 celeborn.master.ha.node.1.ratis.port 9872 celeborn.master.ha.node.2.host clb-2 celeborn.master.ha.node.2.port 9097 celeborn.master.ha.node.2.internal.port 19097 celeborn.master.ha.node.2.ratis.port 9872 celeborn.master.ha.node.3.host clb-3 celeborn.master.ha.node.3.port 9097 celeborn.master.ha.node.3.internal.port 19097 celeborn.master.ha.node.3.ratis.port 9872","title":"Master/Worker configuration for authentication"},{"location":"security/#application-configuration-for-authentication","text":"spark.celeborn.auth.enabled true","title":"Application configuration for Authentication"},{"location":"upgrading/","text":"Upgrading Rolling upgrade It is necessary to support a fast rolling upgrade process for the Celeborn cluster. In order to achieve a fast and unaffected rolling upgrade process, Celeborn should support that the written file in the worker should be committed and support reading after the worker restarted. Celeborn have done the following mechanism to support rolling upgrade. Background Fixed fetch port and client retry In the shuffle reduce side, the read client will obtain the worker's host/port and information of the file to be read. In order to ensure that the data can be read normally after the rolling restart process of the worker is completed, the worker needs to use a fixed fetch service port, the configuration is celeborn.worker.fetch.port , the default value is 0 . At startup, it will automatically select a free port, user need to set a fixed value, such as 9092 . At the same time, users need to adjust the number of retry times and retry wait time of the client according to cluster rolling restart situation to support the shuffle client to read data through retries after worker restarted. The shuffle client fetch data retry times configuration is celeborn.client.fetch.maxRetriesForEachReplica , default value is 3 . The shuffle client fetch data retry wait time configuration is celeborn.data.io.retryWait , default value is 5s . Users can increase the configuration value appropriately according to the situation. Worker store file meta information Shuffle client records the shuffle partition location's host, service port, and filename, to support workers recovering reading existing shuffle data after worker restart, during worker shutdown, workers should store the meta about reading shuffle partition files in RocksDB or LevelDB(deprecated), and restore the meta after restarting workers. Users should set celeborn.worker.graceful.shutdown.enabled to true to enable graceful shutdown. During this process, worker will wait all allocated partition's in this worker to be committed within a timeout of celeborn.worker.graceful.shutdown.checkSlotsFinished.timeout , which default value is 480s . Then worker will wait for partition sorter finish all sort task within a timeout of celeborn.worker.graceful.shutdown.partitionSorter.shutdownTimeout , which default value is 120s . The whole graceful shutdown process should be finished within a timeout of celeborn.worker.graceful.shutdown.timeout , which default value is 600s . Allocated partition do hard split and Pre-commit hard split partition As mentioned in the previous section that the worker needs to wait for all allocated partition files to be committed during the restart process, which means that the worker need to wait for all the shuffle running on this worker to finish running before restarting the worker, otherwise part of the information will be lost, and abnormal partition files are left, and reading cannot be resumed. In order to speed up the restart process, worker let all push data requests return the HARD_SPLIT flag during worker shutdown, and shuffle client will re-apply for a new partition location for these allocated partitions. Then client side can record all HARD_SPLIT partition information and pre-commit these partition, then the worker side allocated partitions can be committed in a very short time. User should enable celeborn.client.shuffle.batchHandleCommitPartition.enabled , the default value is true. Example setting Worker Key Value celeborn.worker.graceful.shutdown.enabled true celeborn.worker.graceful.shutdown.checkSlotsFinished.timeout 480s celeborn.worker.graceful.shutdown.partitionSorter.shutdownTimeout 120s celeborn.worker.graceful.shutdown.timeout 600s celeborn.worker.fetch.port 9092 Client Key Value spark.celeborn.client.shuffle.batchHandleCommitPartition.enabled true spark.celeborn.client.fetch.maxRetriesForEachReplica 5 spark.celeborn.data.io.retryWait 10s","title":"Upgrading"},{"location":"upgrading/#upgrading","text":"","title":"Upgrading"},{"location":"upgrading/#rolling-upgrade","text":"It is necessary to support a fast rolling upgrade process for the Celeborn cluster. In order to achieve a fast and unaffected rolling upgrade process, Celeborn should support that the written file in the worker should be committed and support reading after the worker restarted. Celeborn have done the following mechanism to support rolling upgrade.","title":"Rolling upgrade"},{"location":"upgrading/#background","text":"Fixed fetch port and client retry In the shuffle reduce side, the read client will obtain the worker's host/port and information of the file to be read. In order to ensure that the data can be read normally after the rolling restart process of the worker is completed, the worker needs to use a fixed fetch service port, the configuration is celeborn.worker.fetch.port , the default value is 0 . At startup, it will automatically select a free port, user need to set a fixed value, such as 9092 . At the same time, users need to adjust the number of retry times and retry wait time of the client according to cluster rolling restart situation to support the shuffle client to read data through retries after worker restarted. The shuffle client fetch data retry times configuration is celeborn.client.fetch.maxRetriesForEachReplica , default value is 3 . The shuffle client fetch data retry wait time configuration is celeborn.data.io.retryWait , default value is 5s . Users can increase the configuration value appropriately according to the situation. Worker store file meta information Shuffle client records the shuffle partition location's host, service port, and filename, to support workers recovering reading existing shuffle data after worker restart, during worker shutdown, workers should store the meta about reading shuffle partition files in RocksDB or LevelDB(deprecated), and restore the meta after restarting workers. Users should set celeborn.worker.graceful.shutdown.enabled to true to enable graceful shutdown. During this process, worker will wait all allocated partition's in this worker to be committed within a timeout of celeborn.worker.graceful.shutdown.checkSlotsFinished.timeout , which default value is 480s . Then worker will wait for partition sorter finish all sort task within a timeout of celeborn.worker.graceful.shutdown.partitionSorter.shutdownTimeout , which default value is 120s . The whole graceful shutdown process should be finished within a timeout of celeborn.worker.graceful.shutdown.timeout , which default value is 600s . Allocated partition do hard split and Pre-commit hard split partition As mentioned in the previous section that the worker needs to wait for all allocated partition files to be committed during the restart process, which means that the worker need to wait for all the shuffle running on this worker to finish running before restarting the worker, otherwise part of the information will be lost, and abnormal partition files are left, and reading cannot be resumed. In order to speed up the restart process, worker let all push data requests return the HARD_SPLIT flag during worker shutdown, and shuffle client will re-apply for a new partition location for these allocated partitions. Then client side can record all HARD_SPLIT partition information and pre-commit these partition, then the worker side allocated partitions can be committed in a very short time. User should enable celeborn.client.shuffle.batchHandleCommitPartition.enabled , the default value is true.","title":"Background"},{"location":"upgrading/#example-setting","text":"","title":"Example setting"},{"location":"upgrading/#worker","text":"Key Value celeborn.worker.graceful.shutdown.enabled true celeborn.worker.graceful.shutdown.checkSlotsFinished.timeout 480s celeborn.worker.graceful.shutdown.partitionSorter.shutdownTimeout 120s celeborn.worker.graceful.shutdown.timeout 600s celeborn.worker.fetch.port 9092","title":"Worker"},{"location":"upgrading/#client","text":"Key Value spark.celeborn.client.shuffle.batchHandleCommitPartition.enabled true spark.celeborn.client.fetch.maxRetriesForEachReplica 5 spark.celeborn.data.io.retryWait 10s","title":"Client"},{"location":"configuration/","text":"Configuration Guide This documentation contains Celeborn configuration details and a tuning guide. Important Configurations Environment Variables CELEBORN_WORKER_MEMORY=4g CELEBORN_WORKER_OFFHEAP_MEMORY=24g Celeborn workers tend to improve performance by using off-heap buffers. Off-heap memory requirement can be estimated as below: numDirs = `celeborn.worker.storage.dirs` # the amount of directory will be used by Celeborn storage bufferSize = `celeborn.worker.flusher.buffer.size` # the amount of memory will be used by a single flush buffer off-heap-memory = (disk buffer * disks) + network memory # the disk buffer is a logical memory region that stores shuffle data received from network # shuffle data will be flushed to disks through write tasks # the amount of disk buffer can be set to 1GB or larger for each disk according to the difference of your disk speed and network speed For example, if a Celeborn worker give each disk 1GiB memory and the buffer size is set to 256 KB. Celeborn worker can support up to 4096 concurrent write tasks for each disk. If this worker has 10 disks, the offheap memory should be set to 12GB. Network memory will be consumed when netty reads from a TCP channel, there will need some extra memory. Empirically, Celeborn worker off-heap memory should be set to ((disk buffer * disks) * 1.2) . All Configurations Master Key Default isDynamic Description Since Deprecated celeborn.cluster.name default false Celeborn cluster name. 0.5.0 celeborn.dynamicConfig.refresh.interval 120s false Interval for refreshing the corresponding dynamic config periodically. 0.4.0 celeborn.dynamicConfig.store.backend <undefined> false Store backend for dynamic config service. Available options: FS, DB. If not provided, it means that dynamic configuration is disabled. 0.4.0 celeborn.dynamicConfig.store.db.fetch.pageSize 1000 false The page size for db store to query configurations. 0.5.0 celeborn.dynamicConfig.store.db.hikari.connectionTimeout 30s false The connection timeout that a client will wait for a connection from the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.driverClassName false The jdbc driver class name of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.idleTimeout 600s false The idle timeout that a connection is allowed to sit idle in the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.jdbcUrl false The jdbc url of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.maxLifetime 1800s false The maximum lifetime of a connection in the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.maximumPoolSize 2 false The maximum pool size of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.password false The password of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.username false The username of db store backend. 0.5.0 celeborn.dynamicConfig.store.fs.path <undefined> false The path of dynamic config file for fs store backend. The file format should be yaml. The default path is ${CELEBORN_CONF_DIR}/dynamicConfig.yaml . 0.5.0 celeborn.internal.port.enabled false false Whether to create a internal port on Masters/Workers for inter-Masters/Workers communication. This is beneficial when SASL authentication is enforced for all interactions between clients and Celeborn Services, but the services can exchange messages without being subject to SASL authentication. 0.5.0 celeborn.logConf.enabled false false When true , log the CelebornConf for debugging purposes. 0.5.0 celeborn.master.estimatedPartitionSize.initialSize 64mb false Initial partition size for estimation, it will change according to runtime stats. 0.3.0 celeborn.shuffle.initialEstimatedPartitionSize celeborn.master.estimatedPartitionSize.maxSize <undefined> false Max partition size for estimation. Default value should be celeborn.worker.shuffle.partitionSplit.max * 2. 0.4.1 celeborn.master.estimatedPartitionSize.minSize 8mb false Ignore partition size smaller than this configuration of partition size for estimation. 0.3.0 celeborn.shuffle.minPartitionSizeToEstimate celeborn.master.estimatedPartitionSize.update.initialDelay 5min false Initial delay time before start updating partition size for estimation. 0.3.0 celeborn.shuffle.estimatedPartitionSize.update.initialDelay celeborn.master.estimatedPartitionSize.update.interval 10min false Interval of updating partition size for estimation. 0.3.0 celeborn.shuffle.estimatedPartitionSize.update.interval celeborn.master.hdfs.expireDirs.timeout 1h false The timeout for a expire dirs to be deleted on HDFS. 0.3.0 celeborn.master.heartbeat.application.timeout 300s false Application heartbeat timeout. 0.3.0 celeborn.application.heartbeat.timeout celeborn.master.heartbeat.worker.timeout 120s false Worker heartbeat timeout. 0.3.0 celeborn.worker.heartbeat.timeout celeborn.master.host <localhost> false Hostname for master to bind. 0.2.0 celeborn.master.http.host <localhost> false Master's http host. 0.4.0 celeborn.metrics.master.prometheus.host,celeborn.master.metrics.prometheus.host celeborn.master.http.idleTimeout 30s false Master http server idle timeout. 0.5.0 celeborn.master.http.maxWorkerThreads 200 false Maximum number of threads in the master http worker thread pool. 0.5.0 celeborn.master.http.port 9098 false Master's http port. 0.4.0 celeborn.metrics.master.prometheus.port,celeborn.master.metrics.prometheus.port celeborn.master.http.stopTimeout 5s false Master http server stop timeout. 0.5.0 celeborn.master.internal.port 8097 false Internal port on the master where both workers and other master nodes connect. 0.5.0 celeborn.master.port 9097 false Port for master to bind. 0.2.0 celeborn.master.rackResolver.refresh.interval 30s false Interval for refreshing the node rack information periodically. 0.5.0 celeborn.master.send.applicationMeta.threads 8 false Number of threads used by the Master to send ApplicationMeta to Workers. 0.5.0 celeborn.master.slot.assign.extraSlots 2 false Extra slots number when master assign slots. 0.3.0 celeborn.slots.assign.extraSlots celeborn.master.slot.assign.loadAware.diskGroupGradient 0.1 false This value means how many more workload will be placed into a faster disk group than a slower group. 0.3.0 celeborn.slots.assign.loadAware.diskGroupGradient celeborn.master.slot.assign.loadAware.fetchTimeWeight 1.0 false Weight of average fetch time when calculating ordering in load-aware assignment strategy 0.3.0 celeborn.slots.assign.loadAware.fetchTimeWeight celeborn.master.slot.assign.loadAware.flushTimeWeight 0.0 false Weight of average flush time when calculating ordering in load-aware assignment strategy 0.3.0 celeborn.slots.assign.loadAware.flushTimeWeight celeborn.master.slot.assign.loadAware.numDiskGroups 5 false This configuration is a guidance for load-aware slot allocation algorithm. This value is control how many disk groups will be created. 0.3.0 celeborn.slots.assign.loadAware.numDiskGroups celeborn.master.slot.assign.maxWorkers 10000 false Max workers that slots of one shuffle can be allocated on. Will choose the smaller positive one from Master side and Client side, see celeborn.client.slot.assign.maxWorkers . 0.3.1 celeborn.master.slot.assign.policy ROUNDROBIN false Policy for master to assign slots, Celeborn supports two types of policy: roundrobin and loadaware. Loadaware policy will be ignored when HDFS is enabled in celeborn.storage.activeTypes 0.3.0 celeborn.slots.assign.policy celeborn.master.userResourceConsumption.update.interval 30s false Time length for a window about compute user resource consumption. 0.3.0 celeborn.master.workerUnavailableInfo.expireTimeout 1800s false Worker unavailable info would be cleared when the retention period is expired 0.3.1 celeborn.quota.enabled true false When Master side sets to true, the master will enable to check the quota via QuotaManager. When Client side sets to true, LifecycleManager will request Master side to check whether the current user has enough quota before registration of shuffle. Fallback to the default shuffle service of Spark when Master side checks that there is no enough quota for current user. 0.2.0 celeborn.redaction.regex (?i)secret password token access[.]key false celeborn.storage.availableTypes HDD false Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.activeTypes celeborn.storage.hdfs.dir <undefined> false HDFS base directory for Celeborn to store shuffle data. 0.2.0 celeborn.storage.hdfs.kerberos.keytab <undefined> false Kerberos keytab file path for HDFS storage connection. 0.3.2 celeborn.storage.hdfs.kerberos.principal <undefined> false Kerberos principal for HDFS storage connection. 0.3.2 Apart from these, the following properties are also available for enable master HA: Master HA Key Default isDynamic Description Since Deprecated celeborn.master.ha.enabled false false When true, master nodes run as Raft cluster mode. 0.3.0 celeborn.ha.enabled celeborn.master.ha.node.<id>.host <required> false Host to bind of master node in HA mode. 0.3.0 celeborn.ha.master.node.<id>.host celeborn.master.ha.node.<id>.internal.port 8097 false Internal port for the workers and other masters to bind to a master node in HA mode. 0.5.0 celeborn.master.ha.node.<id>.port 9097 false Port to bind of master node in HA mode. 0.3.0 celeborn.ha.master.node.<id>.port celeborn.master.ha.node.<id>.ratis.port 9872 false Ratis port to bind of master node in HA mode. 0.3.0 celeborn.ha.master.node.<id>.ratis.port celeborn.master.ha.ratis.raft.rpc.type netty false RPC type for Ratis, available options: netty, grpc. 0.3.0 celeborn.ha.master.ratis.raft.rpc.type celeborn.master.ha.ratis.raft.server.storage.dir /tmp/ratis false Root storage directory to hold RaftServer data. 0.3.0 celeborn.ha.master.ratis.raft.server.storage.dir celeborn.master.ha.ratis.raft.server.storage.startup.option RECOVER false Startup option of RaftServer storage. Available options: RECOVER, FORMAT. 0.5.0 Worker Key Default isDynamic Description Since Deprecated celeborn.cluster.name default false Celeborn cluster name. 0.5.0 celeborn.dynamicConfig.refresh.interval 120s false Interval for refreshing the corresponding dynamic config periodically. 0.4.0 celeborn.dynamicConfig.store.backend <undefined> false Store backend for dynamic config service. Available options: FS, DB. If not provided, it means that dynamic configuration is disabled. 0.4.0 celeborn.dynamicConfig.store.db.fetch.pageSize 1000 false The page size for db store to query configurations. 0.5.0 celeborn.dynamicConfig.store.db.hikari.connectionTimeout 30s false The connection timeout that a client will wait for a connection from the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.driverClassName false The jdbc driver class name of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.idleTimeout 600s false The idle timeout that a connection is allowed to sit idle in the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.jdbcUrl false The jdbc url of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.maxLifetime 1800s false The maximum lifetime of a connection in the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.maximumPoolSize 2 false The maximum pool size of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.password false The password of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.username false The username of db store backend. 0.5.0 celeborn.dynamicConfig.store.fs.path <undefined> false The path of dynamic config file for fs store backend. The file format should be yaml. The default path is ${CELEBORN_CONF_DIR}/dynamicConfig.yaml . 0.5.0 celeborn.internal.port.enabled false false Whether to create a internal port on Masters/Workers for inter-Masters/Workers communication. This is beneficial when SASL authentication is enforced for all interactions between clients and Celeborn Services, but the services can exchange messages without being subject to SASL authentication. 0.5.0 celeborn.logConf.enabled false false When true , log the CelebornConf for debugging purposes. 0.5.0 celeborn.master.endpoints <localhost>:9097 false Endpoints of master nodes for celeborn client to connect, allowed pattern is: <host1>:<port1>[,<host2>:<port2>]* , e.g. clb1:9097,clb2:9098,clb3:9099 . If the port is omitted, 9097 will be used. 0.2.0 celeborn.master.estimatedPartitionSize.minSize 8mb false Ignore partition size smaller than this configuration of partition size for estimation. 0.3.0 celeborn.shuffle.minPartitionSizeToEstimate celeborn.master.internal.endpoints <localhost>:8097 false Endpoints of master nodes just for celeborn workers to connect, allowed pattern is: <host1>:<port1>[,<host2>:<port2>]* , e.g. clb1:8097,clb2:8097,clb3:8097 . If the port is omitted, 8097 will be used. 0.5.0 celeborn.redaction.regex (?i)secret password token access[.]key false celeborn.shuffle.chunk.size 8m false Max chunk size of reducer's merged shuffle data. For example, if a reducer's shuffle data is 128M and the data will need 16 fetch chunk requests to fetch. 0.2.0 celeborn.shuffle.sortPartition.block.compactionFactor 0.25 false Combine sorted shuffle blocks such that size of compacted shuffle block does not exceed compactionFactor * celeborn.shuffle.chunk.size 0.4.2 celeborn.storage.availableTypes HDD false Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.activeTypes celeborn.storage.hdfs.dir <undefined> false HDFS base directory for Celeborn to store shuffle data. 0.2.0 celeborn.storage.hdfs.kerberos.keytab <undefined> false Kerberos keytab file path for HDFS storage connection. 0.3.2 celeborn.storage.hdfs.kerberos.principal <undefined> false Kerberos principal for HDFS storage connection. 0.3.2 celeborn.worker.activeConnection.max <undefined> false If the number of active connections on a worker exceeds this configuration value, the worker will be marked as high-load in the heartbeat report, and the master will not include that node in the response of RequestSlots. 0.3.1 celeborn.worker.applicationRegistry.cache.size 10000 false Cache size of the application registry on Workers. 0.5.0 celeborn.worker.bufferStream.threadsPerMountpoint 8 false Threads count for read buffer per mount point. 0.3.0 celeborn.worker.clean.threads 64 false Thread number of worker to clean up expired shuffle keys. 0.3.2 celeborn.worker.closeIdleConnections false false Whether worker will close idle connections. 0.2.0 celeborn.worker.commitFiles.threads 32 false Thread number of worker to commit shuffle data files asynchronously. It's recommended to set at least 128 when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.commit.threads celeborn.worker.commitFiles.timeout 120s false Timeout for a Celeborn worker to commit files of a shuffle. It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.shuffle.commit.timeout celeborn.worker.congestionControl.check.interval 10ms false Interval of worker checks congestion if celeborn.worker.congestionControl.enabled is true. 0.3.2 celeborn.worker.congestionControl.enabled false false Whether to enable congestion control or not. 0.3.0 celeborn.worker.congestionControl.high.watermark <undefined> false If the total bytes in disk buffer exceeds this configure, will start to congestusers whose produce rate is higher than the potential average consume rate. The congestion will stop if the produce rate is lower or equal to the average consume rate, or the total pending bytes lower than celeborn.worker.congestionControl.low.watermark 0.3.0 celeborn.worker.congestionControl.low.watermark <undefined> false Will stop congest users if the total pending bytes of disk buffer is lower than this configuration 0.3.0 celeborn.worker.congestionControl.sample.time.window 10s false The worker holds a time sliding list to calculate users' produce/consume rate 0.3.0 celeborn.worker.congestionControl.user.inactive.interval 10min false How long will consider this user is inactive if it doesn't send data 0.3.0 celeborn.worker.decommission.checkInterval 30s false The wait interval of checking whether all the shuffle expired during worker decommission 0.4.0 celeborn.worker.decommission.forceExitTimeout 6h false The wait time of waiting for all the shuffle expire during worker decommission. 0.4.0 celeborn.worker.directMemoryRatioForMemoryFileStorage 0.0 false Max ratio of direct memory to store shuffle data. This feature is experimental and disabled by default. 0.5.0 celeborn.worker.directMemoryRatioForReadBuffer 0.1 false Max ratio of direct memory for read buffer 0.2.0 celeborn.worker.directMemoryRatioToPauseReceive 0.85 false If direct memory usage reaches this limit, the worker will stop to receive data from Celeborn shuffle clients. 0.2.0 celeborn.worker.directMemoryRatioToPauseReplicate 0.95 false If direct memory usage reaches this limit, the worker will stop to receive replication data from other workers. This value should be higher than celeborn.worker.directMemoryRatioToPauseReceive. 0.2.0 celeborn.worker.directMemoryRatioToResume 0.7 false If direct memory usage is less than this limit, worker will resume. 0.2.0 celeborn.worker.disk.clean.threads 4 false Thread number of worker to clean up directories of expired shuffle keys on disk. 0.3.2 celeborn.worker.fetch.heartbeat.enabled false false enable the heartbeat from worker to client when fetching data 0.3.0 celeborn.worker.fetch.io.threads <undefined> false Netty IO thread number of worker to handle client fetch data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.fetch.port 0 false Server port for Worker to receive fetch data request from ShuffleClient. 0.2.0 celeborn.worker.flusher.buffer.size 256k false Size of buffer used by a single flusher. 0.2.0 celeborn.worker.flusher.diskTime.slidingWindow.size 20 false The size of sliding windows used to calculate statistics about flushed time and count. 0.3.0 celeborn.worker.flusher.avgFlushTime.slidingWindow.size celeborn.worker.flusher.hdd.threads 1 false Flusher's thread count per disk used for write data to HDD disks. 0.2.0 celeborn.worker.flusher.hdfs.buffer.size 4m false Size of buffer used by a HDFS flusher. 0.3.0 celeborn.worker.flusher.hdfs.threads 8 false Flusher's thread count used for write data to HDFS. 0.2.0 celeborn.worker.flusher.shutdownTimeout 3s false Timeout for a flusher to shutdown. 0.2.0 celeborn.worker.flusher.ssd.threads 16 false Flusher's thread count per disk used for write data to SSD disks. 0.2.0 celeborn.worker.flusher.threads 16 false Flusher's thread count per disk for unknown-type disks. 0.2.0 celeborn.worker.graceful.shutdown.checkSlotsFinished.interval 1s false The wait interval of checking whether all released slots to be committed or destroyed during worker graceful shutdown 0.2.0 celeborn.worker.graceful.shutdown.checkSlotsFinished.timeout 480s false The wait time of waiting for the released slots to be committed or destroyed during worker graceful shutdown. 0.2.0 celeborn.worker.graceful.shutdown.enabled false false When true, during worker shutdown, the worker will wait for all released slots to be committed or destroyed. 0.2.0 celeborn.worker.graceful.shutdown.partitionSorter.shutdownTimeout 120s false The wait time of waiting for sorting partition files during worker graceful shutdown. 0.2.0 celeborn.worker.graceful.shutdown.recoverDbBackend ROCKSDB false Specifies a disk-based store used in local db. ROCKSDB or LEVELDB (deprecated). 0.4.0 celeborn.worker.graceful.shutdown.recoverPath <tmp>/recover false The path to store DB. 0.2.0 celeborn.worker.graceful.shutdown.saveCommittedFileInfo.interval 5s false Interval for a Celeborn worker to flush committed file infos into Level DB. 0.3.1 celeborn.worker.graceful.shutdown.saveCommittedFileInfo.sync false false Whether to call sync method to save committed file infos into Level DB to handle OS crash. 0.3.1 celeborn.worker.graceful.shutdown.timeout 600s false The worker's graceful shutdown timeout time. 0.2.0 celeborn.worker.http.host <localhost> false Worker's http host. 0.4.0 celeborn.metrics.worker.prometheus.host,celeborn.worker.metrics.prometheus.host celeborn.worker.http.idleTimeout 30s false Worker http server idle timeout. 0.5.0 celeborn.worker.http.maxWorkerThreads 200 false Maximum number of threads in the worker http worker thread pool. 0.5.0 celeborn.worker.http.port 9096 false Worker's http port. 0.4.0 celeborn.metrics.worker.prometheus.port,celeborn.worker.metrics.prometheus.port celeborn.worker.http.stopTimeout 5s false Worker http server stop timeout. 0.5.0 celeborn.worker.internal.port 0 false Internal server port on the Worker where the master nodes connect. 0.5.0 celeborn.worker.jvmProfiler.enabled false false Turn on code profiling via async_profiler in workers. 0.5.0 celeborn.worker.jvmProfiler.localDir . false Local file system path on worker where profiler output is saved. Defaults to the working directory of the worker process. 0.5.0 celeborn.worker.jvmProfiler.options event=wall,interval=10ms,alloc=2m,lock=10ms,chunktime=300s false Options to pass on to the async profiler. 0.5.0 celeborn.worker.jvmQuake.check.interval 1s false Interval of gc behavior checking for worker jvm quake. 0.4.0 celeborn.worker.jvmQuake.dump.enabled true false Whether to heap dump for the maximum GC 'deficit' during worker jvm quake. 0.4.0 celeborn.worker.jvmQuake.dump.path <tmp>/jvm-quake/dump/<pid> false The path of heap dump for the maximum GC 'deficit' during worker jvm quake. 0.4.0 celeborn.worker.jvmQuake.dump.threshold 30s false The threshold of heap dump for the maximum GC 'deficit' which can be accumulated before jvmquake takes action. Meanwhile, there is no heap dump generated when dump threshold is greater than kill threshold. 0.4.0 celeborn.worker.jvmQuake.enabled false false When true, Celeborn worker will start the jvm quake to monitor of gc behavior, which enables early detection of memory management issues and facilitates fast failure. 0.4.0 celeborn.worker.jvmQuake.exitCode 502 false The exit code of system kill for the maximum GC 'deficit' during worker jvm quake. 0.4.0 celeborn.worker.jvmQuake.kill.threshold 60s false The threshold of system kill for the maximum GC 'deficit' which can be accumulated before jvmquake takes action. 0.4.0 celeborn.worker.jvmQuake.runtimeWeight 5.0 false The factor by which to multiply running JVM time, when weighing it against GCing time. 'Deficit' is accumulated as gc_time - runtime * runtime_weight , and is compared against threshold to determine whether to take action. 0.4.0 celeborn.worker.memoryFileStorage.evict.aggressiveMode.enabled false false If this set to true, memory shuffle files will be evicted when worker is in PAUSED state. If the worker's offheap memory is not ample, set this to true and decrease celeborn.worker.directMemoryRatioForMemoryFileStorage will be helpful. 0.5.1 celeborn.worker.memoryFileStorage.evict.ratio 0.5 false If memory shuffle storage usage rate is above this config, the memory storage shuffle files will evict to free memory. 0.5.1 celeborn.worker.memoryFileStorage.maxFileSize 8MB false Max size for a memory storage file. It must be less than 2GB. 0.5.0 celeborn.worker.monitor.disk.check.interval 30s false Intervals between device monitor to check disk. 0.3.0 celeborn.worker.monitor.disk.checkInterval celeborn.worker.monitor.disk.check.timeout 30s false Timeout time for worker check device status. 0.3.0 celeborn.worker.disk.check.timeout celeborn.worker.monitor.disk.checklist readwrite,diskusage false Monitor type for disk, available items are: iohang, readwrite and diskusage. 0.2.0 celeborn.worker.monitor.disk.enabled true false When true, worker will monitor device and report to master. 0.3.0 celeborn.worker.monitor.disk.notifyError.expireTimeout 10m false The expire timeout of non-critical device error. Only notify critical error when the number of non-critical errors for a period of time exceeds threshold. 0.3.0 celeborn.worker.monitor.disk.notifyError.threshold 64 false Device monitor will only notify critical error once the accumulated valid non-critical error number exceeding this threshold. 0.3.0 celeborn.worker.monitor.disk.sys.block.dir /sys/block false The directory where linux file block information is stored. 0.2.0 celeborn.worker.monitor.memory.check.interval 10ms false Interval of worker direct memory checking. 0.3.0 celeborn.worker.memory.checkInterval celeborn.worker.monitor.memory.report.interval 10s false Interval of worker direct memory tracker reporting to log. 0.3.0 celeborn.worker.memory.reportInterval celeborn.worker.monitor.memory.trimChannelWaitInterval 1s false Wait time after worker trigger channel to trim cache. 0.3.0 celeborn.worker.monitor.memory.trimFlushWaitInterval 1s false Wait time after worker trigger StorageManger to flush data. 0.3.0 celeborn.worker.partition.initial.readBuffersMax 1024 false Max number of initial read buffers 0.3.0 celeborn.worker.partition.initial.readBuffersMin 1 false Min number of initial read buffers 0.3.0 celeborn.worker.partitionSorter.directMemoryRatioThreshold 0.1 false Max ratio of partition sorter's memory for sorting, when reserved memory is higher than max partition sorter memory, partition sorter will stop sorting. 0.2.0 celeborn.worker.push.heartbeat.enabled false false enable the heartbeat from worker to client when pushing data 0.3.0 celeborn.worker.push.io.threads <undefined> false Netty IO thread number of worker to handle client push data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.push.port 0 false Server port for Worker to receive push data request from ShuffleClient. 0.2.0 celeborn.worker.readBuffer.allocationWait 50ms false The time to wait when buffer dispatcher can not allocate a buffer. 0.3.0 celeborn.worker.readBuffer.target.changeThreshold 1mb false The target ratio for pre read memory usage. 0.3.0 celeborn.worker.readBuffer.target.ratio 0.9 false The target ratio for read ahead buffer's memory usage. 0.3.0 celeborn.worker.readBuffer.target.updateInterval 100ms false The interval for memory manager to calculate new read buffer's target memory. 0.3.0 celeborn.worker.readBuffer.toTriggerReadMin 32 false Min buffers count for map data partition to trigger read. 0.3.0 celeborn.worker.register.timeout 180s false Worker register timeout. 0.2.0 celeborn.worker.replicate.fastFail.duration 60s false If a replicate request not replied during the duration, worker will mark the replicate data request as failed.It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes . 0.2.0 celeborn.worker.replicate.io.threads <undefined> false Netty IO thread number of worker to replicate shuffle data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.replicate.port 0 false Server port for Worker to receive replicate data request from other Workers. 0.2.0 celeborn.worker.replicate.randomConnection.enabled true false Whether worker will create random connection to peer when replicate data. When false, worker tend to reuse the same cached TransportClient to a specific replicate worker; when true, worker tend to use different cached TransportClient. Netty will use the same thread to serve the same connection, so with more connections replicate server can leverage more netty threads 0.2.1 celeborn.worker.replicate.threads 64 false Thread number of worker to replicate shuffle data. 0.2.0 celeborn.worker.rpc.port 0 false Server port for Worker to receive RPC request. 0.2.0 celeborn.worker.shuffle.partitionSplit.enabled true false enable the partition split on worker side 0.3.0 celeborn.worker.partition.split.enabled celeborn.worker.shuffle.partitionSplit.max 2g false Specify the maximum partition size for splitting, and ensure that individual partition files are always smaller than this limit. 0.3.0 celeborn.worker.shuffle.partitionSplit.min 1m false Min size for a partition to split 0.3.0 celeborn.shuffle.partitionSplit.min celeborn.worker.sortPartition.indexCache.expire 180s false PartitionSorter's cache item expire time. 0.4.0 celeborn.worker.sortPartition.indexCache.maxWeight 100000 false PartitionSorter's cache max weight for index buffer. 0.4.0 celeborn.worker.sortPartition.prefetch.enabled true false When true, partition sorter will prefetch the original partition files to page cache and reserve memory configured by celeborn.worker.sortPartition.reservedMemoryPerPartition to allocate a block of memory for prefetching while sorting a shuffle file off-heap with page cache for non-hdfs files. Otherwise, partition sorter seeks to position of each block and does not prefetch for non-hdfs files. 0.5.0 celeborn.worker.sortPartition.reservedMemoryPerPartition 1mb false Reserved memory when sorting a shuffle file off-heap. 0.3.0 celeborn.worker.partitionSorter.reservedMemoryPerPartition celeborn.worker.sortPartition.threads <undefined> false PartitionSorter's thread counts. It's recommended to set at least 64 when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.partitionSorter.threads celeborn.worker.sortPartition.timeout 220s false Timeout for a shuffle file to sort. 0.3.0 celeborn.worker.partitionSorter.sort.timeout celeborn.worker.storage.checkDirsEmpty.maxRetries 3 false The number of retries for a worker to check if the working directory is cleaned up before registering with the master. 0.3.0 celeborn.worker.disk.checkFileClean.maxRetries celeborn.worker.storage.checkDirsEmpty.timeout 1000ms false The wait time per retry for a worker to check if the working directory is cleaned up before registering with the master. 0.3.0 celeborn.worker.disk.checkFileClean.timeout celeborn.worker.storage.dirs <undefined> false Directory list to store shuffle data. It's recommended to configure one directory on each disk. Storage size limit can be set for each directory. For the sake of performance, there should be no more than 2 flush threads on the same disk partition if you are using HDD, and should be 8 or more flush threads on the same disk partition if you are using SSD. For example: dir1[:capacity=][:disktype=][:flushthread=],dir2[:capacity=][:disktype=][:flushthread=] 0.2.0 celeborn.worker.storage.disk.reserve.ratio <undefined> false Celeborn worker reserved ratio for each disk. The minimum usable size for each disk is the max space between the reserved space and the space calculate via reserved ratio. 0.3.2 celeborn.worker.storage.disk.reserve.size 5G false Celeborn worker reserved space for each disk. 0.3.0 celeborn.worker.disk.reserve.size celeborn.worker.storage.expireDirs.timeout 1h false The timeout for a expire dirs to be deleted on disk. 0.3.2 celeborn.worker.storage.workingDir celeborn-worker/shuffle_data false Worker's working dir path name. 0.3.0 celeborn.worker.workingDir celeborn.worker.writer.close.timeout 120s false Timeout for a file writer to close 0.2.0 celeborn.worker.writer.create.maxAttempts 3 false Retry count for a file writer to create if its creation was failed. 0.2.0 Client Key Default isDynamic Description Since Deprecated celeborn.client.application.heartbeatInterval 10s false Interval for client to send heartbeat message to master. 0.3.0 celeborn.application.heartbeatInterval celeborn.client.application.unregister.enabled true false When true, Celeborn client will inform celeborn master the application is already shutdown during client exit, this allows the cluster to release resources immediately, resulting in resource savings. 0.3.2 celeborn.client.chunk.prefetch.enabled false false Whether to enable chunk prefetch when creating CelebornInputStream. 0.6.0 celeborn.client.closeIdleConnections true false Whether client will close idle connections. 0.3.0 celeborn.client.commitFiles.ignoreExcludedWorker false false When true, LifecycleManager will skip workers which are in the excluded list. 0.3.0 celeborn.client.eagerlyCreateInputStream.threads 32 false Threads count for streamCreatorPool in CelebornShuffleReader. 0.3.1 celeborn.client.excludePeerWorkerOnFailure.enabled true false When true, Celeborn will exclude partition's peer worker on failure when push data to replica failed. 0.3.0 celeborn.client.excludedWorker.expireTimeout 180s false Timeout time for LifecycleManager to clear reserved excluded worker. Default to be 1.5 * celeborn.master.heartbeat.worker.timeout to cover worker heartbeat timeout check period 0.3.0 celeborn.worker.excluded.expireTimeout celeborn.client.fetch.buffer.size 64k false Size of reducer partition buffer memory for shuffle reader. The fetched data will be buffered in memory before consuming. For performance consideration keep this buffer size not less than celeborn.client.push.buffer.max.size . 0.4.0 celeborn.client.fetch.dfsReadChunkSize 8m false Max chunk size for DfsPartitionReader. 0.3.1 celeborn.client.fetch.excludeWorkerOnFailure.enabled false false Whether to enable shuffle client-side fetch exclude workers on failure. 0.3.0 celeborn.client.fetch.excludedWorker.expireTimeout <value of celeborn.client.excludedWorker.expireTimeout> false ShuffleClient is a static object, it will be used in the whole lifecycle of Executor,We give a expire time for excluded workers to avoid a transient worker issues. 0.3.0 celeborn.client.fetch.maxReqsInFlight 3 false Amount of in-flight chunk fetch request. 0.3.0 celeborn.fetch.maxReqsInFlight celeborn.client.fetch.maxRetriesForEachReplica 3 false Max retry times of fetch chunk on each replica 0.3.0 celeborn.fetch.maxRetriesForEachReplica,celeborn.fetch.maxRetries celeborn.client.fetch.timeout 600s false Timeout for a task to open stream and fetch chunk. 0.3.0 celeborn.fetch.timeout celeborn.client.flink.compression.enabled true false Whether to compress data in Flink plugin. 0.3.0 remote-shuffle.job.enable-data-compression celeborn.client.flink.inputGate.concurrentReadings 2147483647 false Max concurrent reading channels for a input gate. 0.3.0 remote-shuffle.job.concurrent-readings-per-gate celeborn.client.flink.inputGate.memory 32m false Memory reserved for a input gate. 0.3.0 remote-shuffle.job.memory-per-gate celeborn.client.flink.inputGate.supportFloatingBuffer true false Whether to support floating buffer in Flink input gates. 0.3.0 remote-shuffle.job.support-floating-buffer-per-input-gate celeborn.client.flink.resultPartition.memory 64m false Memory reserved for a result partition. 0.3.0 remote-shuffle.job.memory-per-partition celeborn.client.flink.resultPartition.supportFloatingBuffer true false Whether to support floating buffer for result partitions. 0.3.0 remote-shuffle.job.support-floating-buffer-per-output-gate celeborn.client.inputStream.creation.window 16 false Window size that CelebornShuffleReader pre-creates CelebornInputStreams, for coalesced scenariowhere multiple Partitions are read 0.6.0 celeborn.client.mr.pushData.max 32m false Max size for a push data sent from mr client. 0.4.0 celeborn.client.push.buffer.initial.size 8k false 0.3.0 celeborn.push.buffer.initial.size celeborn.client.push.buffer.max.size 64k false Max size of reducer partition buffer memory for shuffle hash writer. The pushed data will be buffered in memory before sending to Celeborn worker. For performance consideration keep this buffer size higher than 32K. Example: If reducer amount is 2000, buffer size is 64K, then each task will consume up to 64KiB * 2000 = 125MiB heap memory. 0.3.0 celeborn.push.buffer.max.size celeborn.client.push.excludeWorkerOnFailure.enabled false false Whether to enable shuffle client-side push exclude workers on failures. 0.3.0 celeborn.client.push.limit.inFlight.sleepInterval 50ms false Sleep interval when check netty in-flight requests to be done. 0.3.0 celeborn.push.limit.inFlight.sleepInterval celeborn.client.push.limit.inFlight.timeout <undefined> false Timeout for netty in-flight requests to be done.Default value should be celeborn.client.push.timeout * 2 . 0.3.0 celeborn.push.limit.inFlight.timeout celeborn.client.push.limit.strategy SIMPLE false The strategy used to control the push speed. Valid strategies are SIMPLE and SLOWSTART. The SLOWSTART strategy usually works with congestion control mechanism on the worker side. 0.3.0 celeborn.client.push.maxReqsInFlight.perWorker 32 false Amount of Netty in-flight requests per worker. Default max memory of in flight requests per worker is celeborn.client.push.maxReqsInFlight.perWorker * celeborn.client.push.buffer.max.size * compression ratio(1 in worst case): 64KiB * 32 = 2MiB. The maximum memory will not exceed celeborn.client.push.maxReqsInFlight.total . 0.3.0 celeborn.client.push.maxReqsInFlight.total 256 false Amount of total Netty in-flight requests. The maximum memory is celeborn.client.push.maxReqsInFlight.total * celeborn.client.push.buffer.max.size * compression ratio(1 in worst case): 64KiB * 256 = 16MiB 0.3.0 celeborn.push.maxReqsInFlight celeborn.client.push.queue.capacity 512 false Push buffer queue size for a task. The maximum memory is celeborn.client.push.buffer.max.size * celeborn.client.push.queue.capacity , default: 64KiB * 512 = 32MiB 0.3.0 celeborn.push.queue.capacity celeborn.client.push.replicate.enabled false false When true, Celeborn worker will replicate shuffle data to another Celeborn worker asynchronously to ensure the pushed shuffle data won't be lost after the node failure. It's recommended to set false when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.push.replicate.enabled celeborn.client.push.retry.threads 8 false Thread number to process shuffle re-send push data requests. 0.3.0 celeborn.push.retry.threads celeborn.client.push.revive.batchSize 2048 false Max number of partitions in one Revive request. 0.3.0 celeborn.client.push.revive.interval 100ms false Interval for client to trigger Revive to LifecycleManager. The number of partitions in one Revive request is celeborn.client.push.revive.batchSize . 0.3.0 celeborn.client.push.revive.maxRetries 5 false Max retry times for reviving when celeborn push data failed. 0.3.0 celeborn.client.push.sendBufferPool.checkExpireInterval 30s false Interval to check expire for send buffer pool. If the pool has been idle for more than celeborn.client.push.sendBufferPool.expireTimeout , the pooled send buffers and push tasks will be cleaned up. 0.3.1 celeborn.client.push.sendBufferPool.expireTimeout 60s false Timeout before clean up SendBufferPool. If SendBufferPool is idle for more than this time, the send buffers and push tasks will be cleaned up. 0.3.1 celeborn.client.push.slowStart.initialSleepTime 500ms false The initial sleep time if the current max in flight requests is 0 0.3.0 celeborn.client.push.slowStart.maxSleepTime 2s false If celeborn.client.push.limit.strategy is set to SLOWSTART, push side will take a sleep strategy for each batch of requests, this controls the max sleep time if the max in flight requests limit is 1 for a long time 0.3.0 celeborn.client.push.sort.randomizePartitionId.enabled false false Whether to randomize partitionId in push sorter. If true, partitionId will be randomized when sort data to avoid skew when push to worker 0.3.0 celeborn.push.sort.randomizePartitionId.enabled celeborn.client.push.stageEnd.timeout <value of celeborn.<module>.io.connectionTimeout> false Timeout for waiting StageEnd. During this process, there are celeborn.client.requestCommitFiles.maxRetries times for retry opportunities for committing files and 1 times for releasing slots request. User can customize this value according to your setting. By default, the value is the max timeout value celeborn.<module>.io.connectionTimeout . 0.3.0 celeborn.push.stageEnd.timeout celeborn.client.push.takeTaskMaxWaitAttempts 1 false Max wait times if no task available to push to worker. 0.3.0 celeborn.client.push.takeTaskWaitInterval 50ms false Wait interval if no task available to push to worker. 0.3.0 celeborn.client.push.timeout 120s false Timeout for a task to push data rpc message. This value should better be more than twice of celeborn.<module>.push.timeoutCheck.interval 0.3.0 celeborn.push.data.timeout celeborn.client.readLocalShuffleFile.enabled false false Enable read local shuffle file for clusters that co-deployed with yarn node manager. 0.3.1 celeborn.client.readLocalShuffleFile.threads 4 false Threads count for read local shuffle file. 0.3.1 celeborn.client.registerShuffle.maxRetries 3 false Max retry times for client to register shuffle. 0.3.0 celeborn.shuffle.register.maxRetries celeborn.client.registerShuffle.retryWait 3s false Wait time before next retry if register shuffle failed. 0.3.0 celeborn.shuffle.register.retryWait celeborn.client.requestCommitFiles.maxRetries 4 false Max retry times for requestCommitFiles RPC. 0.3.0 celeborn.client.reserveSlots.maxRetries 3 false Max retry times for client to reserve slots. 0.3.0 celeborn.slots.reserve.maxRetries celeborn.client.reserveSlots.rackaware.enabled false false Whether need to place different replicates on different racks when allocating slots. 0.3.1 celeborn.client.reserveSlots.rackware.enabled celeborn.client.reserveSlots.retryWait 3s false Wait time before next retry if reserve slots failed. 0.3.0 celeborn.slots.reserve.retryWait celeborn.client.rpc.cache.concurrencyLevel 32 false The number of write locks to update rpc cache. 0.3.0 celeborn.rpc.cache.concurrencyLevel celeborn.client.rpc.cache.expireTime 15s false The time before a cache item is removed. 0.3.0 celeborn.rpc.cache.expireTime celeborn.client.rpc.cache.size 256 false The max cache items count for rpc cache. 0.3.0 celeborn.rpc.cache.size celeborn.client.rpc.commitFiles.askTimeout <value of celeborn.rpc.askTimeout> false Timeout for CommitHandler commit files. 0.4.1 celeborn.client.rpc.getReducerFileGroup.askTimeout <value of celeborn.rpc.askTimeout> false Timeout for ask operations during getting reducer file group information. During this process, there are celeborn.client.requestCommitFiles.maxRetries times for retry opportunities for committing files and 1 times for releasing slots request. User can customize this value according to your setting. 0.2.0 celeborn.client.rpc.maxRetries 3 false Max RPC retry times in LifecycleManager. 0.3.2 celeborn.client.rpc.registerShuffle.askTimeout <value of celeborn.rpc.askTimeout> false Timeout for ask operations during register shuffle. During this process, there are two times for retry opportunities for requesting slots, one request for establishing a connection with Worker and celeborn.client.reserveSlots.maxRetries times for retry opportunities for reserving slots. User can customize this value according to your setting. 0.3.0 celeborn.rpc.registerShuffle.askTimeout celeborn.client.rpc.requestPartition.askTimeout <value of celeborn.rpc.askTimeout> false Timeout for ask operations during requesting change partition location, such as reviving or splitting partition. During this process, there are celeborn.client.reserveSlots.maxRetries times for retry opportunities for reserving slots. User can customize this value according to your setting. 0.2.0 celeborn.client.rpc.reserveSlots.askTimeout <value of celeborn.rpc.askTimeout> false Timeout for LifecycleManager request reserve slots. 0.3.0 celeborn.client.rpc.shared.threads 16 false Number of shared rpc threads in LifecycleManager. 0.3.2 celeborn.client.shuffle.batchHandleChangePartition.interval 100ms false Interval for LifecycleManager to schedule handling change partition requests in batch. 0.3.0 celeborn.shuffle.batchHandleChangePartition.interval celeborn.client.shuffle.batchHandleChangePartition.partitionBuckets 256 false Max number of change partition requests which can be concurrently processed 0.5.0 celeborn.client.shuffle.batchHandleChangePartition.threads 8 false Threads number for LifecycleManager to handle change partition request in batch. 0.3.0 celeborn.shuffle.batchHandleChangePartition.threads celeborn.client.shuffle.batchHandleCommitPartition.interval 5s false Interval for LifecycleManager to schedule handling commit partition requests in batch. 0.3.0 celeborn.shuffle.batchHandleCommitPartition.interval celeborn.client.shuffle.batchHandleCommitPartition.threads 8 false Threads number for LifecycleManager to handle commit partition request in batch. 0.3.0 celeborn.shuffle.batchHandleCommitPartition.threads celeborn.client.shuffle.batchHandleReleasePartition.interval 5s false Interval for LifecycleManager to schedule handling release partition requests in batch. 0.3.0 celeborn.client.shuffle.batchHandleReleasePartition.threads 8 false Threads number for LifecycleManager to handle release partition request in batch. 0.3.0 celeborn.client.shuffle.compression.codec LZ4 false The codec used to compress shuffle data. By default, Celeborn provides three codecs: lz4 , zstd , none . none means that shuffle compression is disabled. Since Flink version 1.17, zstd is supported for Flink shuffle client. 0.3.0 celeborn.shuffle.compression.codec,remote-shuffle.job.compression.codec celeborn.client.shuffle.compression.zstd.level 1 false Compression level for Zstd compression codec, its value should be an integer between -5 and 22. Increasing the compression level will result in better compression at the expense of more CPU and memory. 0.3.0 celeborn.shuffle.compression.zstd.level celeborn.client.shuffle.decompression.lz4.xxhash.instance <undefined> false Decompression XXHash instance for Lz4. Available options: JNI, JAVASAFE, JAVAUNSAFE. 0.3.2 celeborn.client.shuffle.expired.checkInterval 60s false Interval for client to check expired shuffles. 0.3.0 celeborn.shuffle.expired.checkInterval celeborn.client.shuffle.manager.port 0 false Port used by the LifecycleManager on the Driver. 0.3.0 celeborn.shuffle.manager.port celeborn.client.shuffle.mapPartition.split.enabled false false whether to enable shuffle partition split. Currently, this only applies to MapPartition. 0.3.1 celeborn.client.shuffle.partition.type REDUCE false Type of shuffle's partition. 0.3.0 celeborn.shuffle.partition.type celeborn.client.shuffle.partitionSplit.mode SOFT false soft: the shuffle file size might be larger than split threshold. hard: the shuffle file size will be limited to split threshold. 0.3.0 celeborn.shuffle.partitionSplit.mode celeborn.client.shuffle.partitionSplit.threshold 1G false Shuffle file size threshold, if file size exceeds this, trigger split. 0.3.0 celeborn.shuffle.partitionSplit.threshold celeborn.client.shuffle.rangeReadFilter.enabled false false If a spark application have skewed partition, this value can set to true to improve performance. 0.2.0 celeborn.shuffle.rangeReadFilter.enabled celeborn.client.shuffle.register.filterExcludedWorker.enabled false false Whether to filter excluded worker when register shuffle. 0.4.0 celeborn.client.slot.assign.maxWorkers 10000 false Max workers that slots of one shuffle can be allocated on. Will choose the smaller positive one from Master side and Client side, see celeborn.master.slot.assign.maxWorkers . 0.3.1 celeborn.client.spark.fetch.throwsFetchFailure false false client throws FetchFailedException instead of CelebornIOException 0.4.0 celeborn.client.spark.push.dynamicWriteMode.enabled false false Whether to dynamically switch push write mode based on conditions.If true, shuffle mode will be only determined by partition count 0.5.0 celeborn.client.spark.push.dynamicWriteMode.partitionNum.threshold 2000 false Threshold of shuffle partition number for dynamically switching push writer mode. When the shuffle partition number is greater than this value, use the sort-based shuffle writer for memory efficiency; otherwise use the hash-based shuffle writer for speed. This configuration only takes effect when celeborn.client.spark.push.dynamicWriteMode.enabled is true. 0.5.0 celeborn.client.spark.push.sort.memory.maxMemoryFactor 0.4 false the max portion of executor memory which can be used for SortBasedWriter buffer (only valid when celeborn.client.spark.push.sort.memory.useAdaptiveThreshold is enabled 0.5.0 celeborn.client.spark.push.sort.memory.smallPushTolerateFactor 0.2 false Only be in effect when celeborn.client.spark.push.sort.memory.useAdaptiveThreshold is turned on. The larger this value is, the more aggressive Celeborn will enlarge the Sort-based Shuffle writer's memory threshold. Specifically, this config controls when to enlarge the sort shuffle writer's memory threshold. With N bytes data in memory and V as the value of this config, if the number of pushes, C, when using sort based shuffle writer C >= (1 + V) * C' where C' is the number of pushes if we were using hash based writer, we will enlarge the memory threshold by 2X. 0.5.0 celeborn.client.spark.push.sort.memory.threshold 64m false When SortBasedPusher use memory over the threshold, will trigger push data. 0.3.0 celeborn.push.sortMemory.threshold celeborn.client.spark.push.sort.memory.useAdaptiveThreshold false false Adaptively adjust sort-based shuffle writer's memory threshold 0.5.0 celeborn.client.spark.push.unsafeRow.fastWrite.enabled true false This is Celeborn's optimization on UnsafeRow for Spark and it's true by default. If you have changed UnsafeRow's memory layout set this to false. 0.2.2 celeborn.client.spark.shuffle.checkWorker.enabled true false When true, before registering shuffle, LifecycleManager should check if current cluster have available workers, if cluster don't have available workers, fallback to Spark's default shuffle 0.5.0 celeborn.client.spark.shuffle.fallback.numPartitionsThreshold 2147483647 false Celeborn will only accept shuffle of partition number lower than this configuration value. This configuration only takes effect when celeborn.client.spark.shuffle.fallback.policy is AUTO . 0.5.0 celeborn.shuffle.forceFallback.numPartitionsThreshold,celeborn.client.spark.shuffle.forceFallback.numPartitionsThreshold celeborn.client.spark.shuffle.fallback.policy AUTO false Celeborn supports the following kind of fallback policies. 1. ALWAYS: always use spark built-in shuffle implementation; 2. AUTO: prefer to use celeborn shuffle implementation, and fallback to use spark built-in shuffle implementation based on certain factors, e.g. availability of enough workers and quota, shuffle partition number; 3. NEVER: always use celeborn shuffle implementation, and fail fast when it it is concluded that fallback is required based on factors above. 0.5.0 celeborn.client.spark.shuffle.forceFallback.enabled false false Always use spark built-in shuffle implementation. This configuration is deprecated, consider configuring celeborn.client.spark.shuffle.fallback.policy instead. 0.3.0 celeborn.shuffle.forceFallback.enabled celeborn.client.spark.shuffle.writer HASH false Celeborn supports the following kind of shuffle writers. 1. hash: hash-based shuffle writer works fine when shuffle partition count is normal; 2. sort: sort-based shuffle writer works fine when memory pressure is high or shuffle partition count is huge. This configuration only takes effect when celeborn.client.spark.push.dynamicWriteMode.enabled is false. 0.3.0 celeborn.shuffle.writer celeborn.master.endpoints <localhost>:9097 false Endpoints of master nodes for celeborn client to connect, allowed pattern is: <host1>:<port1>[,<host2>:<port2>]* , e.g. clb1:9097,clb2:9098,clb3:9099 . If the port is omitted, 9097 will be used. 0.2.0 celeborn.quota.enabled true false When Master side sets to true, the master will enable to check the quota via QuotaManager. When Client side sets to true, LifecycleManager will request Master side to check whether the current user has enough quota before registration of shuffle. Fallback to the default shuffle service of Spark when Master side checks that there is no enough quota for current user. 0.2.0 celeborn.quota.identity.provider org.apache.celeborn.common.identity.DefaultIdentityProvider false IdentityProvider class name. Default class is org.apache.celeborn.common.identity.DefaultIdentityProvider . Optional values: org.apache.celeborn.common.identity.HadoopBasedIdentityProvider user name will be obtained by UserGroupInformation.getUserName; org.apache.celeborn.common.identity.DefaultIdentityProvider user name and tenant id are default values or user-specific values. 0.2.0 celeborn.quota.identity.user-specific.tenant default false Tenant id if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.quota.identity.user-specific.userName default false User name if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.storage.availableTypes HDD false Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.activeTypes celeborn.storage.hdfs.dir <undefined> false HDFS base directory for Celeborn to store shuffle data. 0.2.0 Quota Key Default isDynamic Description Since Deprecated celeborn.quota.enabled true false When Master side sets to true, the master will enable to check the quota via QuotaManager. When Client side sets to true, LifecycleManager will request Master side to check whether the current user has enough quota before registration of shuffle. Fallback to the default shuffle service of Spark when Master side checks that there is no enough quota for current user. 0.2.0 celeborn.quota.identity.provider org.apache.celeborn.common.identity.DefaultIdentityProvider false IdentityProvider class name. Default class is org.apache.celeborn.common.identity.DefaultIdentityProvider . Optional values: org.apache.celeborn.common.identity.HadoopBasedIdentityProvider user name will be obtained by UserGroupInformation.getUserName; org.apache.celeborn.common.identity.DefaultIdentityProvider user name and tenant id are default values or user-specific values. 0.2.0 celeborn.quota.identity.user-specific.tenant default false Tenant id if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.quota.identity.user-specific.userName default false User name if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.quota.tenant.diskBytesWritten 9223372036854775807 true Quota dynamic configuration for written disk bytes. 0.5.0 celeborn.quota.tenant.diskFileCount 9223372036854775807 true Quota dynamic configuration for written disk file count. 0.5.0 celeborn.quota.tenant.hdfsBytesWritten 9223372036854775807 true Quota dynamic configuration for written hdfs bytes. 0.5.0 celeborn.quota.tenant.hdfsFileCount 9223372036854775807 true Quota dynamic configuration for written hdfs file count. 0.5.0 Network The various transport modules which can be configured are: Module Parent Module Description rpc_app rpc Configure control plane RPC environment used by Celeborn within the application. For backward compatibility, supports fallback to rpc parent module for missing configuration. Note, this is for RPC environment - see below for other transport modules rpc_service rpc Configure control plane RPC environment when communicating with Celeborn service hosts. This includes all RPC communication from application to Celeborn Master/Workers, as well as between Celeborn masters/workers themselves. For backward compatibility, supports fallback to rpc parent module for missing configuration. As with rpc_app , this is only for RPC environment see below for other transport modules. rpc - Fallback parent transport module for rpc_app and rpc_service . It is advisible to use the specific transport modules while configuring - rpc exists primarily for backward compatibility push - Configure transport module for handling data push at Celeborn workers fetch - Configure transport module for handling data fetch at Celeborn workers data - Configure transport module for handling data push and fetch at Celeborn apps replicate - Configure transport module for handling data replication between Celeborn workers Some network configurations might apply in specific scenarios, for example push module for io.maxRetries and io.retryWait in flink client. Please see the full list below for details. Key Default isDynamic Description Since Deprecated celeborn.<module>.fetch.timeoutCheck.interval 5s false Interval for checking fetch data timeout. It only support setting to data since it works for shuffle client fetch data. 0.3.0 celeborn.<module>.fetch.timeoutCheck.threads 4 false Threads num for checking fetch data timeout. It only support setting to data since it works for shuffle client fetch data. 0.3.0 celeborn.<module>.heartbeat.interval 60s false The heartbeat interval between worker and client. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to replicate , it works for replicate client of worker replicating data to peer worker.If you are using the \"celeborn.client.heartbeat.interval\", please use the new configs for each module according to your needs or replace it with \"celeborn.rpc.heartbeat.interval\", \"celeborn.data.heartbeat.interval\" and\"celeborn.replicate.heartbeat.interval\". 0.3.0 celeborn.client.heartbeat.interval celeborn.<module>.io.backLog 0 false Requested maximum length of the queue of incoming connections. Default 0 for no backlog. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. celeborn.<module>.io.clientThreads 0 false Number of threads used in the client thread pool. Default to 0, which is 2x#cores. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to replicate , it works for replicate client of worker replicating data to peer worker. celeborn.<module>.io.connectTimeout <value of celeborn.network.connect.timeout> false Socket connect timeout. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to replicate , it works for the replicate client of worker replicating data to peer worker. celeborn.<module>.io.connectionTimeout <value of celeborn.network.timeout> false Connection active timeout. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server or client of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. celeborn.<module>.io.enableVerboseMetrics false false Whether to track Netty memory detailed metrics. If true, the detailed metrics of Netty PoolByteBufAllocator will be gotten, otherwise only general memory usage will be tracked. celeborn.<module>.io.lazyFD true false Whether to initialize FileDescriptor lazily or not. If true, file descriptors are created only when data is going to be transferred. This can reduce the number of open files. If setting to fetch , it works for worker fetch server. celeborn.<module>.io.maxRetries 3 false Max number of times we will try IO exceptions (such as connection timeouts) per request. If set to 0, we will not do any retries. If setting to push , it works for Flink shuffle client push data. celeborn.<module>.io.mode NIO false Netty EventLoopGroup backend, available options: NIO, EPOLL. celeborn.<module>.io.numConnectionsPerPeer 1 false Number of concurrent connections between two nodes. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to replicate , it works for replicate client of worker replicating data to peer worker. celeborn.<module>.io.preferDirectBufs true false If true, we will prefer allocating off-heap byte buffers within Netty. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server or client of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. celeborn.<module>.io.receiveBuffer 0b false Receive buffer size (SO_RCVBUF). Note: the optimal size for receive buffer and send buffer should be latency * network_bandwidth. Assuming latency = 1ms, network_bandwidth = 10Gbps buffer size should be ~ 1.25MB. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server or client of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. 0.2.0 celeborn.<module>.io.retryWait 5s false Time that we will wait in order to perform a retry after an IOException. Only relevant if maxIORetries > 0. If setting to data , it works for shuffle client push and fetch data. If setting to push , it works for Flink shuffle client push data. 0.2.0 celeborn.<module>.io.saslTimeout 30s false Timeout for a single round trip of auth message exchange, in milliseconds. 0.5.0 celeborn.<module>.io.sendBuffer 0b false Send buffer size (SO_SNDBUF). If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server or client of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. 0.2.0 celeborn.<module>.io.serverThreads 0 false Number of threads used in the server thread pool. Default to 0, which is 2x#cores. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. celeborn.<module>.push.timeoutCheck.interval 5s false Interval for checking push data timeout. If setting to data , it works for shuffle client push data. If setting to push , it works for Flink shuffle client push data. If setting to replicate , it works for replicate client of worker replicating data to peer worker. 0.3.0 celeborn.<module>.push.timeoutCheck.threads 4 false Threads num for checking push data timeout. If setting to data , it works for shuffle client push data. If setting to push , it works for Flink shuffle client push data. If setting to replicate , it works for replicate client of worker replicating data to peer worker. 0.3.0 celeborn.<role>.rpc.dispatcher.threads <value of celeborn.rpc.dispatcher.threads> false Threads number of message dispatcher event loop for roles celeborn.io.maxDefaultNettyThreads 64 false Max default netty threads 0.3.2 celeborn.network.bind.preferIpAddress true false When true , prefer to use IP address, otherwise FQDN. This configuration only takes effects when the bind hostname is not set explicitly, in such case, Celeborn will find the first non-loopback address to bind. 0.3.0 celeborn.network.connect.timeout 10s false Default socket connect timeout. 0.2.0 celeborn.network.memory.allocator.numArenas <undefined> false Number of arenas for pooled memory allocator. Default value is Runtime.getRuntime.availableProcessors, min value is 2. 0.3.0 celeborn.network.memory.allocator.verbose.metric false false Whether to enable verbose metric for pooled allocator. 0.3.0 celeborn.network.timeout 240s false Default timeout for network operations. 0.2.0 celeborn.port.maxRetries 1 false When port is occupied, we will retry for max retry times. 0.2.0 celeborn.rpc.askTimeout 60s false Timeout for RPC ask operations. It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes 0.2.0 celeborn.rpc.connect.threads 64 false 0.2.0 celeborn.rpc.dispatcher.threads 0 false Threads number of message dispatcher event loop. Default to 0, which is availableCore. 0.3.0 celeborn.rpc.dispatcher.numThreads celeborn.rpc.inbox.capacity 0 false Specifies size of the in memory bounded capacity. 0.5.0 celeborn.rpc.io.threads <undefined> false Netty IO thread number of NettyRpcEnv to handle RPC request. The default threads number is the number of runtime available processors. 0.2.0 celeborn.rpc.lookupTimeout 30s false Timeout for RPC lookup operations. 0.2.0 celeborn.shuffle.io.maxChunksBeingTransferred <undefined> false The max number of chunks allowed to be transferred at the same time on shuffle service. Note that new incoming connections will be closed when the max number is hit. The client will retry according to the shuffle retry configs (see celeborn.<module>.io.maxRetries and celeborn.<module>.io.retryWait ), if those limits are reached the task will fail with fetch failure. 0.2.0 celeborn.ssl.<module>.enabled false false Enables SSL for securing wire traffic. 0.5.0 celeborn.ssl.<module>.enabledAlgorithms <undefined> false A comma-separated list of ciphers. The specified ciphers must be supported by JVM. The reference list of protocols can be found in the \"JSSE Cipher Suite Names\" section of the Java security guide. The list for Java 11, for example, can be found at this page Note: If not set, the default cipher suite for the JRE will be used 0.5.0 celeborn.ssl.<module>.keyStore <undefined> false Path to the key store file. The path can be absolute or relative to the directory in which the process is started. 0.5.0 celeborn.ssl.<module>.keyStorePassword <undefined> false Password to the key store. 0.5.0 celeborn.ssl.<module>.protocol TLSv1.2 false TLS protocol to use. The protocol must be supported by JVM. The reference list of protocols can be found in the \"Additional JSSE Standard Names\" section of the Java security guide. For Java 11, for example, the list can be found here 0.5.0 celeborn.ssl.<module>.trustStore <undefined> false Path to the trust store file. The path can be absolute or relative to the directory in which the process is started. 0.5.0 celeborn.ssl.<module>.trustStorePassword <undefined> false Password for the trust store. 0.5.0 celeborn.ssl.<module>.trustStoreReloadIntervalMs 10s false The interval at which the trust store should be reloaded (in milliseconds), when enabled. This setting is mostly only useful for server components, not applications. 0.5.0 celeborn.ssl.<module>.trustStoreReloadingEnabled false false Whether the trust store should be reloaded periodically. This setting is mostly only useful for Celeborn services (masters, workers), and not applications. 0.5.0 Columnar Shuffle Key Default isDynamic Description Since Deprecated celeborn.columnarShuffle.batch.size 10000 false Vector batch size for columnar shuffle. 0.3.0 celeborn.columnar.shuffle.batch.size celeborn.columnarShuffle.codegen.enabled false false Whether to use codegen for columnar-based shuffle. 0.3.0 celeborn.columnar.shuffle.codegen.enabled celeborn.columnarShuffle.enabled false false Whether to enable columnar-based shuffle. 0.2.0 celeborn.columnar.shuffle.enabled celeborn.columnarShuffle.encoding.dictionary.enabled false false Whether to use dictionary encoding for columnar-based shuffle data. 0.3.0 celeborn.columnar.shuffle.encoding.dictionary.enabled celeborn.columnarShuffle.encoding.dictionary.maxFactor 0.3 false Max factor for dictionary size. The max dictionary size is min(32.0 KiB, celeborn.columnarShuffle.batch.size * celeborn.columnar.shuffle.encoding.dictionary.maxFactor) . 0.3.0 celeborn.columnar.shuffle.encoding.dictionary.maxFactor celeborn.columnarShuffle.offHeap.enabled false false Whether to use off heap columnar vector. 0.3.0 celeborn.columnar.offHeap.enabled Metrics Below metrics configuration both work for master and worker. Key Default isDynamic Description Since Deprecated celeborn.metrics.app.topDiskUsage.count 50 false Size for top items about top disk usage applications list. 0.2.0 celeborn.metrics.app.topDiskUsage.interval 10min false Time length for a window about top disk usage application list. 0.2.0 celeborn.metrics.app.topDiskUsage.windowSize 24 false Window size about top disk usage application list. 0.2.0 celeborn.metrics.capacity 4096 false The maximum number of metrics which a source can use to generate output strings. 0.2.0 celeborn.metrics.collectPerfCritical.enabled false false It controls whether to collect metrics which may affect performance. When enable, Celeborn collects them. 0.2.0 celeborn.metrics.conf <undefined> false Custom metrics configuration file path. Default use metrics.properties in classpath. 0.3.0 celeborn.metrics.enabled true false When true, enable metrics system. 0.2.0 celeborn.metrics.extraLabels false If default metric labels are not enough, extra metric labels can be customized. Labels' pattern is: <label1_key>=<label1_value>[,<label2_key>=<label2_value>]* ; e.g. env=prod,version=1 0.3.0 celeborn.metrics.json.path /metrics/json false URI context path of json metrics HTTP server. 0.4.0 celeborn.metrics.json.pretty.enabled true false When true, view metrics in json pretty format 0.4.0 celeborn.metrics.prometheus.path /metrics/prometheus false URI context path of prometheus metrics HTTP server. 0.4.0 celeborn.metrics.sample.rate 1.0 false It controls if Celeborn collect timer metrics for some operations. Its value should be in [0.0, 1.0]. 0.2.0 celeborn.metrics.timer.slidingWindow.size 4096 false The sliding window size of timer metric. 0.2.0 celeborn.metrics.worker.pauseSpentTime.forceAppend.threshold 10 false Force append worker pause spent time even if worker still in pause serving state.Help user can find worker pause spent time increase, when worker always been pause state. metrics.properties *.sink.csv.class = org.apache.celeborn.common.metrics.sink.CsvSink *.sink.prometheusServlet.class = org.apache.celeborn.common.metrics.sink.PrometheusServlet Environment Variables Recommend configuring in conf/celeborn-env.sh . Key Default Description CELEBORN_HOME $(cd \"`dirname \"$0\"`\"/..; pwd) CELEBORN_CONF_DIR ${CELEBORN_CONF_DIR:-\"${CELEBORN_HOME}/conf\"} CELEBORN_MASTER_MEMORY 1 GB CELEBORN_WORKER_MEMORY 1 GB CELEBORN_WORKER_OFFHEAP_MEMORY 1 GB CELEBORN_MASTER_JAVA_OPTS CELEBORN_WORKER_JAVA_OPTS CELEBORN_PID_DIR ${CELEBORN_HOME}/pids CELEBORN_LOG_DIR ${CELEBORN_HOME}/logs CELEBORN_SSH_OPTS -o StrictHostKeyChecking=no CELEBORN_SLEEP Waiting time for start-all and stop-all operations CELEBORN_PREFER_JEMALLOC set true to enable jemalloc memory allocator CELEBORN_JEMALLOC_PATH jemalloc library path Tuning Assume we have a cluster described as below: 5 Celeborn Workers with 20 GB off-heap memory and 10 disks. As we need to reserve 20% off-heap memory for netty, so we could assume 16 GB off-heap memory can be used for flush buffers. If spark.celeborn.client.push.buffer.max.size is 64 KB, we can have in-flight requests up to 1310720. If you have 8192 mapper tasks, you could set spark.celeborn.client.push.maxReqsInFlight=160 to gain performance improvements. If celeborn.worker.flusher.buffer.size is 256 KB, we can have total slots up to 327680 slots. Rack Awareness Celeborn can be rack-aware by setting celeborn.client.reserveSlots.rackware.enabled to true on client side. Shuffle partition block replica placement will use rack awareness for fault tolerance by placing one shuffle partition replica on a different rack. This provides data availability in the event of a network switch failure or partition within the cluster. Celeborn master daemons obtain the rack id of the cluster workers by invoking either an external script or Java class as specified by configuration files. Using either the Java class or external script for topology, output must adhere to the java org.apache.hadoop.net.DNSToSwitchMapping interface. The interface expects a one-to-one correspondence to be maintained and the topology information in the format of /myrack/myhost , where / is the topology delimiter, myrack is the rack identifier, and myhost is the individual host. Assuming a single /24 subnet per rack, one could use the format of /192.168.100.0/192.168.100.5 as a unique rack-host topology mapping. To use the Java class for topology mapping, the class name is specified by the celeborn.hadoop.net.topology.node.switch.mapping.impl parameter in the master configuration file. An example, NetworkTopology.java , is included with the Celeborn distribution and can be customized by the Celeborn administrator. Using a Java class instead of an external script has a performance benefit in that Celeborn doesn't need to fork an external process when a new worker node registers itself. If implementing an external script, it will be specified with the celeborn.hadoop.net.topology.script.file.name parameter in the master side configuration files. Unlike the Java class, the external topology script is not included with the Celeborn distribution and is provided by the administrator. Celeborn will send multiple IP addresses to ARGV when forking the topology script. The number of IP addresses sent to the topology script is controlled with celeborn.hadoop.net.topology.script.number.args and defaults to 100. If celeborn.hadoop.net.topology.script.number.args was changed to 1, a topology script would get forked for each IP submitted by workers. If celeborn.hadoop.net.topology.script.file.name or celeborn.hadoop.net.topology.node.switch.mapping.impl is not set, the rack id /default-rack is returned for any passed IP address. While this behavior appears desirable, it can cause issues with shuffle partition block replication as default behavior is to write one replicated block off rack and is unable to do so as there is only a single rack named /default-rack . Example can refer to Hadoop Rack Awareness since Celeborn use hadoop's code about rack-aware. Worker Recover Status After Restart ShuffleClient records the shuffle partition location's host, service port, and filename, to support workers recovering reading existing shuffle data after worker restart, during worker shutdown, workers should store the meta about reading shuffle partition files in RocksDB or LevelDB(deprecated), and restore the meta after restarting workers, also workers should keep a stable service port to support ShuffleClient retry reading data. Users should set celeborn.worker.graceful.shutdown.enabled to true and set below service port with stable port to support worker recover status. celeborn.worker.rpc.port celeborn.worker.fetch.port celeborn.worker.push.port celeborn.worker.replicate.port","title":"Configuration"},{"location":"configuration/#configuration-guide","text":"This documentation contains Celeborn configuration details and a tuning guide.","title":"Configuration Guide"},{"location":"configuration/#important-configurations","text":"","title":"Important Configurations"},{"location":"configuration/#environment-variables","text":"CELEBORN_WORKER_MEMORY=4g CELEBORN_WORKER_OFFHEAP_MEMORY=24g Celeborn workers tend to improve performance by using off-heap buffers. Off-heap memory requirement can be estimated as below: numDirs = `celeborn.worker.storage.dirs` # the amount of directory will be used by Celeborn storage bufferSize = `celeborn.worker.flusher.buffer.size` # the amount of memory will be used by a single flush buffer off-heap-memory = (disk buffer * disks) + network memory # the disk buffer is a logical memory region that stores shuffle data received from network # shuffle data will be flushed to disks through write tasks # the amount of disk buffer can be set to 1GB or larger for each disk according to the difference of your disk speed and network speed For example, if a Celeborn worker give each disk 1GiB memory and the buffer size is set to 256 KB. Celeborn worker can support up to 4096 concurrent write tasks for each disk. If this worker has 10 disks, the offheap memory should be set to 12GB. Network memory will be consumed when netty reads from a TCP channel, there will need some extra memory. Empirically, Celeborn worker off-heap memory should be set to ((disk buffer * disks) * 1.2) .","title":"Environment Variables"},{"location":"configuration/#all-configurations","text":"","title":"All Configurations"},{"location":"configuration/#master","text":"Key Default isDynamic Description Since Deprecated celeborn.cluster.name default false Celeborn cluster name. 0.5.0 celeborn.dynamicConfig.refresh.interval 120s false Interval for refreshing the corresponding dynamic config periodically. 0.4.0 celeborn.dynamicConfig.store.backend <undefined> false Store backend for dynamic config service. Available options: FS, DB. If not provided, it means that dynamic configuration is disabled. 0.4.0 celeborn.dynamicConfig.store.db.fetch.pageSize 1000 false The page size for db store to query configurations. 0.5.0 celeborn.dynamicConfig.store.db.hikari.connectionTimeout 30s false The connection timeout that a client will wait for a connection from the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.driverClassName false The jdbc driver class name of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.idleTimeout 600s false The idle timeout that a connection is allowed to sit idle in the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.jdbcUrl false The jdbc url of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.maxLifetime 1800s false The maximum lifetime of a connection in the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.maximumPoolSize 2 false The maximum pool size of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.password false The password of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.username false The username of db store backend. 0.5.0 celeborn.dynamicConfig.store.fs.path <undefined> false The path of dynamic config file for fs store backend. The file format should be yaml. The default path is ${CELEBORN_CONF_DIR}/dynamicConfig.yaml . 0.5.0 celeborn.internal.port.enabled false false Whether to create a internal port on Masters/Workers for inter-Masters/Workers communication. This is beneficial when SASL authentication is enforced for all interactions between clients and Celeborn Services, but the services can exchange messages without being subject to SASL authentication. 0.5.0 celeborn.logConf.enabled false false When true , log the CelebornConf for debugging purposes. 0.5.0 celeborn.master.estimatedPartitionSize.initialSize 64mb false Initial partition size for estimation, it will change according to runtime stats. 0.3.0 celeborn.shuffle.initialEstimatedPartitionSize celeborn.master.estimatedPartitionSize.maxSize <undefined> false Max partition size for estimation. Default value should be celeborn.worker.shuffle.partitionSplit.max * 2. 0.4.1 celeborn.master.estimatedPartitionSize.minSize 8mb false Ignore partition size smaller than this configuration of partition size for estimation. 0.3.0 celeborn.shuffle.minPartitionSizeToEstimate celeborn.master.estimatedPartitionSize.update.initialDelay 5min false Initial delay time before start updating partition size for estimation. 0.3.0 celeborn.shuffle.estimatedPartitionSize.update.initialDelay celeborn.master.estimatedPartitionSize.update.interval 10min false Interval of updating partition size for estimation. 0.3.0 celeborn.shuffle.estimatedPartitionSize.update.interval celeborn.master.hdfs.expireDirs.timeout 1h false The timeout for a expire dirs to be deleted on HDFS. 0.3.0 celeborn.master.heartbeat.application.timeout 300s false Application heartbeat timeout. 0.3.0 celeborn.application.heartbeat.timeout celeborn.master.heartbeat.worker.timeout 120s false Worker heartbeat timeout. 0.3.0 celeborn.worker.heartbeat.timeout celeborn.master.host <localhost> false Hostname for master to bind. 0.2.0 celeborn.master.http.host <localhost> false Master's http host. 0.4.0 celeborn.metrics.master.prometheus.host,celeborn.master.metrics.prometheus.host celeborn.master.http.idleTimeout 30s false Master http server idle timeout. 0.5.0 celeborn.master.http.maxWorkerThreads 200 false Maximum number of threads in the master http worker thread pool. 0.5.0 celeborn.master.http.port 9098 false Master's http port. 0.4.0 celeborn.metrics.master.prometheus.port,celeborn.master.metrics.prometheus.port celeborn.master.http.stopTimeout 5s false Master http server stop timeout. 0.5.0 celeborn.master.internal.port 8097 false Internal port on the master where both workers and other master nodes connect. 0.5.0 celeborn.master.port 9097 false Port for master to bind. 0.2.0 celeborn.master.rackResolver.refresh.interval 30s false Interval for refreshing the node rack information periodically. 0.5.0 celeborn.master.send.applicationMeta.threads 8 false Number of threads used by the Master to send ApplicationMeta to Workers. 0.5.0 celeborn.master.slot.assign.extraSlots 2 false Extra slots number when master assign slots. 0.3.0 celeborn.slots.assign.extraSlots celeborn.master.slot.assign.loadAware.diskGroupGradient 0.1 false This value means how many more workload will be placed into a faster disk group than a slower group. 0.3.0 celeborn.slots.assign.loadAware.diskGroupGradient celeborn.master.slot.assign.loadAware.fetchTimeWeight 1.0 false Weight of average fetch time when calculating ordering in load-aware assignment strategy 0.3.0 celeborn.slots.assign.loadAware.fetchTimeWeight celeborn.master.slot.assign.loadAware.flushTimeWeight 0.0 false Weight of average flush time when calculating ordering in load-aware assignment strategy 0.3.0 celeborn.slots.assign.loadAware.flushTimeWeight celeborn.master.slot.assign.loadAware.numDiskGroups 5 false This configuration is a guidance for load-aware slot allocation algorithm. This value is control how many disk groups will be created. 0.3.0 celeborn.slots.assign.loadAware.numDiskGroups celeborn.master.slot.assign.maxWorkers 10000 false Max workers that slots of one shuffle can be allocated on. Will choose the smaller positive one from Master side and Client side, see celeborn.client.slot.assign.maxWorkers . 0.3.1 celeborn.master.slot.assign.policy ROUNDROBIN false Policy for master to assign slots, Celeborn supports two types of policy: roundrobin and loadaware. Loadaware policy will be ignored when HDFS is enabled in celeborn.storage.activeTypes 0.3.0 celeborn.slots.assign.policy celeborn.master.userResourceConsumption.update.interval 30s false Time length for a window about compute user resource consumption. 0.3.0 celeborn.master.workerUnavailableInfo.expireTimeout 1800s false Worker unavailable info would be cleared when the retention period is expired 0.3.1 celeborn.quota.enabled true false When Master side sets to true, the master will enable to check the quota via QuotaManager. When Client side sets to true, LifecycleManager will request Master side to check whether the current user has enough quota before registration of shuffle. Fallback to the default shuffle service of Spark when Master side checks that there is no enough quota for current user. 0.2.0 celeborn.redaction.regex (?i)secret password token access[.]key false celeborn.storage.availableTypes HDD false Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.activeTypes celeborn.storage.hdfs.dir <undefined> false HDFS base directory for Celeborn to store shuffle data. 0.2.0 celeborn.storage.hdfs.kerberos.keytab <undefined> false Kerberos keytab file path for HDFS storage connection. 0.3.2 celeborn.storage.hdfs.kerberos.principal <undefined> false Kerberos principal for HDFS storage connection. 0.3.2 Apart from these, the following properties are also available for enable master HA:","title":"Master"},{"location":"configuration/#master-ha","text":"Key Default isDynamic Description Since Deprecated celeborn.master.ha.enabled false false When true, master nodes run as Raft cluster mode. 0.3.0 celeborn.ha.enabled celeborn.master.ha.node.<id>.host <required> false Host to bind of master node in HA mode. 0.3.0 celeborn.ha.master.node.<id>.host celeborn.master.ha.node.<id>.internal.port 8097 false Internal port for the workers and other masters to bind to a master node in HA mode. 0.5.0 celeborn.master.ha.node.<id>.port 9097 false Port to bind of master node in HA mode. 0.3.0 celeborn.ha.master.node.<id>.port celeborn.master.ha.node.<id>.ratis.port 9872 false Ratis port to bind of master node in HA mode. 0.3.0 celeborn.ha.master.node.<id>.ratis.port celeborn.master.ha.ratis.raft.rpc.type netty false RPC type for Ratis, available options: netty, grpc. 0.3.0 celeborn.ha.master.ratis.raft.rpc.type celeborn.master.ha.ratis.raft.server.storage.dir /tmp/ratis false Root storage directory to hold RaftServer data. 0.3.0 celeborn.ha.master.ratis.raft.server.storage.dir celeborn.master.ha.ratis.raft.server.storage.startup.option RECOVER false Startup option of RaftServer storage. Available options: RECOVER, FORMAT. 0.5.0","title":"Master HA"},{"location":"configuration/#worker","text":"Key Default isDynamic Description Since Deprecated celeborn.cluster.name default false Celeborn cluster name. 0.5.0 celeborn.dynamicConfig.refresh.interval 120s false Interval for refreshing the corresponding dynamic config periodically. 0.4.0 celeborn.dynamicConfig.store.backend <undefined> false Store backend for dynamic config service. Available options: FS, DB. If not provided, it means that dynamic configuration is disabled. 0.4.0 celeborn.dynamicConfig.store.db.fetch.pageSize 1000 false The page size for db store to query configurations. 0.5.0 celeborn.dynamicConfig.store.db.hikari.connectionTimeout 30s false The connection timeout that a client will wait for a connection from the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.driverClassName false The jdbc driver class name of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.idleTimeout 600s false The idle timeout that a connection is allowed to sit idle in the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.jdbcUrl false The jdbc url of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.maxLifetime 1800s false The maximum lifetime of a connection in the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.maximumPoolSize 2 false The maximum pool size of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.password false The password of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.username false The username of db store backend. 0.5.0 celeborn.dynamicConfig.store.fs.path <undefined> false The path of dynamic config file for fs store backend. The file format should be yaml. The default path is ${CELEBORN_CONF_DIR}/dynamicConfig.yaml . 0.5.0 celeborn.internal.port.enabled false false Whether to create a internal port on Masters/Workers for inter-Masters/Workers communication. This is beneficial when SASL authentication is enforced for all interactions between clients and Celeborn Services, but the services can exchange messages without being subject to SASL authentication. 0.5.0 celeborn.logConf.enabled false false When true , log the CelebornConf for debugging purposes. 0.5.0 celeborn.master.endpoints <localhost>:9097 false Endpoints of master nodes for celeborn client to connect, allowed pattern is: <host1>:<port1>[,<host2>:<port2>]* , e.g. clb1:9097,clb2:9098,clb3:9099 . If the port is omitted, 9097 will be used. 0.2.0 celeborn.master.estimatedPartitionSize.minSize 8mb false Ignore partition size smaller than this configuration of partition size for estimation. 0.3.0 celeborn.shuffle.minPartitionSizeToEstimate celeborn.master.internal.endpoints <localhost>:8097 false Endpoints of master nodes just for celeborn workers to connect, allowed pattern is: <host1>:<port1>[,<host2>:<port2>]* , e.g. clb1:8097,clb2:8097,clb3:8097 . If the port is omitted, 8097 will be used. 0.5.0 celeborn.redaction.regex (?i)secret password token access[.]key false celeborn.shuffle.chunk.size 8m false Max chunk size of reducer's merged shuffle data. For example, if a reducer's shuffle data is 128M and the data will need 16 fetch chunk requests to fetch. 0.2.0 celeborn.shuffle.sortPartition.block.compactionFactor 0.25 false Combine sorted shuffle blocks such that size of compacted shuffle block does not exceed compactionFactor * celeborn.shuffle.chunk.size 0.4.2 celeborn.storage.availableTypes HDD false Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.activeTypes celeborn.storage.hdfs.dir <undefined> false HDFS base directory for Celeborn to store shuffle data. 0.2.0 celeborn.storage.hdfs.kerberos.keytab <undefined> false Kerberos keytab file path for HDFS storage connection. 0.3.2 celeborn.storage.hdfs.kerberos.principal <undefined> false Kerberos principal for HDFS storage connection. 0.3.2 celeborn.worker.activeConnection.max <undefined> false If the number of active connections on a worker exceeds this configuration value, the worker will be marked as high-load in the heartbeat report, and the master will not include that node in the response of RequestSlots. 0.3.1 celeborn.worker.applicationRegistry.cache.size 10000 false Cache size of the application registry on Workers. 0.5.0 celeborn.worker.bufferStream.threadsPerMountpoint 8 false Threads count for read buffer per mount point. 0.3.0 celeborn.worker.clean.threads 64 false Thread number of worker to clean up expired shuffle keys. 0.3.2 celeborn.worker.closeIdleConnections false false Whether worker will close idle connections. 0.2.0 celeborn.worker.commitFiles.threads 32 false Thread number of worker to commit shuffle data files asynchronously. It's recommended to set at least 128 when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.commit.threads celeborn.worker.commitFiles.timeout 120s false Timeout for a Celeborn worker to commit files of a shuffle. It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.shuffle.commit.timeout celeborn.worker.congestionControl.check.interval 10ms false Interval of worker checks congestion if celeborn.worker.congestionControl.enabled is true. 0.3.2 celeborn.worker.congestionControl.enabled false false Whether to enable congestion control or not. 0.3.0 celeborn.worker.congestionControl.high.watermark <undefined> false If the total bytes in disk buffer exceeds this configure, will start to congestusers whose produce rate is higher than the potential average consume rate. The congestion will stop if the produce rate is lower or equal to the average consume rate, or the total pending bytes lower than celeborn.worker.congestionControl.low.watermark 0.3.0 celeborn.worker.congestionControl.low.watermark <undefined> false Will stop congest users if the total pending bytes of disk buffer is lower than this configuration 0.3.0 celeborn.worker.congestionControl.sample.time.window 10s false The worker holds a time sliding list to calculate users' produce/consume rate 0.3.0 celeborn.worker.congestionControl.user.inactive.interval 10min false How long will consider this user is inactive if it doesn't send data 0.3.0 celeborn.worker.decommission.checkInterval 30s false The wait interval of checking whether all the shuffle expired during worker decommission 0.4.0 celeborn.worker.decommission.forceExitTimeout 6h false The wait time of waiting for all the shuffle expire during worker decommission. 0.4.0 celeborn.worker.directMemoryRatioForMemoryFileStorage 0.0 false Max ratio of direct memory to store shuffle data. This feature is experimental and disabled by default. 0.5.0 celeborn.worker.directMemoryRatioForReadBuffer 0.1 false Max ratio of direct memory for read buffer 0.2.0 celeborn.worker.directMemoryRatioToPauseReceive 0.85 false If direct memory usage reaches this limit, the worker will stop to receive data from Celeborn shuffle clients. 0.2.0 celeborn.worker.directMemoryRatioToPauseReplicate 0.95 false If direct memory usage reaches this limit, the worker will stop to receive replication data from other workers. This value should be higher than celeborn.worker.directMemoryRatioToPauseReceive. 0.2.0 celeborn.worker.directMemoryRatioToResume 0.7 false If direct memory usage is less than this limit, worker will resume. 0.2.0 celeborn.worker.disk.clean.threads 4 false Thread number of worker to clean up directories of expired shuffle keys on disk. 0.3.2 celeborn.worker.fetch.heartbeat.enabled false false enable the heartbeat from worker to client when fetching data 0.3.0 celeborn.worker.fetch.io.threads <undefined> false Netty IO thread number of worker to handle client fetch data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.fetch.port 0 false Server port for Worker to receive fetch data request from ShuffleClient. 0.2.0 celeborn.worker.flusher.buffer.size 256k false Size of buffer used by a single flusher. 0.2.0 celeborn.worker.flusher.diskTime.slidingWindow.size 20 false The size of sliding windows used to calculate statistics about flushed time and count. 0.3.0 celeborn.worker.flusher.avgFlushTime.slidingWindow.size celeborn.worker.flusher.hdd.threads 1 false Flusher's thread count per disk used for write data to HDD disks. 0.2.0 celeborn.worker.flusher.hdfs.buffer.size 4m false Size of buffer used by a HDFS flusher. 0.3.0 celeborn.worker.flusher.hdfs.threads 8 false Flusher's thread count used for write data to HDFS. 0.2.0 celeborn.worker.flusher.shutdownTimeout 3s false Timeout for a flusher to shutdown. 0.2.0 celeborn.worker.flusher.ssd.threads 16 false Flusher's thread count per disk used for write data to SSD disks. 0.2.0 celeborn.worker.flusher.threads 16 false Flusher's thread count per disk for unknown-type disks. 0.2.0 celeborn.worker.graceful.shutdown.checkSlotsFinished.interval 1s false The wait interval of checking whether all released slots to be committed or destroyed during worker graceful shutdown 0.2.0 celeborn.worker.graceful.shutdown.checkSlotsFinished.timeout 480s false The wait time of waiting for the released slots to be committed or destroyed during worker graceful shutdown. 0.2.0 celeborn.worker.graceful.shutdown.enabled false false When true, during worker shutdown, the worker will wait for all released slots to be committed or destroyed. 0.2.0 celeborn.worker.graceful.shutdown.partitionSorter.shutdownTimeout 120s false The wait time of waiting for sorting partition files during worker graceful shutdown. 0.2.0 celeborn.worker.graceful.shutdown.recoverDbBackend ROCKSDB false Specifies a disk-based store used in local db. ROCKSDB or LEVELDB (deprecated). 0.4.0 celeborn.worker.graceful.shutdown.recoverPath <tmp>/recover false The path to store DB. 0.2.0 celeborn.worker.graceful.shutdown.saveCommittedFileInfo.interval 5s false Interval for a Celeborn worker to flush committed file infos into Level DB. 0.3.1 celeborn.worker.graceful.shutdown.saveCommittedFileInfo.sync false false Whether to call sync method to save committed file infos into Level DB to handle OS crash. 0.3.1 celeborn.worker.graceful.shutdown.timeout 600s false The worker's graceful shutdown timeout time. 0.2.0 celeborn.worker.http.host <localhost> false Worker's http host. 0.4.0 celeborn.metrics.worker.prometheus.host,celeborn.worker.metrics.prometheus.host celeborn.worker.http.idleTimeout 30s false Worker http server idle timeout. 0.5.0 celeborn.worker.http.maxWorkerThreads 200 false Maximum number of threads in the worker http worker thread pool. 0.5.0 celeborn.worker.http.port 9096 false Worker's http port. 0.4.0 celeborn.metrics.worker.prometheus.port,celeborn.worker.metrics.prometheus.port celeborn.worker.http.stopTimeout 5s false Worker http server stop timeout. 0.5.0 celeborn.worker.internal.port 0 false Internal server port on the Worker where the master nodes connect. 0.5.0 celeborn.worker.jvmProfiler.enabled false false Turn on code profiling via async_profiler in workers. 0.5.0 celeborn.worker.jvmProfiler.localDir . false Local file system path on worker where profiler output is saved. Defaults to the working directory of the worker process. 0.5.0 celeborn.worker.jvmProfiler.options event=wall,interval=10ms,alloc=2m,lock=10ms,chunktime=300s false Options to pass on to the async profiler. 0.5.0 celeborn.worker.jvmQuake.check.interval 1s false Interval of gc behavior checking for worker jvm quake. 0.4.0 celeborn.worker.jvmQuake.dump.enabled true false Whether to heap dump for the maximum GC 'deficit' during worker jvm quake. 0.4.0 celeborn.worker.jvmQuake.dump.path <tmp>/jvm-quake/dump/<pid> false The path of heap dump for the maximum GC 'deficit' during worker jvm quake. 0.4.0 celeborn.worker.jvmQuake.dump.threshold 30s false The threshold of heap dump for the maximum GC 'deficit' which can be accumulated before jvmquake takes action. Meanwhile, there is no heap dump generated when dump threshold is greater than kill threshold. 0.4.0 celeborn.worker.jvmQuake.enabled false false When true, Celeborn worker will start the jvm quake to monitor of gc behavior, which enables early detection of memory management issues and facilitates fast failure. 0.4.0 celeborn.worker.jvmQuake.exitCode 502 false The exit code of system kill for the maximum GC 'deficit' during worker jvm quake. 0.4.0 celeborn.worker.jvmQuake.kill.threshold 60s false The threshold of system kill for the maximum GC 'deficit' which can be accumulated before jvmquake takes action. 0.4.0 celeborn.worker.jvmQuake.runtimeWeight 5.0 false The factor by which to multiply running JVM time, when weighing it against GCing time. 'Deficit' is accumulated as gc_time - runtime * runtime_weight , and is compared against threshold to determine whether to take action. 0.4.0 celeborn.worker.memoryFileStorage.evict.aggressiveMode.enabled false false If this set to true, memory shuffle files will be evicted when worker is in PAUSED state. If the worker's offheap memory is not ample, set this to true and decrease celeborn.worker.directMemoryRatioForMemoryFileStorage will be helpful. 0.5.1 celeborn.worker.memoryFileStorage.evict.ratio 0.5 false If memory shuffle storage usage rate is above this config, the memory storage shuffle files will evict to free memory. 0.5.1 celeborn.worker.memoryFileStorage.maxFileSize 8MB false Max size for a memory storage file. It must be less than 2GB. 0.5.0 celeborn.worker.monitor.disk.check.interval 30s false Intervals between device monitor to check disk. 0.3.0 celeborn.worker.monitor.disk.checkInterval celeborn.worker.monitor.disk.check.timeout 30s false Timeout time for worker check device status. 0.3.0 celeborn.worker.disk.check.timeout celeborn.worker.monitor.disk.checklist readwrite,diskusage false Monitor type for disk, available items are: iohang, readwrite and diskusage. 0.2.0 celeborn.worker.monitor.disk.enabled true false When true, worker will monitor device and report to master. 0.3.0 celeborn.worker.monitor.disk.notifyError.expireTimeout 10m false The expire timeout of non-critical device error. Only notify critical error when the number of non-critical errors for a period of time exceeds threshold. 0.3.0 celeborn.worker.monitor.disk.notifyError.threshold 64 false Device monitor will only notify critical error once the accumulated valid non-critical error number exceeding this threshold. 0.3.0 celeborn.worker.monitor.disk.sys.block.dir /sys/block false The directory where linux file block information is stored. 0.2.0 celeborn.worker.monitor.memory.check.interval 10ms false Interval of worker direct memory checking. 0.3.0 celeborn.worker.memory.checkInterval celeborn.worker.monitor.memory.report.interval 10s false Interval of worker direct memory tracker reporting to log. 0.3.0 celeborn.worker.memory.reportInterval celeborn.worker.monitor.memory.trimChannelWaitInterval 1s false Wait time after worker trigger channel to trim cache. 0.3.0 celeborn.worker.monitor.memory.trimFlushWaitInterval 1s false Wait time after worker trigger StorageManger to flush data. 0.3.0 celeborn.worker.partition.initial.readBuffersMax 1024 false Max number of initial read buffers 0.3.0 celeborn.worker.partition.initial.readBuffersMin 1 false Min number of initial read buffers 0.3.0 celeborn.worker.partitionSorter.directMemoryRatioThreshold 0.1 false Max ratio of partition sorter's memory for sorting, when reserved memory is higher than max partition sorter memory, partition sorter will stop sorting. 0.2.0 celeborn.worker.push.heartbeat.enabled false false enable the heartbeat from worker to client when pushing data 0.3.0 celeborn.worker.push.io.threads <undefined> false Netty IO thread number of worker to handle client push data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.push.port 0 false Server port for Worker to receive push data request from ShuffleClient. 0.2.0 celeborn.worker.readBuffer.allocationWait 50ms false The time to wait when buffer dispatcher can not allocate a buffer. 0.3.0 celeborn.worker.readBuffer.target.changeThreshold 1mb false The target ratio for pre read memory usage. 0.3.0 celeborn.worker.readBuffer.target.ratio 0.9 false The target ratio for read ahead buffer's memory usage. 0.3.0 celeborn.worker.readBuffer.target.updateInterval 100ms false The interval for memory manager to calculate new read buffer's target memory. 0.3.0 celeborn.worker.readBuffer.toTriggerReadMin 32 false Min buffers count for map data partition to trigger read. 0.3.0 celeborn.worker.register.timeout 180s false Worker register timeout. 0.2.0 celeborn.worker.replicate.fastFail.duration 60s false If a replicate request not replied during the duration, worker will mark the replicate data request as failed.It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes . 0.2.0 celeborn.worker.replicate.io.threads <undefined> false Netty IO thread number of worker to replicate shuffle data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.replicate.port 0 false Server port for Worker to receive replicate data request from other Workers. 0.2.0 celeborn.worker.replicate.randomConnection.enabled true false Whether worker will create random connection to peer when replicate data. When false, worker tend to reuse the same cached TransportClient to a specific replicate worker; when true, worker tend to use different cached TransportClient. Netty will use the same thread to serve the same connection, so with more connections replicate server can leverage more netty threads 0.2.1 celeborn.worker.replicate.threads 64 false Thread number of worker to replicate shuffle data. 0.2.0 celeborn.worker.rpc.port 0 false Server port for Worker to receive RPC request. 0.2.0 celeborn.worker.shuffle.partitionSplit.enabled true false enable the partition split on worker side 0.3.0 celeborn.worker.partition.split.enabled celeborn.worker.shuffle.partitionSplit.max 2g false Specify the maximum partition size for splitting, and ensure that individual partition files are always smaller than this limit. 0.3.0 celeborn.worker.shuffle.partitionSplit.min 1m false Min size for a partition to split 0.3.0 celeborn.shuffle.partitionSplit.min celeborn.worker.sortPartition.indexCache.expire 180s false PartitionSorter's cache item expire time. 0.4.0 celeborn.worker.sortPartition.indexCache.maxWeight 100000 false PartitionSorter's cache max weight for index buffer. 0.4.0 celeborn.worker.sortPartition.prefetch.enabled true false When true, partition sorter will prefetch the original partition files to page cache and reserve memory configured by celeborn.worker.sortPartition.reservedMemoryPerPartition to allocate a block of memory for prefetching while sorting a shuffle file off-heap with page cache for non-hdfs files. Otherwise, partition sorter seeks to position of each block and does not prefetch for non-hdfs files. 0.5.0 celeborn.worker.sortPartition.reservedMemoryPerPartition 1mb false Reserved memory when sorting a shuffle file off-heap. 0.3.0 celeborn.worker.partitionSorter.reservedMemoryPerPartition celeborn.worker.sortPartition.threads <undefined> false PartitionSorter's thread counts. It's recommended to set at least 64 when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.partitionSorter.threads celeborn.worker.sortPartition.timeout 220s false Timeout for a shuffle file to sort. 0.3.0 celeborn.worker.partitionSorter.sort.timeout celeborn.worker.storage.checkDirsEmpty.maxRetries 3 false The number of retries for a worker to check if the working directory is cleaned up before registering with the master. 0.3.0 celeborn.worker.disk.checkFileClean.maxRetries celeborn.worker.storage.checkDirsEmpty.timeout 1000ms false The wait time per retry for a worker to check if the working directory is cleaned up before registering with the master. 0.3.0 celeborn.worker.disk.checkFileClean.timeout celeborn.worker.storage.dirs <undefined> false Directory list to store shuffle data. It's recommended to configure one directory on each disk. Storage size limit can be set for each directory. For the sake of performance, there should be no more than 2 flush threads on the same disk partition if you are using HDD, and should be 8 or more flush threads on the same disk partition if you are using SSD. For example: dir1[:capacity=][:disktype=][:flushthread=],dir2[:capacity=][:disktype=][:flushthread=] 0.2.0 celeborn.worker.storage.disk.reserve.ratio <undefined> false Celeborn worker reserved ratio for each disk. The minimum usable size for each disk is the max space between the reserved space and the space calculate via reserved ratio. 0.3.2 celeborn.worker.storage.disk.reserve.size 5G false Celeborn worker reserved space for each disk. 0.3.0 celeborn.worker.disk.reserve.size celeborn.worker.storage.expireDirs.timeout 1h false The timeout for a expire dirs to be deleted on disk. 0.3.2 celeborn.worker.storage.workingDir celeborn-worker/shuffle_data false Worker's working dir path name. 0.3.0 celeborn.worker.workingDir celeborn.worker.writer.close.timeout 120s false Timeout for a file writer to close 0.2.0 celeborn.worker.writer.create.maxAttempts 3 false Retry count for a file writer to create if its creation was failed. 0.2.0","title":"Worker"},{"location":"configuration/#client","text":"Key Default isDynamic Description Since Deprecated celeborn.client.application.heartbeatInterval 10s false Interval for client to send heartbeat message to master. 0.3.0 celeborn.application.heartbeatInterval celeborn.client.application.unregister.enabled true false When true, Celeborn client will inform celeborn master the application is already shutdown during client exit, this allows the cluster to release resources immediately, resulting in resource savings. 0.3.2 celeborn.client.chunk.prefetch.enabled false false Whether to enable chunk prefetch when creating CelebornInputStream. 0.6.0 celeborn.client.closeIdleConnections true false Whether client will close idle connections. 0.3.0 celeborn.client.commitFiles.ignoreExcludedWorker false false When true, LifecycleManager will skip workers which are in the excluded list. 0.3.0 celeborn.client.eagerlyCreateInputStream.threads 32 false Threads count for streamCreatorPool in CelebornShuffleReader. 0.3.1 celeborn.client.excludePeerWorkerOnFailure.enabled true false When true, Celeborn will exclude partition's peer worker on failure when push data to replica failed. 0.3.0 celeborn.client.excludedWorker.expireTimeout 180s false Timeout time for LifecycleManager to clear reserved excluded worker. Default to be 1.5 * celeborn.master.heartbeat.worker.timeout to cover worker heartbeat timeout check period 0.3.0 celeborn.worker.excluded.expireTimeout celeborn.client.fetch.buffer.size 64k false Size of reducer partition buffer memory for shuffle reader. The fetched data will be buffered in memory before consuming. For performance consideration keep this buffer size not less than celeborn.client.push.buffer.max.size . 0.4.0 celeborn.client.fetch.dfsReadChunkSize 8m false Max chunk size for DfsPartitionReader. 0.3.1 celeborn.client.fetch.excludeWorkerOnFailure.enabled false false Whether to enable shuffle client-side fetch exclude workers on failure. 0.3.0 celeborn.client.fetch.excludedWorker.expireTimeout <value of celeborn.client.excludedWorker.expireTimeout> false ShuffleClient is a static object, it will be used in the whole lifecycle of Executor,We give a expire time for excluded workers to avoid a transient worker issues. 0.3.0 celeborn.client.fetch.maxReqsInFlight 3 false Amount of in-flight chunk fetch request. 0.3.0 celeborn.fetch.maxReqsInFlight celeborn.client.fetch.maxRetriesForEachReplica 3 false Max retry times of fetch chunk on each replica 0.3.0 celeborn.fetch.maxRetriesForEachReplica,celeborn.fetch.maxRetries celeborn.client.fetch.timeout 600s false Timeout for a task to open stream and fetch chunk. 0.3.0 celeborn.fetch.timeout celeborn.client.flink.compression.enabled true false Whether to compress data in Flink plugin. 0.3.0 remote-shuffle.job.enable-data-compression celeborn.client.flink.inputGate.concurrentReadings 2147483647 false Max concurrent reading channels for a input gate. 0.3.0 remote-shuffle.job.concurrent-readings-per-gate celeborn.client.flink.inputGate.memory 32m false Memory reserved for a input gate. 0.3.0 remote-shuffle.job.memory-per-gate celeborn.client.flink.inputGate.supportFloatingBuffer true false Whether to support floating buffer in Flink input gates. 0.3.0 remote-shuffle.job.support-floating-buffer-per-input-gate celeborn.client.flink.resultPartition.memory 64m false Memory reserved for a result partition. 0.3.0 remote-shuffle.job.memory-per-partition celeborn.client.flink.resultPartition.supportFloatingBuffer true false Whether to support floating buffer for result partitions. 0.3.0 remote-shuffle.job.support-floating-buffer-per-output-gate celeborn.client.inputStream.creation.window 16 false Window size that CelebornShuffleReader pre-creates CelebornInputStreams, for coalesced scenariowhere multiple Partitions are read 0.6.0 celeborn.client.mr.pushData.max 32m false Max size for a push data sent from mr client. 0.4.0 celeborn.client.push.buffer.initial.size 8k false 0.3.0 celeborn.push.buffer.initial.size celeborn.client.push.buffer.max.size 64k false Max size of reducer partition buffer memory for shuffle hash writer. The pushed data will be buffered in memory before sending to Celeborn worker. For performance consideration keep this buffer size higher than 32K. Example: If reducer amount is 2000, buffer size is 64K, then each task will consume up to 64KiB * 2000 = 125MiB heap memory. 0.3.0 celeborn.push.buffer.max.size celeborn.client.push.excludeWorkerOnFailure.enabled false false Whether to enable shuffle client-side push exclude workers on failures. 0.3.0 celeborn.client.push.limit.inFlight.sleepInterval 50ms false Sleep interval when check netty in-flight requests to be done. 0.3.0 celeborn.push.limit.inFlight.sleepInterval celeborn.client.push.limit.inFlight.timeout <undefined> false Timeout for netty in-flight requests to be done.Default value should be celeborn.client.push.timeout * 2 . 0.3.0 celeborn.push.limit.inFlight.timeout celeborn.client.push.limit.strategy SIMPLE false The strategy used to control the push speed. Valid strategies are SIMPLE and SLOWSTART. The SLOWSTART strategy usually works with congestion control mechanism on the worker side. 0.3.0 celeborn.client.push.maxReqsInFlight.perWorker 32 false Amount of Netty in-flight requests per worker. Default max memory of in flight requests per worker is celeborn.client.push.maxReqsInFlight.perWorker * celeborn.client.push.buffer.max.size * compression ratio(1 in worst case): 64KiB * 32 = 2MiB. The maximum memory will not exceed celeborn.client.push.maxReqsInFlight.total . 0.3.0 celeborn.client.push.maxReqsInFlight.total 256 false Amount of total Netty in-flight requests. The maximum memory is celeborn.client.push.maxReqsInFlight.total * celeborn.client.push.buffer.max.size * compression ratio(1 in worst case): 64KiB * 256 = 16MiB 0.3.0 celeborn.push.maxReqsInFlight celeborn.client.push.queue.capacity 512 false Push buffer queue size for a task. The maximum memory is celeborn.client.push.buffer.max.size * celeborn.client.push.queue.capacity , default: 64KiB * 512 = 32MiB 0.3.0 celeborn.push.queue.capacity celeborn.client.push.replicate.enabled false false When true, Celeborn worker will replicate shuffle data to another Celeborn worker asynchronously to ensure the pushed shuffle data won't be lost after the node failure. It's recommended to set false when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.push.replicate.enabled celeborn.client.push.retry.threads 8 false Thread number to process shuffle re-send push data requests. 0.3.0 celeborn.push.retry.threads celeborn.client.push.revive.batchSize 2048 false Max number of partitions in one Revive request. 0.3.0 celeborn.client.push.revive.interval 100ms false Interval for client to trigger Revive to LifecycleManager. The number of partitions in one Revive request is celeborn.client.push.revive.batchSize . 0.3.0 celeborn.client.push.revive.maxRetries 5 false Max retry times for reviving when celeborn push data failed. 0.3.0 celeborn.client.push.sendBufferPool.checkExpireInterval 30s false Interval to check expire for send buffer pool. If the pool has been idle for more than celeborn.client.push.sendBufferPool.expireTimeout , the pooled send buffers and push tasks will be cleaned up. 0.3.1 celeborn.client.push.sendBufferPool.expireTimeout 60s false Timeout before clean up SendBufferPool. If SendBufferPool is idle for more than this time, the send buffers and push tasks will be cleaned up. 0.3.1 celeborn.client.push.slowStart.initialSleepTime 500ms false The initial sleep time if the current max in flight requests is 0 0.3.0 celeborn.client.push.slowStart.maxSleepTime 2s false If celeborn.client.push.limit.strategy is set to SLOWSTART, push side will take a sleep strategy for each batch of requests, this controls the max sleep time if the max in flight requests limit is 1 for a long time 0.3.0 celeborn.client.push.sort.randomizePartitionId.enabled false false Whether to randomize partitionId in push sorter. If true, partitionId will be randomized when sort data to avoid skew when push to worker 0.3.0 celeborn.push.sort.randomizePartitionId.enabled celeborn.client.push.stageEnd.timeout <value of celeborn.<module>.io.connectionTimeout> false Timeout for waiting StageEnd. During this process, there are celeborn.client.requestCommitFiles.maxRetries times for retry opportunities for committing files and 1 times for releasing slots request. User can customize this value according to your setting. By default, the value is the max timeout value celeborn.<module>.io.connectionTimeout . 0.3.0 celeborn.push.stageEnd.timeout celeborn.client.push.takeTaskMaxWaitAttempts 1 false Max wait times if no task available to push to worker. 0.3.0 celeborn.client.push.takeTaskWaitInterval 50ms false Wait interval if no task available to push to worker. 0.3.0 celeborn.client.push.timeout 120s false Timeout for a task to push data rpc message. This value should better be more than twice of celeborn.<module>.push.timeoutCheck.interval 0.3.0 celeborn.push.data.timeout celeborn.client.readLocalShuffleFile.enabled false false Enable read local shuffle file for clusters that co-deployed with yarn node manager. 0.3.1 celeborn.client.readLocalShuffleFile.threads 4 false Threads count for read local shuffle file. 0.3.1 celeborn.client.registerShuffle.maxRetries 3 false Max retry times for client to register shuffle. 0.3.0 celeborn.shuffle.register.maxRetries celeborn.client.registerShuffle.retryWait 3s false Wait time before next retry if register shuffle failed. 0.3.0 celeborn.shuffle.register.retryWait celeborn.client.requestCommitFiles.maxRetries 4 false Max retry times for requestCommitFiles RPC. 0.3.0 celeborn.client.reserveSlots.maxRetries 3 false Max retry times for client to reserve slots. 0.3.0 celeborn.slots.reserve.maxRetries celeborn.client.reserveSlots.rackaware.enabled false false Whether need to place different replicates on different racks when allocating slots. 0.3.1 celeborn.client.reserveSlots.rackware.enabled celeborn.client.reserveSlots.retryWait 3s false Wait time before next retry if reserve slots failed. 0.3.0 celeborn.slots.reserve.retryWait celeborn.client.rpc.cache.concurrencyLevel 32 false The number of write locks to update rpc cache. 0.3.0 celeborn.rpc.cache.concurrencyLevel celeborn.client.rpc.cache.expireTime 15s false The time before a cache item is removed. 0.3.0 celeborn.rpc.cache.expireTime celeborn.client.rpc.cache.size 256 false The max cache items count for rpc cache. 0.3.0 celeborn.rpc.cache.size celeborn.client.rpc.commitFiles.askTimeout <value of celeborn.rpc.askTimeout> false Timeout for CommitHandler commit files. 0.4.1 celeborn.client.rpc.getReducerFileGroup.askTimeout <value of celeborn.rpc.askTimeout> false Timeout for ask operations during getting reducer file group information. During this process, there are celeborn.client.requestCommitFiles.maxRetries times for retry opportunities for committing files and 1 times for releasing slots request. User can customize this value according to your setting. 0.2.0 celeborn.client.rpc.maxRetries 3 false Max RPC retry times in LifecycleManager. 0.3.2 celeborn.client.rpc.registerShuffle.askTimeout <value of celeborn.rpc.askTimeout> false Timeout for ask operations during register shuffle. During this process, there are two times for retry opportunities for requesting slots, one request for establishing a connection with Worker and celeborn.client.reserveSlots.maxRetries times for retry opportunities for reserving slots. User can customize this value according to your setting. 0.3.0 celeborn.rpc.registerShuffle.askTimeout celeborn.client.rpc.requestPartition.askTimeout <value of celeborn.rpc.askTimeout> false Timeout for ask operations during requesting change partition location, such as reviving or splitting partition. During this process, there are celeborn.client.reserveSlots.maxRetries times for retry opportunities for reserving slots. User can customize this value according to your setting. 0.2.0 celeborn.client.rpc.reserveSlots.askTimeout <value of celeborn.rpc.askTimeout> false Timeout for LifecycleManager request reserve slots. 0.3.0 celeborn.client.rpc.shared.threads 16 false Number of shared rpc threads in LifecycleManager. 0.3.2 celeborn.client.shuffle.batchHandleChangePartition.interval 100ms false Interval for LifecycleManager to schedule handling change partition requests in batch. 0.3.0 celeborn.shuffle.batchHandleChangePartition.interval celeborn.client.shuffle.batchHandleChangePartition.partitionBuckets 256 false Max number of change partition requests which can be concurrently processed 0.5.0 celeborn.client.shuffle.batchHandleChangePartition.threads 8 false Threads number for LifecycleManager to handle change partition request in batch. 0.3.0 celeborn.shuffle.batchHandleChangePartition.threads celeborn.client.shuffle.batchHandleCommitPartition.interval 5s false Interval for LifecycleManager to schedule handling commit partition requests in batch. 0.3.0 celeborn.shuffle.batchHandleCommitPartition.interval celeborn.client.shuffle.batchHandleCommitPartition.threads 8 false Threads number for LifecycleManager to handle commit partition request in batch. 0.3.0 celeborn.shuffle.batchHandleCommitPartition.threads celeborn.client.shuffle.batchHandleReleasePartition.interval 5s false Interval for LifecycleManager to schedule handling release partition requests in batch. 0.3.0 celeborn.client.shuffle.batchHandleReleasePartition.threads 8 false Threads number for LifecycleManager to handle release partition request in batch. 0.3.0 celeborn.client.shuffle.compression.codec LZ4 false The codec used to compress shuffle data. By default, Celeborn provides three codecs: lz4 , zstd , none . none means that shuffle compression is disabled. Since Flink version 1.17, zstd is supported for Flink shuffle client. 0.3.0 celeborn.shuffle.compression.codec,remote-shuffle.job.compression.codec celeborn.client.shuffle.compression.zstd.level 1 false Compression level for Zstd compression codec, its value should be an integer between -5 and 22. Increasing the compression level will result in better compression at the expense of more CPU and memory. 0.3.0 celeborn.shuffle.compression.zstd.level celeborn.client.shuffle.decompression.lz4.xxhash.instance <undefined> false Decompression XXHash instance for Lz4. Available options: JNI, JAVASAFE, JAVAUNSAFE. 0.3.2 celeborn.client.shuffle.expired.checkInterval 60s false Interval for client to check expired shuffles. 0.3.0 celeborn.shuffle.expired.checkInterval celeborn.client.shuffle.manager.port 0 false Port used by the LifecycleManager on the Driver. 0.3.0 celeborn.shuffle.manager.port celeborn.client.shuffle.mapPartition.split.enabled false false whether to enable shuffle partition split. Currently, this only applies to MapPartition. 0.3.1 celeborn.client.shuffle.partition.type REDUCE false Type of shuffle's partition. 0.3.0 celeborn.shuffle.partition.type celeborn.client.shuffle.partitionSplit.mode SOFT false soft: the shuffle file size might be larger than split threshold. hard: the shuffle file size will be limited to split threshold. 0.3.0 celeborn.shuffle.partitionSplit.mode celeborn.client.shuffle.partitionSplit.threshold 1G false Shuffle file size threshold, if file size exceeds this, trigger split. 0.3.0 celeborn.shuffle.partitionSplit.threshold celeborn.client.shuffle.rangeReadFilter.enabled false false If a spark application have skewed partition, this value can set to true to improve performance. 0.2.0 celeborn.shuffle.rangeReadFilter.enabled celeborn.client.shuffle.register.filterExcludedWorker.enabled false false Whether to filter excluded worker when register shuffle. 0.4.0 celeborn.client.slot.assign.maxWorkers 10000 false Max workers that slots of one shuffle can be allocated on. Will choose the smaller positive one from Master side and Client side, see celeborn.master.slot.assign.maxWorkers . 0.3.1 celeborn.client.spark.fetch.throwsFetchFailure false false client throws FetchFailedException instead of CelebornIOException 0.4.0 celeborn.client.spark.push.dynamicWriteMode.enabled false false Whether to dynamically switch push write mode based on conditions.If true, shuffle mode will be only determined by partition count 0.5.0 celeborn.client.spark.push.dynamicWriteMode.partitionNum.threshold 2000 false Threshold of shuffle partition number for dynamically switching push writer mode. When the shuffle partition number is greater than this value, use the sort-based shuffle writer for memory efficiency; otherwise use the hash-based shuffle writer for speed. This configuration only takes effect when celeborn.client.spark.push.dynamicWriteMode.enabled is true. 0.5.0 celeborn.client.spark.push.sort.memory.maxMemoryFactor 0.4 false the max portion of executor memory which can be used for SortBasedWriter buffer (only valid when celeborn.client.spark.push.sort.memory.useAdaptiveThreshold is enabled 0.5.0 celeborn.client.spark.push.sort.memory.smallPushTolerateFactor 0.2 false Only be in effect when celeborn.client.spark.push.sort.memory.useAdaptiveThreshold is turned on. The larger this value is, the more aggressive Celeborn will enlarge the Sort-based Shuffle writer's memory threshold. Specifically, this config controls when to enlarge the sort shuffle writer's memory threshold. With N bytes data in memory and V as the value of this config, if the number of pushes, C, when using sort based shuffle writer C >= (1 + V) * C' where C' is the number of pushes if we were using hash based writer, we will enlarge the memory threshold by 2X. 0.5.0 celeborn.client.spark.push.sort.memory.threshold 64m false When SortBasedPusher use memory over the threshold, will trigger push data. 0.3.0 celeborn.push.sortMemory.threshold celeborn.client.spark.push.sort.memory.useAdaptiveThreshold false false Adaptively adjust sort-based shuffle writer's memory threshold 0.5.0 celeborn.client.spark.push.unsafeRow.fastWrite.enabled true false This is Celeborn's optimization on UnsafeRow for Spark and it's true by default. If you have changed UnsafeRow's memory layout set this to false. 0.2.2 celeborn.client.spark.shuffle.checkWorker.enabled true false When true, before registering shuffle, LifecycleManager should check if current cluster have available workers, if cluster don't have available workers, fallback to Spark's default shuffle 0.5.0 celeborn.client.spark.shuffle.fallback.numPartitionsThreshold 2147483647 false Celeborn will only accept shuffle of partition number lower than this configuration value. This configuration only takes effect when celeborn.client.spark.shuffle.fallback.policy is AUTO . 0.5.0 celeborn.shuffle.forceFallback.numPartitionsThreshold,celeborn.client.spark.shuffle.forceFallback.numPartitionsThreshold celeborn.client.spark.shuffle.fallback.policy AUTO false Celeborn supports the following kind of fallback policies. 1. ALWAYS: always use spark built-in shuffle implementation; 2. AUTO: prefer to use celeborn shuffle implementation, and fallback to use spark built-in shuffle implementation based on certain factors, e.g. availability of enough workers and quota, shuffle partition number; 3. NEVER: always use celeborn shuffle implementation, and fail fast when it it is concluded that fallback is required based on factors above. 0.5.0 celeborn.client.spark.shuffle.forceFallback.enabled false false Always use spark built-in shuffle implementation. This configuration is deprecated, consider configuring celeborn.client.spark.shuffle.fallback.policy instead. 0.3.0 celeborn.shuffle.forceFallback.enabled celeborn.client.spark.shuffle.writer HASH false Celeborn supports the following kind of shuffle writers. 1. hash: hash-based shuffle writer works fine when shuffle partition count is normal; 2. sort: sort-based shuffle writer works fine when memory pressure is high or shuffle partition count is huge. This configuration only takes effect when celeborn.client.spark.push.dynamicWriteMode.enabled is false. 0.3.0 celeborn.shuffle.writer celeborn.master.endpoints <localhost>:9097 false Endpoints of master nodes for celeborn client to connect, allowed pattern is: <host1>:<port1>[,<host2>:<port2>]* , e.g. clb1:9097,clb2:9098,clb3:9099 . If the port is omitted, 9097 will be used. 0.2.0 celeborn.quota.enabled true false When Master side sets to true, the master will enable to check the quota via QuotaManager. When Client side sets to true, LifecycleManager will request Master side to check whether the current user has enough quota before registration of shuffle. Fallback to the default shuffle service of Spark when Master side checks that there is no enough quota for current user. 0.2.0 celeborn.quota.identity.provider org.apache.celeborn.common.identity.DefaultIdentityProvider false IdentityProvider class name. Default class is org.apache.celeborn.common.identity.DefaultIdentityProvider . Optional values: org.apache.celeborn.common.identity.HadoopBasedIdentityProvider user name will be obtained by UserGroupInformation.getUserName; org.apache.celeborn.common.identity.DefaultIdentityProvider user name and tenant id are default values or user-specific values. 0.2.0 celeborn.quota.identity.user-specific.tenant default false Tenant id if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.quota.identity.user-specific.userName default false User name if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.storage.availableTypes HDD false Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.activeTypes celeborn.storage.hdfs.dir <undefined> false HDFS base directory for Celeborn to store shuffle data. 0.2.0","title":"Client"},{"location":"configuration/#quota","text":"Key Default isDynamic Description Since Deprecated celeborn.quota.enabled true false When Master side sets to true, the master will enable to check the quota via QuotaManager. When Client side sets to true, LifecycleManager will request Master side to check whether the current user has enough quota before registration of shuffle. Fallback to the default shuffle service of Spark when Master side checks that there is no enough quota for current user. 0.2.0 celeborn.quota.identity.provider org.apache.celeborn.common.identity.DefaultIdentityProvider false IdentityProvider class name. Default class is org.apache.celeborn.common.identity.DefaultIdentityProvider . Optional values: org.apache.celeborn.common.identity.HadoopBasedIdentityProvider user name will be obtained by UserGroupInformation.getUserName; org.apache.celeborn.common.identity.DefaultIdentityProvider user name and tenant id are default values or user-specific values. 0.2.0 celeborn.quota.identity.user-specific.tenant default false Tenant id if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.quota.identity.user-specific.userName default false User name if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.quota.tenant.diskBytesWritten 9223372036854775807 true Quota dynamic configuration for written disk bytes. 0.5.0 celeborn.quota.tenant.diskFileCount 9223372036854775807 true Quota dynamic configuration for written disk file count. 0.5.0 celeborn.quota.tenant.hdfsBytesWritten 9223372036854775807 true Quota dynamic configuration for written hdfs bytes. 0.5.0 celeborn.quota.tenant.hdfsFileCount 9223372036854775807 true Quota dynamic configuration for written hdfs file count. 0.5.0","title":"Quota"},{"location":"configuration/#network","text":"The various transport modules which can be configured are: Module Parent Module Description rpc_app rpc Configure control plane RPC environment used by Celeborn within the application. For backward compatibility, supports fallback to rpc parent module for missing configuration. Note, this is for RPC environment - see below for other transport modules rpc_service rpc Configure control plane RPC environment when communicating with Celeborn service hosts. This includes all RPC communication from application to Celeborn Master/Workers, as well as between Celeborn masters/workers themselves. For backward compatibility, supports fallback to rpc parent module for missing configuration. As with rpc_app , this is only for RPC environment see below for other transport modules. rpc - Fallback parent transport module for rpc_app and rpc_service . It is advisible to use the specific transport modules while configuring - rpc exists primarily for backward compatibility push - Configure transport module for handling data push at Celeborn workers fetch - Configure transport module for handling data fetch at Celeborn workers data - Configure transport module for handling data push and fetch at Celeborn apps replicate - Configure transport module for handling data replication between Celeborn workers Some network configurations might apply in specific scenarios, for example push module for io.maxRetries and io.retryWait in flink client. Please see the full list below for details. Key Default isDynamic Description Since Deprecated celeborn.<module>.fetch.timeoutCheck.interval 5s false Interval for checking fetch data timeout. It only support setting to data since it works for shuffle client fetch data. 0.3.0 celeborn.<module>.fetch.timeoutCheck.threads 4 false Threads num for checking fetch data timeout. It only support setting to data since it works for shuffle client fetch data. 0.3.0 celeborn.<module>.heartbeat.interval 60s false The heartbeat interval between worker and client. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to replicate , it works for replicate client of worker replicating data to peer worker.If you are using the \"celeborn.client.heartbeat.interval\", please use the new configs for each module according to your needs or replace it with \"celeborn.rpc.heartbeat.interval\", \"celeborn.data.heartbeat.interval\" and\"celeborn.replicate.heartbeat.interval\". 0.3.0 celeborn.client.heartbeat.interval celeborn.<module>.io.backLog 0 false Requested maximum length of the queue of incoming connections. Default 0 for no backlog. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. celeborn.<module>.io.clientThreads 0 false Number of threads used in the client thread pool. Default to 0, which is 2x#cores. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to replicate , it works for replicate client of worker replicating data to peer worker. celeborn.<module>.io.connectTimeout <value of celeborn.network.connect.timeout> false Socket connect timeout. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to replicate , it works for the replicate client of worker replicating data to peer worker. celeborn.<module>.io.connectionTimeout <value of celeborn.network.timeout> false Connection active timeout. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server or client of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. celeborn.<module>.io.enableVerboseMetrics false false Whether to track Netty memory detailed metrics. If true, the detailed metrics of Netty PoolByteBufAllocator will be gotten, otherwise only general memory usage will be tracked. celeborn.<module>.io.lazyFD true false Whether to initialize FileDescriptor lazily or not. If true, file descriptors are created only when data is going to be transferred. This can reduce the number of open files. If setting to fetch , it works for worker fetch server. celeborn.<module>.io.maxRetries 3 false Max number of times we will try IO exceptions (such as connection timeouts) per request. If set to 0, we will not do any retries. If setting to push , it works for Flink shuffle client push data. celeborn.<module>.io.mode NIO false Netty EventLoopGroup backend, available options: NIO, EPOLL. celeborn.<module>.io.numConnectionsPerPeer 1 false Number of concurrent connections between two nodes. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to replicate , it works for replicate client of worker replicating data to peer worker. celeborn.<module>.io.preferDirectBufs true false If true, we will prefer allocating off-heap byte buffers within Netty. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server or client of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. celeborn.<module>.io.receiveBuffer 0b false Receive buffer size (SO_RCVBUF). Note: the optimal size for receive buffer and send buffer should be latency * network_bandwidth. Assuming latency = 1ms, network_bandwidth = 10Gbps buffer size should be ~ 1.25MB. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server or client of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. 0.2.0 celeborn.<module>.io.retryWait 5s false Time that we will wait in order to perform a retry after an IOException. Only relevant if maxIORetries > 0. If setting to data , it works for shuffle client push and fetch data. If setting to push , it works for Flink shuffle client push data. 0.2.0 celeborn.<module>.io.saslTimeout 30s false Timeout for a single round trip of auth message exchange, in milliseconds. 0.5.0 celeborn.<module>.io.sendBuffer 0b false Send buffer size (SO_SNDBUF). If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server or client of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. 0.2.0 celeborn.<module>.io.serverThreads 0 false Number of threads used in the server thread pool. Default to 0, which is 2x#cores. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. celeborn.<module>.push.timeoutCheck.interval 5s false Interval for checking push data timeout. If setting to data , it works for shuffle client push data. If setting to push , it works for Flink shuffle client push data. If setting to replicate , it works for replicate client of worker replicating data to peer worker. 0.3.0 celeborn.<module>.push.timeoutCheck.threads 4 false Threads num for checking push data timeout. If setting to data , it works for shuffle client push data. If setting to push , it works for Flink shuffle client push data. If setting to replicate , it works for replicate client of worker replicating data to peer worker. 0.3.0 celeborn.<role>.rpc.dispatcher.threads <value of celeborn.rpc.dispatcher.threads> false Threads number of message dispatcher event loop for roles celeborn.io.maxDefaultNettyThreads 64 false Max default netty threads 0.3.2 celeborn.network.bind.preferIpAddress true false When true , prefer to use IP address, otherwise FQDN. This configuration only takes effects when the bind hostname is not set explicitly, in such case, Celeborn will find the first non-loopback address to bind. 0.3.0 celeborn.network.connect.timeout 10s false Default socket connect timeout. 0.2.0 celeborn.network.memory.allocator.numArenas <undefined> false Number of arenas for pooled memory allocator. Default value is Runtime.getRuntime.availableProcessors, min value is 2. 0.3.0 celeborn.network.memory.allocator.verbose.metric false false Whether to enable verbose metric for pooled allocator. 0.3.0 celeborn.network.timeout 240s false Default timeout for network operations. 0.2.0 celeborn.port.maxRetries 1 false When port is occupied, we will retry for max retry times. 0.2.0 celeborn.rpc.askTimeout 60s false Timeout for RPC ask operations. It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes 0.2.0 celeborn.rpc.connect.threads 64 false 0.2.0 celeborn.rpc.dispatcher.threads 0 false Threads number of message dispatcher event loop. Default to 0, which is availableCore. 0.3.0 celeborn.rpc.dispatcher.numThreads celeborn.rpc.inbox.capacity 0 false Specifies size of the in memory bounded capacity. 0.5.0 celeborn.rpc.io.threads <undefined> false Netty IO thread number of NettyRpcEnv to handle RPC request. The default threads number is the number of runtime available processors. 0.2.0 celeborn.rpc.lookupTimeout 30s false Timeout for RPC lookup operations. 0.2.0 celeborn.shuffle.io.maxChunksBeingTransferred <undefined> false The max number of chunks allowed to be transferred at the same time on shuffle service. Note that new incoming connections will be closed when the max number is hit. The client will retry according to the shuffle retry configs (see celeborn.<module>.io.maxRetries and celeborn.<module>.io.retryWait ), if those limits are reached the task will fail with fetch failure. 0.2.0 celeborn.ssl.<module>.enabled false false Enables SSL for securing wire traffic. 0.5.0 celeborn.ssl.<module>.enabledAlgorithms <undefined> false A comma-separated list of ciphers. The specified ciphers must be supported by JVM. The reference list of protocols can be found in the \"JSSE Cipher Suite Names\" section of the Java security guide. The list for Java 11, for example, can be found at this page Note: If not set, the default cipher suite for the JRE will be used 0.5.0 celeborn.ssl.<module>.keyStore <undefined> false Path to the key store file. The path can be absolute or relative to the directory in which the process is started. 0.5.0 celeborn.ssl.<module>.keyStorePassword <undefined> false Password to the key store. 0.5.0 celeborn.ssl.<module>.protocol TLSv1.2 false TLS protocol to use. The protocol must be supported by JVM. The reference list of protocols can be found in the \"Additional JSSE Standard Names\" section of the Java security guide. For Java 11, for example, the list can be found here 0.5.0 celeborn.ssl.<module>.trustStore <undefined> false Path to the trust store file. The path can be absolute or relative to the directory in which the process is started. 0.5.0 celeborn.ssl.<module>.trustStorePassword <undefined> false Password for the trust store. 0.5.0 celeborn.ssl.<module>.trustStoreReloadIntervalMs 10s false The interval at which the trust store should be reloaded (in milliseconds), when enabled. This setting is mostly only useful for server components, not applications. 0.5.0 celeborn.ssl.<module>.trustStoreReloadingEnabled false false Whether the trust store should be reloaded periodically. This setting is mostly only useful for Celeborn services (masters, workers), and not applications. 0.5.0","title":"Network"},{"location":"configuration/#columnar-shuffle","text":"Key Default isDynamic Description Since Deprecated celeborn.columnarShuffle.batch.size 10000 false Vector batch size for columnar shuffle. 0.3.0 celeborn.columnar.shuffle.batch.size celeborn.columnarShuffle.codegen.enabled false false Whether to use codegen for columnar-based shuffle. 0.3.0 celeborn.columnar.shuffle.codegen.enabled celeborn.columnarShuffle.enabled false false Whether to enable columnar-based shuffle. 0.2.0 celeborn.columnar.shuffle.enabled celeborn.columnarShuffle.encoding.dictionary.enabled false false Whether to use dictionary encoding for columnar-based shuffle data. 0.3.0 celeborn.columnar.shuffle.encoding.dictionary.enabled celeborn.columnarShuffle.encoding.dictionary.maxFactor 0.3 false Max factor for dictionary size. The max dictionary size is min(32.0 KiB, celeborn.columnarShuffle.batch.size * celeborn.columnar.shuffle.encoding.dictionary.maxFactor) . 0.3.0 celeborn.columnar.shuffle.encoding.dictionary.maxFactor celeborn.columnarShuffle.offHeap.enabled false false Whether to use off heap columnar vector. 0.3.0 celeborn.columnar.offHeap.enabled","title":"Columnar Shuffle"},{"location":"configuration/#metrics","text":"Below metrics configuration both work for master and worker. Key Default isDynamic Description Since Deprecated celeborn.metrics.app.topDiskUsage.count 50 false Size for top items about top disk usage applications list. 0.2.0 celeborn.metrics.app.topDiskUsage.interval 10min false Time length for a window about top disk usage application list. 0.2.0 celeborn.metrics.app.topDiskUsage.windowSize 24 false Window size about top disk usage application list. 0.2.0 celeborn.metrics.capacity 4096 false The maximum number of metrics which a source can use to generate output strings. 0.2.0 celeborn.metrics.collectPerfCritical.enabled false false It controls whether to collect metrics which may affect performance. When enable, Celeborn collects them. 0.2.0 celeborn.metrics.conf <undefined> false Custom metrics configuration file path. Default use metrics.properties in classpath. 0.3.0 celeborn.metrics.enabled true false When true, enable metrics system. 0.2.0 celeborn.metrics.extraLabels false If default metric labels are not enough, extra metric labels can be customized. Labels' pattern is: <label1_key>=<label1_value>[,<label2_key>=<label2_value>]* ; e.g. env=prod,version=1 0.3.0 celeborn.metrics.json.path /metrics/json false URI context path of json metrics HTTP server. 0.4.0 celeborn.metrics.json.pretty.enabled true false When true, view metrics in json pretty format 0.4.0 celeborn.metrics.prometheus.path /metrics/prometheus false URI context path of prometheus metrics HTTP server. 0.4.0 celeborn.metrics.sample.rate 1.0 false It controls if Celeborn collect timer metrics for some operations. Its value should be in [0.0, 1.0]. 0.2.0 celeborn.metrics.timer.slidingWindow.size 4096 false The sliding window size of timer metric. 0.2.0 celeborn.metrics.worker.pauseSpentTime.forceAppend.threshold 10 false Force append worker pause spent time even if worker still in pause serving state.Help user can find worker pause spent time increase, when worker always been pause state.","title":"Metrics"},{"location":"configuration/#metricsproperties","text":"*.sink.csv.class = org.apache.celeborn.common.metrics.sink.CsvSink *.sink.prometheusServlet.class = org.apache.celeborn.common.metrics.sink.PrometheusServlet","title":"metrics.properties"},{"location":"configuration/#environment-variables_1","text":"Recommend configuring in conf/celeborn-env.sh . Key Default Description CELEBORN_HOME $(cd \"`dirname \"$0\"`\"/..; pwd) CELEBORN_CONF_DIR ${CELEBORN_CONF_DIR:-\"${CELEBORN_HOME}/conf\"} CELEBORN_MASTER_MEMORY 1 GB CELEBORN_WORKER_MEMORY 1 GB CELEBORN_WORKER_OFFHEAP_MEMORY 1 GB CELEBORN_MASTER_JAVA_OPTS CELEBORN_WORKER_JAVA_OPTS CELEBORN_PID_DIR ${CELEBORN_HOME}/pids CELEBORN_LOG_DIR ${CELEBORN_HOME}/logs CELEBORN_SSH_OPTS -o StrictHostKeyChecking=no CELEBORN_SLEEP Waiting time for start-all and stop-all operations CELEBORN_PREFER_JEMALLOC set true to enable jemalloc memory allocator CELEBORN_JEMALLOC_PATH jemalloc library path","title":"Environment Variables"},{"location":"configuration/#tuning","text":"Assume we have a cluster described as below: 5 Celeborn Workers with 20 GB off-heap memory and 10 disks. As we need to reserve 20% off-heap memory for netty, so we could assume 16 GB off-heap memory can be used for flush buffers. If spark.celeborn.client.push.buffer.max.size is 64 KB, we can have in-flight requests up to 1310720. If you have 8192 mapper tasks, you could set spark.celeborn.client.push.maxReqsInFlight=160 to gain performance improvements. If celeborn.worker.flusher.buffer.size is 256 KB, we can have total slots up to 327680 slots.","title":"Tuning"},{"location":"configuration/#rack-awareness","text":"Celeborn can be rack-aware by setting celeborn.client.reserveSlots.rackware.enabled to true on client side. Shuffle partition block replica placement will use rack awareness for fault tolerance by placing one shuffle partition replica on a different rack. This provides data availability in the event of a network switch failure or partition within the cluster. Celeborn master daemons obtain the rack id of the cluster workers by invoking either an external script or Java class as specified by configuration files. Using either the Java class or external script for topology, output must adhere to the java org.apache.hadoop.net.DNSToSwitchMapping interface. The interface expects a one-to-one correspondence to be maintained and the topology information in the format of /myrack/myhost , where / is the topology delimiter, myrack is the rack identifier, and myhost is the individual host. Assuming a single /24 subnet per rack, one could use the format of /192.168.100.0/192.168.100.5 as a unique rack-host topology mapping. To use the Java class for topology mapping, the class name is specified by the celeborn.hadoop.net.topology.node.switch.mapping.impl parameter in the master configuration file. An example, NetworkTopology.java , is included with the Celeborn distribution and can be customized by the Celeborn administrator. Using a Java class instead of an external script has a performance benefit in that Celeborn doesn't need to fork an external process when a new worker node registers itself. If implementing an external script, it will be specified with the celeborn.hadoop.net.topology.script.file.name parameter in the master side configuration files. Unlike the Java class, the external topology script is not included with the Celeborn distribution and is provided by the administrator. Celeborn will send multiple IP addresses to ARGV when forking the topology script. The number of IP addresses sent to the topology script is controlled with celeborn.hadoop.net.topology.script.number.args and defaults to 100. If celeborn.hadoop.net.topology.script.number.args was changed to 1, a topology script would get forked for each IP submitted by workers. If celeborn.hadoop.net.topology.script.file.name or celeborn.hadoop.net.topology.node.switch.mapping.impl is not set, the rack id /default-rack is returned for any passed IP address. While this behavior appears desirable, it can cause issues with shuffle partition block replication as default behavior is to write one replicated block off rack and is unable to do so as there is only a single rack named /default-rack . Example can refer to Hadoop Rack Awareness since Celeborn use hadoop's code about rack-aware.","title":"Rack Awareness"},{"location":"configuration/#worker-recover-status-after-restart","text":"ShuffleClient records the shuffle partition location's host, service port, and filename, to support workers recovering reading existing shuffle data after worker restart, during worker shutdown, workers should store the meta about reading shuffle partition files in RocksDB or LevelDB(deprecated), and restore the meta after restarting workers, also workers should keep a stable service port to support ShuffleClient retry reading data. Users should set celeborn.worker.graceful.shutdown.enabled to true and set below service port with stable port to support worker recover status. celeborn.worker.rpc.port celeborn.worker.fetch.port celeborn.worker.push.port celeborn.worker.replicate.port","title":"Worker Recover Status After Restart"},{"location":"configuration/client/","text":"Key Default isDynamic Description Since Deprecated celeborn.client.application.heartbeatInterval 10s false Interval for client to send heartbeat message to master. 0.3.0 celeborn.application.heartbeatInterval celeborn.client.application.unregister.enabled true false When true, Celeborn client will inform celeborn master the application is already shutdown during client exit, this allows the cluster to release resources immediately, resulting in resource savings. 0.3.2 celeborn.client.chunk.prefetch.enabled false false Whether to enable chunk prefetch when creating CelebornInputStream. 0.6.0 celeborn.client.closeIdleConnections true false Whether client will close idle connections. 0.3.0 celeborn.client.commitFiles.ignoreExcludedWorker false false When true, LifecycleManager will skip workers which are in the excluded list. 0.3.0 celeborn.client.eagerlyCreateInputStream.threads 32 false Threads count for streamCreatorPool in CelebornShuffleReader. 0.3.1 celeborn.client.excludePeerWorkerOnFailure.enabled true false When true, Celeborn will exclude partition's peer worker on failure when push data to replica failed. 0.3.0 celeborn.client.excludedWorker.expireTimeout 180s false Timeout time for LifecycleManager to clear reserved excluded worker. Default to be 1.5 * celeborn.master.heartbeat.worker.timeout to cover worker heartbeat timeout check period 0.3.0 celeborn.worker.excluded.expireTimeout celeborn.client.fetch.buffer.size 64k false Size of reducer partition buffer memory for shuffle reader. The fetched data will be buffered in memory before consuming. For performance consideration keep this buffer size not less than celeborn.client.push.buffer.max.size . 0.4.0 celeborn.client.fetch.dfsReadChunkSize 8m false Max chunk size for DfsPartitionReader. 0.3.1 celeborn.client.fetch.excludeWorkerOnFailure.enabled false false Whether to enable shuffle client-side fetch exclude workers on failure. 0.3.0 celeborn.client.fetch.excludedWorker.expireTimeout <value of celeborn.client.excludedWorker.expireTimeout> false ShuffleClient is a static object, it will be used in the whole lifecycle of Executor,We give a expire time for excluded workers to avoid a transient worker issues. 0.3.0 celeborn.client.fetch.maxReqsInFlight 3 false Amount of in-flight chunk fetch request. 0.3.0 celeborn.fetch.maxReqsInFlight celeborn.client.fetch.maxRetriesForEachReplica 3 false Max retry times of fetch chunk on each replica 0.3.0 celeborn.fetch.maxRetriesForEachReplica,celeborn.fetch.maxRetries celeborn.client.fetch.timeout 600s false Timeout for a task to open stream and fetch chunk. 0.3.0 celeborn.fetch.timeout celeborn.client.flink.compression.enabled true false Whether to compress data in Flink plugin. 0.3.0 remote-shuffle.job.enable-data-compression celeborn.client.flink.inputGate.concurrentReadings 2147483647 false Max concurrent reading channels for a input gate. 0.3.0 remote-shuffle.job.concurrent-readings-per-gate celeborn.client.flink.inputGate.memory 32m false Memory reserved for a input gate. 0.3.0 remote-shuffle.job.memory-per-gate celeborn.client.flink.inputGate.supportFloatingBuffer true false Whether to support floating buffer in Flink input gates. 0.3.0 remote-shuffle.job.support-floating-buffer-per-input-gate celeborn.client.flink.resultPartition.memory 64m false Memory reserved for a result partition. 0.3.0 remote-shuffle.job.memory-per-partition celeborn.client.flink.resultPartition.supportFloatingBuffer true false Whether to support floating buffer for result partitions. 0.3.0 remote-shuffle.job.support-floating-buffer-per-output-gate celeborn.client.inputStream.creation.window 16 false Window size that CelebornShuffleReader pre-creates CelebornInputStreams, for coalesced scenariowhere multiple Partitions are read 0.6.0 celeborn.client.mr.pushData.max 32m false Max size for a push data sent from mr client. 0.4.0 celeborn.client.push.buffer.initial.size 8k false 0.3.0 celeborn.push.buffer.initial.size celeborn.client.push.buffer.max.size 64k false Max size of reducer partition buffer memory for shuffle hash writer. The pushed data will be buffered in memory before sending to Celeborn worker. For performance consideration keep this buffer size higher than 32K. Example: If reducer amount is 2000, buffer size is 64K, then each task will consume up to 64KiB * 2000 = 125MiB heap memory. 0.3.0 celeborn.push.buffer.max.size celeborn.client.push.excludeWorkerOnFailure.enabled false false Whether to enable shuffle client-side push exclude workers on failures. 0.3.0 celeborn.client.push.limit.inFlight.sleepInterval 50ms false Sleep interval when check netty in-flight requests to be done. 0.3.0 celeborn.push.limit.inFlight.sleepInterval celeborn.client.push.limit.inFlight.timeout <undefined> false Timeout for netty in-flight requests to be done.Default value should be celeborn.client.push.timeout * 2 . 0.3.0 celeborn.push.limit.inFlight.timeout celeborn.client.push.limit.strategy SIMPLE false The strategy used to control the push speed. Valid strategies are SIMPLE and SLOWSTART. The SLOWSTART strategy usually works with congestion control mechanism on the worker side. 0.3.0 celeborn.client.push.maxReqsInFlight.perWorker 32 false Amount of Netty in-flight requests per worker. Default max memory of in flight requests per worker is celeborn.client.push.maxReqsInFlight.perWorker * celeborn.client.push.buffer.max.size * compression ratio(1 in worst case): 64KiB * 32 = 2MiB. The maximum memory will not exceed celeborn.client.push.maxReqsInFlight.total . 0.3.0 celeborn.client.push.maxReqsInFlight.total 256 false Amount of total Netty in-flight requests. The maximum memory is celeborn.client.push.maxReqsInFlight.total * celeborn.client.push.buffer.max.size * compression ratio(1 in worst case): 64KiB * 256 = 16MiB 0.3.0 celeborn.push.maxReqsInFlight celeborn.client.push.queue.capacity 512 false Push buffer queue size for a task. The maximum memory is celeborn.client.push.buffer.max.size * celeborn.client.push.queue.capacity , default: 64KiB * 512 = 32MiB 0.3.0 celeborn.push.queue.capacity celeborn.client.push.replicate.enabled false false When true, Celeborn worker will replicate shuffle data to another Celeborn worker asynchronously to ensure the pushed shuffle data won't be lost after the node failure. It's recommended to set false when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.push.replicate.enabled celeborn.client.push.retry.threads 8 false Thread number to process shuffle re-send push data requests. 0.3.0 celeborn.push.retry.threads celeborn.client.push.revive.batchSize 2048 false Max number of partitions in one Revive request. 0.3.0 celeborn.client.push.revive.interval 100ms false Interval for client to trigger Revive to LifecycleManager. The number of partitions in one Revive request is celeborn.client.push.revive.batchSize . 0.3.0 celeborn.client.push.revive.maxRetries 5 false Max retry times for reviving when celeborn push data failed. 0.3.0 celeborn.client.push.sendBufferPool.checkExpireInterval 30s false Interval to check expire for send buffer pool. If the pool has been idle for more than celeborn.client.push.sendBufferPool.expireTimeout , the pooled send buffers and push tasks will be cleaned up. 0.3.1 celeborn.client.push.sendBufferPool.expireTimeout 60s false Timeout before clean up SendBufferPool. If SendBufferPool is idle for more than this time, the send buffers and push tasks will be cleaned up. 0.3.1 celeborn.client.push.slowStart.initialSleepTime 500ms false The initial sleep time if the current max in flight requests is 0 0.3.0 celeborn.client.push.slowStart.maxSleepTime 2s false If celeborn.client.push.limit.strategy is set to SLOWSTART, push side will take a sleep strategy for each batch of requests, this controls the max sleep time if the max in flight requests limit is 1 for a long time 0.3.0 celeborn.client.push.sort.randomizePartitionId.enabled false false Whether to randomize partitionId in push sorter. If true, partitionId will be randomized when sort data to avoid skew when push to worker 0.3.0 celeborn.push.sort.randomizePartitionId.enabled celeborn.client.push.stageEnd.timeout <value of celeborn.<module>.io.connectionTimeout> false Timeout for waiting StageEnd. During this process, there are celeborn.client.requestCommitFiles.maxRetries times for retry opportunities for committing files and 1 times for releasing slots request. User can customize this value according to your setting. By default, the value is the max timeout value celeborn.<module>.io.connectionTimeout . 0.3.0 celeborn.push.stageEnd.timeout celeborn.client.push.takeTaskMaxWaitAttempts 1 false Max wait times if no task available to push to worker. 0.3.0 celeborn.client.push.takeTaskWaitInterval 50ms false Wait interval if no task available to push to worker. 0.3.0 celeborn.client.push.timeout 120s false Timeout for a task to push data rpc message. This value should better be more than twice of celeborn.<module>.push.timeoutCheck.interval 0.3.0 celeborn.push.data.timeout celeborn.client.readLocalShuffleFile.enabled false false Enable read local shuffle file for clusters that co-deployed with yarn node manager. 0.3.1 celeborn.client.readLocalShuffleFile.threads 4 false Threads count for read local shuffle file. 0.3.1 celeborn.client.registerShuffle.maxRetries 3 false Max retry times for client to register shuffle. 0.3.0 celeborn.shuffle.register.maxRetries celeborn.client.registerShuffle.retryWait 3s false Wait time before next retry if register shuffle failed. 0.3.0 celeborn.shuffle.register.retryWait celeborn.client.requestCommitFiles.maxRetries 4 false Max retry times for requestCommitFiles RPC. 0.3.0 celeborn.client.reserveSlots.maxRetries 3 false Max retry times for client to reserve slots. 0.3.0 celeborn.slots.reserve.maxRetries celeborn.client.reserveSlots.rackaware.enabled false false Whether need to place different replicates on different racks when allocating slots. 0.3.1 celeborn.client.reserveSlots.rackware.enabled celeborn.client.reserveSlots.retryWait 3s false Wait time before next retry if reserve slots failed. 0.3.0 celeborn.slots.reserve.retryWait celeborn.client.rpc.cache.concurrencyLevel 32 false The number of write locks to update rpc cache. 0.3.0 celeborn.rpc.cache.concurrencyLevel celeborn.client.rpc.cache.expireTime 15s false The time before a cache item is removed. 0.3.0 celeborn.rpc.cache.expireTime celeborn.client.rpc.cache.size 256 false The max cache items count for rpc cache. 0.3.0 celeborn.rpc.cache.size celeborn.client.rpc.commitFiles.askTimeout <value of celeborn.rpc.askTimeout> false Timeout for CommitHandler commit files. 0.4.1 celeborn.client.rpc.getReducerFileGroup.askTimeout <value of celeborn.rpc.askTimeout> false Timeout for ask operations during getting reducer file group information. During this process, there are celeborn.client.requestCommitFiles.maxRetries times for retry opportunities for committing files and 1 times for releasing slots request. User can customize this value according to your setting. 0.2.0 celeborn.client.rpc.maxRetries 3 false Max RPC retry times in LifecycleManager. 0.3.2 celeborn.client.rpc.registerShuffle.askTimeout <value of celeborn.rpc.askTimeout> false Timeout for ask operations during register shuffle. During this process, there are two times for retry opportunities for requesting slots, one request for establishing a connection with Worker and celeborn.client.reserveSlots.maxRetries times for retry opportunities for reserving slots. User can customize this value according to your setting. 0.3.0 celeborn.rpc.registerShuffle.askTimeout celeborn.client.rpc.requestPartition.askTimeout <value of celeborn.rpc.askTimeout> false Timeout for ask operations during requesting change partition location, such as reviving or splitting partition. During this process, there are celeborn.client.reserveSlots.maxRetries times for retry opportunities for reserving slots. User can customize this value according to your setting. 0.2.0 celeborn.client.rpc.reserveSlots.askTimeout <value of celeborn.rpc.askTimeout> false Timeout for LifecycleManager request reserve slots. 0.3.0 celeborn.client.rpc.shared.threads 16 false Number of shared rpc threads in LifecycleManager. 0.3.2 celeborn.client.shuffle.batchHandleChangePartition.interval 100ms false Interval for LifecycleManager to schedule handling change partition requests in batch. 0.3.0 celeborn.shuffle.batchHandleChangePartition.interval celeborn.client.shuffle.batchHandleChangePartition.partitionBuckets 256 false Max number of change partition requests which can be concurrently processed 0.5.0 celeborn.client.shuffle.batchHandleChangePartition.threads 8 false Threads number for LifecycleManager to handle change partition request in batch. 0.3.0 celeborn.shuffle.batchHandleChangePartition.threads celeborn.client.shuffle.batchHandleCommitPartition.interval 5s false Interval for LifecycleManager to schedule handling commit partition requests in batch. 0.3.0 celeborn.shuffle.batchHandleCommitPartition.interval celeborn.client.shuffle.batchHandleCommitPartition.threads 8 false Threads number for LifecycleManager to handle commit partition request in batch. 0.3.0 celeborn.shuffle.batchHandleCommitPartition.threads celeborn.client.shuffle.batchHandleReleasePartition.interval 5s false Interval for LifecycleManager to schedule handling release partition requests in batch. 0.3.0 celeborn.client.shuffle.batchHandleReleasePartition.threads 8 false Threads number for LifecycleManager to handle release partition request in batch. 0.3.0 celeborn.client.shuffle.compression.codec LZ4 false The codec used to compress shuffle data. By default, Celeborn provides three codecs: lz4 , zstd , none . none means that shuffle compression is disabled. Since Flink version 1.17, zstd is supported for Flink shuffle client. 0.3.0 celeborn.shuffle.compression.codec,remote-shuffle.job.compression.codec celeborn.client.shuffle.compression.zstd.level 1 false Compression level for Zstd compression codec, its value should be an integer between -5 and 22. Increasing the compression level will result in better compression at the expense of more CPU and memory. 0.3.0 celeborn.shuffle.compression.zstd.level celeborn.client.shuffle.decompression.lz4.xxhash.instance <undefined> false Decompression XXHash instance for Lz4. Available options: JNI, JAVASAFE, JAVAUNSAFE. 0.3.2 celeborn.client.shuffle.expired.checkInterval 60s false Interval for client to check expired shuffles. 0.3.0 celeborn.shuffle.expired.checkInterval celeborn.client.shuffle.manager.port 0 false Port used by the LifecycleManager on the Driver. 0.3.0 celeborn.shuffle.manager.port celeborn.client.shuffle.mapPartition.split.enabled false false whether to enable shuffle partition split. Currently, this only applies to MapPartition. 0.3.1 celeborn.client.shuffle.partition.type REDUCE false Type of shuffle's partition. 0.3.0 celeborn.shuffle.partition.type celeborn.client.shuffle.partitionSplit.mode SOFT false soft: the shuffle file size might be larger than split threshold. hard: the shuffle file size will be limited to split threshold. 0.3.0 celeborn.shuffle.partitionSplit.mode celeborn.client.shuffle.partitionSplit.threshold 1G false Shuffle file size threshold, if file size exceeds this, trigger split. 0.3.0 celeborn.shuffle.partitionSplit.threshold celeborn.client.shuffle.rangeReadFilter.enabled false false If a spark application have skewed partition, this value can set to true to improve performance. 0.2.0 celeborn.shuffle.rangeReadFilter.enabled celeborn.client.shuffle.register.filterExcludedWorker.enabled false false Whether to filter excluded worker when register shuffle. 0.4.0 celeborn.client.slot.assign.maxWorkers 10000 false Max workers that slots of one shuffle can be allocated on. Will choose the smaller positive one from Master side and Client side, see celeborn.master.slot.assign.maxWorkers . 0.3.1 celeborn.client.spark.fetch.throwsFetchFailure false false client throws FetchFailedException instead of CelebornIOException 0.4.0 celeborn.client.spark.push.dynamicWriteMode.enabled false false Whether to dynamically switch push write mode based on conditions.If true, shuffle mode will be only determined by partition count 0.5.0 celeborn.client.spark.push.dynamicWriteMode.partitionNum.threshold 2000 false Threshold of shuffle partition number for dynamically switching push writer mode. When the shuffle partition number is greater than this value, use the sort-based shuffle writer for memory efficiency; otherwise use the hash-based shuffle writer for speed. This configuration only takes effect when celeborn.client.spark.push.dynamicWriteMode.enabled is true. 0.5.0 celeborn.client.spark.push.sort.memory.maxMemoryFactor 0.4 false the max portion of executor memory which can be used for SortBasedWriter buffer (only valid when celeborn.client.spark.push.sort.memory.useAdaptiveThreshold is enabled 0.5.0 celeborn.client.spark.push.sort.memory.smallPushTolerateFactor 0.2 false Only be in effect when celeborn.client.spark.push.sort.memory.useAdaptiveThreshold is turned on. The larger this value is, the more aggressive Celeborn will enlarge the Sort-based Shuffle writer's memory threshold. Specifically, this config controls when to enlarge the sort shuffle writer's memory threshold. With N bytes data in memory and V as the value of this config, if the number of pushes, C, when using sort based shuffle writer C >= (1 + V) * C' where C' is the number of pushes if we were using hash based writer, we will enlarge the memory threshold by 2X. 0.5.0 celeborn.client.spark.push.sort.memory.threshold 64m false When SortBasedPusher use memory over the threshold, will trigger push data. 0.3.0 celeborn.push.sortMemory.threshold celeborn.client.spark.push.sort.memory.useAdaptiveThreshold false false Adaptively adjust sort-based shuffle writer's memory threshold 0.5.0 celeborn.client.spark.push.unsafeRow.fastWrite.enabled true false This is Celeborn's optimization on UnsafeRow for Spark and it's true by default. If you have changed UnsafeRow's memory layout set this to false. 0.2.2 celeborn.client.spark.shuffle.checkWorker.enabled true false When true, before registering shuffle, LifecycleManager should check if current cluster have available workers, if cluster don't have available workers, fallback to Spark's default shuffle 0.5.0 celeborn.client.spark.shuffle.fallback.numPartitionsThreshold 2147483647 false Celeborn will only accept shuffle of partition number lower than this configuration value. This configuration only takes effect when celeborn.client.spark.shuffle.fallback.policy is AUTO . 0.5.0 celeborn.shuffle.forceFallback.numPartitionsThreshold,celeborn.client.spark.shuffle.forceFallback.numPartitionsThreshold celeborn.client.spark.shuffle.fallback.policy AUTO false Celeborn supports the following kind of fallback policies. 1. ALWAYS: always use spark built-in shuffle implementation; 2. AUTO: prefer to use celeborn shuffle implementation, and fallback to use spark built-in shuffle implementation based on certain factors, e.g. availability of enough workers and quota, shuffle partition number; 3. NEVER: always use celeborn shuffle implementation, and fail fast when it it is concluded that fallback is required based on factors above. 0.5.0 celeborn.client.spark.shuffle.forceFallback.enabled false false Always use spark built-in shuffle implementation. This configuration is deprecated, consider configuring celeborn.client.spark.shuffle.fallback.policy instead. 0.3.0 celeborn.shuffle.forceFallback.enabled celeborn.client.spark.shuffle.writer HASH false Celeborn supports the following kind of shuffle writers. 1. hash: hash-based shuffle writer works fine when shuffle partition count is normal; 2. sort: sort-based shuffle writer works fine when memory pressure is high or shuffle partition count is huge. This configuration only takes effect when celeborn.client.spark.push.dynamicWriteMode.enabled is false. 0.3.0 celeborn.shuffle.writer celeborn.master.endpoints <localhost>:9097 false Endpoints of master nodes for celeborn client to connect, allowed pattern is: <host1>:<port1>[,<host2>:<port2>]* , e.g. clb1:9097,clb2:9098,clb3:9099 . If the port is omitted, 9097 will be used. 0.2.0 celeborn.quota.enabled true false When Master side sets to true, the master will enable to check the quota via QuotaManager. When Client side sets to true, LifecycleManager will request Master side to check whether the current user has enough quota before registration of shuffle. Fallback to the default shuffle service of Spark when Master side checks that there is no enough quota for current user. 0.2.0 celeborn.quota.identity.provider org.apache.celeborn.common.identity.DefaultIdentityProvider false IdentityProvider class name. Default class is org.apache.celeborn.common.identity.DefaultIdentityProvider . Optional values: org.apache.celeborn.common.identity.HadoopBasedIdentityProvider user name will be obtained by UserGroupInformation.getUserName; org.apache.celeborn.common.identity.DefaultIdentityProvider user name and tenant id are default values or user-specific values. 0.2.0 celeborn.quota.identity.user-specific.tenant default false Tenant id if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.quota.identity.user-specific.userName default false User name if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.storage.availableTypes HDD false Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.activeTypes celeborn.storage.hdfs.dir <undefined> false HDFS base directory for Celeborn to store shuffle data. 0.2.0","title":"Client"},{"location":"configuration/columnar-shuffle/","text":"Key Default isDynamic Description Since Deprecated celeborn.columnarShuffle.batch.size 10000 false Vector batch size for columnar shuffle. 0.3.0 celeborn.columnar.shuffle.batch.size celeborn.columnarShuffle.codegen.enabled false false Whether to use codegen for columnar-based shuffle. 0.3.0 celeborn.columnar.shuffle.codegen.enabled celeborn.columnarShuffle.enabled false false Whether to enable columnar-based shuffle. 0.2.0 celeborn.columnar.shuffle.enabled celeborn.columnarShuffle.encoding.dictionary.enabled false false Whether to use dictionary encoding for columnar-based shuffle data. 0.3.0 celeborn.columnar.shuffle.encoding.dictionary.enabled celeborn.columnarShuffle.encoding.dictionary.maxFactor 0.3 false Max factor for dictionary size. The max dictionary size is min(32.0 KiB, celeborn.columnarShuffle.batch.size * celeborn.columnar.shuffle.encoding.dictionary.maxFactor) . 0.3.0 celeborn.columnar.shuffle.encoding.dictionary.maxFactor celeborn.columnarShuffle.offHeap.enabled false false Whether to use off heap columnar vector. 0.3.0 celeborn.columnar.offHeap.enabled","title":"Columnar shuffle"},{"location":"configuration/ha/","text":"Key Default isDynamic Description Since Deprecated celeborn.master.ha.enabled false false When true, master nodes run as Raft cluster mode. 0.3.0 celeborn.ha.enabled celeborn.master.ha.node.<id>.host <required> false Host to bind of master node in HA mode. 0.3.0 celeborn.ha.master.node.<id>.host celeborn.master.ha.node.<id>.internal.port 8097 false Internal port for the workers and other masters to bind to a master node in HA mode. 0.5.0 celeborn.master.ha.node.<id>.port 9097 false Port to bind of master node in HA mode. 0.3.0 celeborn.ha.master.node.<id>.port celeborn.master.ha.node.<id>.ratis.port 9872 false Ratis port to bind of master node in HA mode. 0.3.0 celeborn.ha.master.node.<id>.ratis.port celeborn.master.ha.ratis.raft.rpc.type netty false RPC type for Ratis, available options: netty, grpc. 0.3.0 celeborn.ha.master.ratis.raft.rpc.type celeborn.master.ha.ratis.raft.server.storage.dir /tmp/ratis false Root storage directory to hold RaftServer data. 0.3.0 celeborn.ha.master.ratis.raft.server.storage.dir celeborn.master.ha.ratis.raft.server.storage.startup.option RECOVER false Startup option of RaftServer storage. Available options: RECOVER, FORMAT. 0.5.0","title":"Ha"},{"location":"configuration/master/","text":"Key Default isDynamic Description Since Deprecated celeborn.cluster.name default false Celeborn cluster name. 0.5.0 celeborn.dynamicConfig.refresh.interval 120s false Interval for refreshing the corresponding dynamic config periodically. 0.4.0 celeborn.dynamicConfig.store.backend <undefined> false Store backend for dynamic config service. Available options: FS, DB. If not provided, it means that dynamic configuration is disabled. 0.4.0 celeborn.dynamicConfig.store.db.fetch.pageSize 1000 false The page size for db store to query configurations. 0.5.0 celeborn.dynamicConfig.store.db.hikari.connectionTimeout 30s false The connection timeout that a client will wait for a connection from the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.driverClassName false The jdbc driver class name of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.idleTimeout 600s false The idle timeout that a connection is allowed to sit idle in the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.jdbcUrl false The jdbc url of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.maxLifetime 1800s false The maximum lifetime of a connection in the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.maximumPoolSize 2 false The maximum pool size of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.password false The password of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.username false The username of db store backend. 0.5.0 celeborn.dynamicConfig.store.fs.path <undefined> false The path of dynamic config file for fs store backend. The file format should be yaml. The default path is ${CELEBORN_CONF_DIR}/dynamicConfig.yaml . 0.5.0 celeborn.internal.port.enabled false false Whether to create a internal port on Masters/Workers for inter-Masters/Workers communication. This is beneficial when SASL authentication is enforced for all interactions between clients and Celeborn Services, but the services can exchange messages without being subject to SASL authentication. 0.5.0 celeborn.logConf.enabled false false When true , log the CelebornConf for debugging purposes. 0.5.0 celeborn.master.estimatedPartitionSize.initialSize 64mb false Initial partition size for estimation, it will change according to runtime stats. 0.3.0 celeborn.shuffle.initialEstimatedPartitionSize celeborn.master.estimatedPartitionSize.maxSize <undefined> false Max partition size for estimation. Default value should be celeborn.worker.shuffle.partitionSplit.max * 2. 0.4.1 celeborn.master.estimatedPartitionSize.minSize 8mb false Ignore partition size smaller than this configuration of partition size for estimation. 0.3.0 celeborn.shuffle.minPartitionSizeToEstimate celeborn.master.estimatedPartitionSize.update.initialDelay 5min false Initial delay time before start updating partition size for estimation. 0.3.0 celeborn.shuffle.estimatedPartitionSize.update.initialDelay celeborn.master.estimatedPartitionSize.update.interval 10min false Interval of updating partition size for estimation. 0.3.0 celeborn.shuffle.estimatedPartitionSize.update.interval celeborn.master.hdfs.expireDirs.timeout 1h false The timeout for a expire dirs to be deleted on HDFS. 0.3.0 celeborn.master.heartbeat.application.timeout 300s false Application heartbeat timeout. 0.3.0 celeborn.application.heartbeat.timeout celeborn.master.heartbeat.worker.timeout 120s false Worker heartbeat timeout. 0.3.0 celeborn.worker.heartbeat.timeout celeborn.master.host <localhost> false Hostname for master to bind. 0.2.0 celeborn.master.http.host <localhost> false Master's http host. 0.4.0 celeborn.metrics.master.prometheus.host,celeborn.master.metrics.prometheus.host celeborn.master.http.idleTimeout 30s false Master http server idle timeout. 0.5.0 celeborn.master.http.maxWorkerThreads 200 false Maximum number of threads in the master http worker thread pool. 0.5.0 celeborn.master.http.port 9098 false Master's http port. 0.4.0 celeborn.metrics.master.prometheus.port,celeborn.master.metrics.prometheus.port celeborn.master.http.stopTimeout 5s false Master http server stop timeout. 0.5.0 celeborn.master.internal.port 8097 false Internal port on the master where both workers and other master nodes connect. 0.5.0 celeborn.master.port 9097 false Port for master to bind. 0.2.0 celeborn.master.rackResolver.refresh.interval 30s false Interval for refreshing the node rack information periodically. 0.5.0 celeborn.master.send.applicationMeta.threads 8 false Number of threads used by the Master to send ApplicationMeta to Workers. 0.5.0 celeborn.master.slot.assign.extraSlots 2 false Extra slots number when master assign slots. 0.3.0 celeborn.slots.assign.extraSlots celeborn.master.slot.assign.loadAware.diskGroupGradient 0.1 false This value means how many more workload will be placed into a faster disk group than a slower group. 0.3.0 celeborn.slots.assign.loadAware.diskGroupGradient celeborn.master.slot.assign.loadAware.fetchTimeWeight 1.0 false Weight of average fetch time when calculating ordering in load-aware assignment strategy 0.3.0 celeborn.slots.assign.loadAware.fetchTimeWeight celeborn.master.slot.assign.loadAware.flushTimeWeight 0.0 false Weight of average flush time when calculating ordering in load-aware assignment strategy 0.3.0 celeborn.slots.assign.loadAware.flushTimeWeight celeborn.master.slot.assign.loadAware.numDiskGroups 5 false This configuration is a guidance for load-aware slot allocation algorithm. This value is control how many disk groups will be created. 0.3.0 celeborn.slots.assign.loadAware.numDiskGroups celeborn.master.slot.assign.maxWorkers 10000 false Max workers that slots of one shuffle can be allocated on. Will choose the smaller positive one from Master side and Client side, see celeborn.client.slot.assign.maxWorkers . 0.3.1 celeborn.master.slot.assign.policy ROUNDROBIN false Policy for master to assign slots, Celeborn supports two types of policy: roundrobin and loadaware. Loadaware policy will be ignored when HDFS is enabled in celeborn.storage.activeTypes 0.3.0 celeborn.slots.assign.policy celeborn.master.userResourceConsumption.update.interval 30s false Time length for a window about compute user resource consumption. 0.3.0 celeborn.master.workerUnavailableInfo.expireTimeout 1800s false Worker unavailable info would be cleared when the retention period is expired 0.3.1 celeborn.quota.enabled true false When Master side sets to true, the master will enable to check the quota via QuotaManager. When Client side sets to true, LifecycleManager will request Master side to check whether the current user has enough quota before registration of shuffle. Fallback to the default shuffle service of Spark when Master side checks that there is no enough quota for current user. 0.2.0 celeborn.redaction.regex (?i)secret password token access[.]key false celeborn.storage.availableTypes HDD false Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.activeTypes celeborn.storage.hdfs.dir <undefined> false HDFS base directory for Celeborn to store shuffle data. 0.2.0 celeborn.storage.hdfs.kerberos.keytab <undefined> false Kerberos keytab file path for HDFS storage connection. 0.3.2 celeborn.storage.hdfs.kerberos.principal <undefined> false Kerberos principal for HDFS storage connection. 0.3.2","title":"Master"},{"location":"configuration/metrics/","text":"Key Default isDynamic Description Since Deprecated celeborn.metrics.app.topDiskUsage.count 50 false Size for top items about top disk usage applications list. 0.2.0 celeborn.metrics.app.topDiskUsage.interval 10min false Time length for a window about top disk usage application list. 0.2.0 celeborn.metrics.app.topDiskUsage.windowSize 24 false Window size about top disk usage application list. 0.2.0 celeborn.metrics.capacity 4096 false The maximum number of metrics which a source can use to generate output strings. 0.2.0 celeborn.metrics.collectPerfCritical.enabled false false It controls whether to collect metrics which may affect performance. When enable, Celeborn collects them. 0.2.0 celeborn.metrics.conf <undefined> false Custom metrics configuration file path. Default use metrics.properties in classpath. 0.3.0 celeborn.metrics.enabled true false When true, enable metrics system. 0.2.0 celeborn.metrics.extraLabels false If default metric labels are not enough, extra metric labels can be customized. Labels' pattern is: <label1_key>=<label1_value>[,<label2_key>=<label2_value>]* ; e.g. env=prod,version=1 0.3.0 celeborn.metrics.json.path /metrics/json false URI context path of json metrics HTTP server. 0.4.0 celeborn.metrics.json.pretty.enabled true false When true, view metrics in json pretty format 0.4.0 celeborn.metrics.prometheus.path /metrics/prometheus false URI context path of prometheus metrics HTTP server. 0.4.0 celeborn.metrics.sample.rate 1.0 false It controls if Celeborn collect timer metrics for some operations. Its value should be in [0.0, 1.0]. 0.2.0 celeborn.metrics.timer.slidingWindow.size 4096 false The sliding window size of timer metric. 0.2.0 celeborn.metrics.worker.pauseSpentTime.forceAppend.threshold 10 false Force append worker pause spent time even if worker still in pause serving state.Help user can find worker pause spent time increase, when worker always been pause state.","title":"Metrics"},{"location":"configuration/network-module/","text":"The various transport modules which can be configured are: Module Parent Module Description rpc_app rpc Configure control plane RPC environment used by Celeborn within the application. For backward compatibility, supports fallback to rpc parent module for missing configuration. Note, this is for RPC environment - see below for other transport modules rpc_service rpc Configure control plane RPC environment when communicating with Celeborn service hosts. This includes all RPC communication from application to Celeborn Master/Workers, as well as between Celeborn masters/workers themselves. For backward compatibility, supports fallback to rpc parent module for missing configuration. As with rpc_app , this is only for RPC environment see below for other transport modules. rpc - Fallback parent transport module for rpc_app and rpc_service . It is advisible to use the specific transport modules while configuring - rpc exists primarily for backward compatibility push - Configure transport module for handling data push at Celeborn workers fetch - Configure transport module for handling data fetch at Celeborn workers data - Configure transport module for handling data push and fetch at Celeborn apps replicate - Configure transport module for handling data replication between Celeborn workers","title":"Network module"},{"location":"configuration/network/","text":"Key Default isDynamic Description Since Deprecated celeborn.<module>.fetch.timeoutCheck.interval 5s false Interval for checking fetch data timeout. It only support setting to data since it works for shuffle client fetch data. 0.3.0 celeborn.<module>.fetch.timeoutCheck.threads 4 false Threads num for checking fetch data timeout. It only support setting to data since it works for shuffle client fetch data. 0.3.0 celeborn.<module>.heartbeat.interval 60s false The heartbeat interval between worker and client. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to replicate , it works for replicate client of worker replicating data to peer worker.If you are using the \"celeborn.client.heartbeat.interval\", please use the new configs for each module according to your needs or replace it with \"celeborn.rpc.heartbeat.interval\", \"celeborn.data.heartbeat.interval\" and\"celeborn.replicate.heartbeat.interval\". 0.3.0 celeborn.client.heartbeat.interval celeborn.<module>.io.backLog 0 false Requested maximum length of the queue of incoming connections. Default 0 for no backlog. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. celeborn.<module>.io.clientThreads 0 false Number of threads used in the client thread pool. Default to 0, which is 2x#cores. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to replicate , it works for replicate client of worker replicating data to peer worker. celeborn.<module>.io.connectTimeout <value of celeborn.network.connect.timeout> false Socket connect timeout. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to replicate , it works for the replicate client of worker replicating data to peer worker. celeborn.<module>.io.connectionTimeout <value of celeborn.network.timeout> false Connection active timeout. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server or client of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. celeborn.<module>.io.enableVerboseMetrics false false Whether to track Netty memory detailed metrics. If true, the detailed metrics of Netty PoolByteBufAllocator will be gotten, otherwise only general memory usage will be tracked. celeborn.<module>.io.lazyFD true false Whether to initialize FileDescriptor lazily or not. If true, file descriptors are created only when data is going to be transferred. This can reduce the number of open files. If setting to fetch , it works for worker fetch server. celeborn.<module>.io.maxRetries 3 false Max number of times we will try IO exceptions (such as connection timeouts) per request. If set to 0, we will not do any retries. If setting to push , it works for Flink shuffle client push data. celeborn.<module>.io.mode NIO false Netty EventLoopGroup backend, available options: NIO, EPOLL. celeborn.<module>.io.numConnectionsPerPeer 1 false Number of concurrent connections between two nodes. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to replicate , it works for replicate client of worker replicating data to peer worker. celeborn.<module>.io.preferDirectBufs true false If true, we will prefer allocating off-heap byte buffers within Netty. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server or client of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. celeborn.<module>.io.receiveBuffer 0b false Receive buffer size (SO_RCVBUF). Note: the optimal size for receive buffer and send buffer should be latency * network_bandwidth. Assuming latency = 1ms, network_bandwidth = 10Gbps buffer size should be ~ 1.25MB. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server or client of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. 0.2.0 celeborn.<module>.io.retryWait 5s false Time that we will wait in order to perform a retry after an IOException. Only relevant if maxIORetries > 0. If setting to data , it works for shuffle client push and fetch data. If setting to push , it works for Flink shuffle client push data. 0.2.0 celeborn.<module>.io.saslTimeout 30s false Timeout for a single round trip of auth message exchange, in milliseconds. 0.5.0 celeborn.<module>.io.sendBuffer 0b false Send buffer size (SO_SNDBUF). If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to data , it works for shuffle client push and fetch data. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server or client of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. 0.2.0 celeborn.<module>.io.serverThreads 0 false Number of threads used in the server thread pool. Default to 0, which is 2x#cores. If setting to rpc_app , works for shuffle client. If setting to rpc_service , works for master or worker. If setting to push , it works for worker receiving push data. If setting to replicate , it works for replicate server of worker replicating data to peer worker. If setting to fetch , it works for worker fetch server. celeborn.<module>.push.timeoutCheck.interval 5s false Interval for checking push data timeout. If setting to data , it works for shuffle client push data. If setting to push , it works for Flink shuffle client push data. If setting to replicate , it works for replicate client of worker replicating data to peer worker. 0.3.0 celeborn.<module>.push.timeoutCheck.threads 4 false Threads num for checking push data timeout. If setting to data , it works for shuffle client push data. If setting to push , it works for Flink shuffle client push data. If setting to replicate , it works for replicate client of worker replicating data to peer worker. 0.3.0 celeborn.<role>.rpc.dispatcher.threads <value of celeborn.rpc.dispatcher.threads> false Threads number of message dispatcher event loop for roles celeborn.io.maxDefaultNettyThreads 64 false Max default netty threads 0.3.2 celeborn.network.bind.preferIpAddress true false When true , prefer to use IP address, otherwise FQDN. This configuration only takes effects when the bind hostname is not set explicitly, in such case, Celeborn will find the first non-loopback address to bind. 0.3.0 celeborn.network.connect.timeout 10s false Default socket connect timeout. 0.2.0 celeborn.network.memory.allocator.numArenas <undefined> false Number of arenas for pooled memory allocator. Default value is Runtime.getRuntime.availableProcessors, min value is 2. 0.3.0 celeborn.network.memory.allocator.verbose.metric false false Whether to enable verbose metric for pooled allocator. 0.3.0 celeborn.network.timeout 240s false Default timeout for network operations. 0.2.0 celeborn.port.maxRetries 1 false When port is occupied, we will retry for max retry times. 0.2.0 celeborn.rpc.askTimeout 60s false Timeout for RPC ask operations. It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes 0.2.0 celeborn.rpc.connect.threads 64 false 0.2.0 celeborn.rpc.dispatcher.threads 0 false Threads number of message dispatcher event loop. Default to 0, which is availableCore. 0.3.0 celeborn.rpc.dispatcher.numThreads celeborn.rpc.inbox.capacity 0 false Specifies size of the in memory bounded capacity. 0.5.0 celeborn.rpc.io.threads <undefined> false Netty IO thread number of NettyRpcEnv to handle RPC request. The default threads number is the number of runtime available processors. 0.2.0 celeborn.rpc.lookupTimeout 30s false Timeout for RPC lookup operations. 0.2.0 celeborn.shuffle.io.maxChunksBeingTransferred <undefined> false The max number of chunks allowed to be transferred at the same time on shuffle service. Note that new incoming connections will be closed when the max number is hit. The client will retry according to the shuffle retry configs (see celeborn.<module>.io.maxRetries and celeborn.<module>.io.retryWait ), if those limits are reached the task will fail with fetch failure. 0.2.0 celeborn.ssl.<module>.enabled false false Enables SSL for securing wire traffic. 0.5.0 celeborn.ssl.<module>.enabledAlgorithms <undefined> false A comma-separated list of ciphers. The specified ciphers must be supported by JVM. The reference list of protocols can be found in the \"JSSE Cipher Suite Names\" section of the Java security guide. The list for Java 11, for example, can be found at this page Note: If not set, the default cipher suite for the JRE will be used 0.5.0 celeborn.ssl.<module>.keyStore <undefined> false Path to the key store file. The path can be absolute or relative to the directory in which the process is started. 0.5.0 celeborn.ssl.<module>.keyStorePassword <undefined> false Password to the key store. 0.5.0 celeborn.ssl.<module>.protocol TLSv1.2 false TLS protocol to use. The protocol must be supported by JVM. The reference list of protocols can be found in the \"Additional JSSE Standard Names\" section of the Java security guide. For Java 11, for example, the list can be found here 0.5.0 celeborn.ssl.<module>.trustStore <undefined> false Path to the trust store file. The path can be absolute or relative to the directory in which the process is started. 0.5.0 celeborn.ssl.<module>.trustStorePassword <undefined> false Password for the trust store. 0.5.0 celeborn.ssl.<module>.trustStoreReloadIntervalMs 10s false The interval at which the trust store should be reloaded (in milliseconds), when enabled. This setting is mostly only useful for server components, not applications. 0.5.0 celeborn.ssl.<module>.trustStoreReloadingEnabled false false Whether the trust store should be reloaded periodically. This setting is mostly only useful for Celeborn services (masters, workers), and not applications. 0.5.0","title":"Network"},{"location":"configuration/quota/","text":"Key Default isDynamic Description Since Deprecated celeborn.quota.enabled true false When Master side sets to true, the master will enable to check the quota via QuotaManager. When Client side sets to true, LifecycleManager will request Master side to check whether the current user has enough quota before registration of shuffle. Fallback to the default shuffle service of Spark when Master side checks that there is no enough quota for current user. 0.2.0 celeborn.quota.identity.provider org.apache.celeborn.common.identity.DefaultIdentityProvider false IdentityProvider class name. Default class is org.apache.celeborn.common.identity.DefaultIdentityProvider . Optional values: org.apache.celeborn.common.identity.HadoopBasedIdentityProvider user name will be obtained by UserGroupInformation.getUserName; org.apache.celeborn.common.identity.DefaultIdentityProvider user name and tenant id are default values or user-specific values. 0.2.0 celeborn.quota.identity.user-specific.tenant default false Tenant id if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.quota.identity.user-specific.userName default false User name if celeborn.quota.identity.provider is org.apache.celeborn.common.identity.DefaultIdentityProvider. 0.3.0 celeborn.quota.tenant.diskBytesWritten 9223372036854775807 true Quota dynamic configuration for written disk bytes. 0.5.0 celeborn.quota.tenant.diskFileCount 9223372036854775807 true Quota dynamic configuration for written disk file count. 0.5.0 celeborn.quota.tenant.hdfsBytesWritten 9223372036854775807 true Quota dynamic configuration for written hdfs bytes. 0.5.0 celeborn.quota.tenant.hdfsFileCount 9223372036854775807 true Quota dynamic configuration for written hdfs file count. 0.5.0","title":"Quota"},{"location":"configuration/worker/","text":"Key Default isDynamic Description Since Deprecated celeborn.cluster.name default false Celeborn cluster name. 0.5.0 celeborn.dynamicConfig.refresh.interval 120s false Interval for refreshing the corresponding dynamic config periodically. 0.4.0 celeborn.dynamicConfig.store.backend <undefined> false Store backend for dynamic config service. Available options: FS, DB. If not provided, it means that dynamic configuration is disabled. 0.4.0 celeborn.dynamicConfig.store.db.fetch.pageSize 1000 false The page size for db store to query configurations. 0.5.0 celeborn.dynamicConfig.store.db.hikari.connectionTimeout 30s false The connection timeout that a client will wait for a connection from the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.driverClassName false The jdbc driver class name of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.idleTimeout 600s false The idle timeout that a connection is allowed to sit idle in the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.jdbcUrl false The jdbc url of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.maxLifetime 1800s false The maximum lifetime of a connection in the pool for db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.maximumPoolSize 2 false The maximum pool size of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.password false The password of db store backend. 0.5.0 celeborn.dynamicConfig.store.db.hikari.username false The username of db store backend. 0.5.0 celeborn.dynamicConfig.store.fs.path <undefined> false The path of dynamic config file for fs store backend. The file format should be yaml. The default path is ${CELEBORN_CONF_DIR}/dynamicConfig.yaml . 0.5.0 celeborn.internal.port.enabled false false Whether to create a internal port on Masters/Workers for inter-Masters/Workers communication. This is beneficial when SASL authentication is enforced for all interactions between clients and Celeborn Services, but the services can exchange messages without being subject to SASL authentication. 0.5.0 celeborn.logConf.enabled false false When true , log the CelebornConf for debugging purposes. 0.5.0 celeborn.master.endpoints <localhost>:9097 false Endpoints of master nodes for celeborn client to connect, allowed pattern is: <host1>:<port1>[,<host2>:<port2>]* , e.g. clb1:9097,clb2:9098,clb3:9099 . If the port is omitted, 9097 will be used. 0.2.0 celeborn.master.estimatedPartitionSize.minSize 8mb false Ignore partition size smaller than this configuration of partition size for estimation. 0.3.0 celeborn.shuffle.minPartitionSizeToEstimate celeborn.master.internal.endpoints <localhost>:8097 false Endpoints of master nodes just for celeborn workers to connect, allowed pattern is: <host1>:<port1>[,<host2>:<port2>]* , e.g. clb1:8097,clb2:8097,clb3:8097 . If the port is omitted, 8097 will be used. 0.5.0 celeborn.redaction.regex (?i)secret password token access[.]key false celeborn.shuffle.chunk.size 8m false Max chunk size of reducer's merged shuffle data. For example, if a reducer's shuffle data is 128M and the data will need 16 fetch chunk requests to fetch. 0.2.0 celeborn.shuffle.sortPartition.block.compactionFactor 0.25 false Combine sorted shuffle blocks such that size of compacted shuffle block does not exceed compactionFactor * celeborn.shuffle.chunk.size 0.4.2 celeborn.storage.availableTypes HDD false Enabled storages. Available options: MEMORY,HDD,SSD,HDFS. Note: HDD and SSD would be treated as identical. 0.3.0 celeborn.storage.activeTypes celeborn.storage.hdfs.dir <undefined> false HDFS base directory for Celeborn to store shuffle data. 0.2.0 celeborn.storage.hdfs.kerberos.keytab <undefined> false Kerberos keytab file path for HDFS storage connection. 0.3.2 celeborn.storage.hdfs.kerberos.principal <undefined> false Kerberos principal for HDFS storage connection. 0.3.2 celeborn.worker.activeConnection.max <undefined> false If the number of active connections on a worker exceeds this configuration value, the worker will be marked as high-load in the heartbeat report, and the master will not include that node in the response of RequestSlots. 0.3.1 celeborn.worker.applicationRegistry.cache.size 10000 false Cache size of the application registry on Workers. 0.5.0 celeborn.worker.bufferStream.threadsPerMountpoint 8 false Threads count for read buffer per mount point. 0.3.0 celeborn.worker.clean.threads 64 false Thread number of worker to clean up expired shuffle keys. 0.3.2 celeborn.worker.closeIdleConnections false false Whether worker will close idle connections. 0.2.0 celeborn.worker.commitFiles.threads 32 false Thread number of worker to commit shuffle data files asynchronously. It's recommended to set at least 128 when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.commit.threads celeborn.worker.commitFiles.timeout 120s false Timeout for a Celeborn worker to commit files of a shuffle. It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.shuffle.commit.timeout celeborn.worker.congestionControl.check.interval 10ms false Interval of worker checks congestion if celeborn.worker.congestionControl.enabled is true. 0.3.2 celeborn.worker.congestionControl.enabled false false Whether to enable congestion control or not. 0.3.0 celeborn.worker.congestionControl.high.watermark <undefined> false If the total bytes in disk buffer exceeds this configure, will start to congestusers whose produce rate is higher than the potential average consume rate. The congestion will stop if the produce rate is lower or equal to the average consume rate, or the total pending bytes lower than celeborn.worker.congestionControl.low.watermark 0.3.0 celeborn.worker.congestionControl.low.watermark <undefined> false Will stop congest users if the total pending bytes of disk buffer is lower than this configuration 0.3.0 celeborn.worker.congestionControl.sample.time.window 10s false The worker holds a time sliding list to calculate users' produce/consume rate 0.3.0 celeborn.worker.congestionControl.user.inactive.interval 10min false How long will consider this user is inactive if it doesn't send data 0.3.0 celeborn.worker.decommission.checkInterval 30s false The wait interval of checking whether all the shuffle expired during worker decommission 0.4.0 celeborn.worker.decommission.forceExitTimeout 6h false The wait time of waiting for all the shuffle expire during worker decommission. 0.4.0 celeborn.worker.directMemoryRatioForMemoryFileStorage 0.0 false Max ratio of direct memory to store shuffle data. This feature is experimental and disabled by default. 0.5.0 celeborn.worker.directMemoryRatioForReadBuffer 0.1 false Max ratio of direct memory for read buffer 0.2.0 celeborn.worker.directMemoryRatioToPauseReceive 0.85 false If direct memory usage reaches this limit, the worker will stop to receive data from Celeborn shuffle clients. 0.2.0 celeborn.worker.directMemoryRatioToPauseReplicate 0.95 false If direct memory usage reaches this limit, the worker will stop to receive replication data from other workers. This value should be higher than celeborn.worker.directMemoryRatioToPauseReceive. 0.2.0 celeborn.worker.directMemoryRatioToResume 0.7 false If direct memory usage is less than this limit, worker will resume. 0.2.0 celeborn.worker.disk.clean.threads 4 false Thread number of worker to clean up directories of expired shuffle keys on disk. 0.3.2 celeborn.worker.fetch.heartbeat.enabled false false enable the heartbeat from worker to client when fetching data 0.3.0 celeborn.worker.fetch.io.threads <undefined> false Netty IO thread number of worker to handle client fetch data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.fetch.port 0 false Server port for Worker to receive fetch data request from ShuffleClient. 0.2.0 celeborn.worker.flusher.buffer.size 256k false Size of buffer used by a single flusher. 0.2.0 celeborn.worker.flusher.diskTime.slidingWindow.size 20 false The size of sliding windows used to calculate statistics about flushed time and count. 0.3.0 celeborn.worker.flusher.avgFlushTime.slidingWindow.size celeborn.worker.flusher.hdd.threads 1 false Flusher's thread count per disk used for write data to HDD disks. 0.2.0 celeborn.worker.flusher.hdfs.buffer.size 4m false Size of buffer used by a HDFS flusher. 0.3.0 celeborn.worker.flusher.hdfs.threads 8 false Flusher's thread count used for write data to HDFS. 0.2.0 celeborn.worker.flusher.shutdownTimeout 3s false Timeout for a flusher to shutdown. 0.2.0 celeborn.worker.flusher.ssd.threads 16 false Flusher's thread count per disk used for write data to SSD disks. 0.2.0 celeborn.worker.flusher.threads 16 false Flusher's thread count per disk for unknown-type disks. 0.2.0 celeborn.worker.graceful.shutdown.checkSlotsFinished.interval 1s false The wait interval of checking whether all released slots to be committed or destroyed during worker graceful shutdown 0.2.0 celeborn.worker.graceful.shutdown.checkSlotsFinished.timeout 480s false The wait time of waiting for the released slots to be committed or destroyed during worker graceful shutdown. 0.2.0 celeborn.worker.graceful.shutdown.enabled false false When true, during worker shutdown, the worker will wait for all released slots to be committed or destroyed. 0.2.0 celeborn.worker.graceful.shutdown.partitionSorter.shutdownTimeout 120s false The wait time of waiting for sorting partition files during worker graceful shutdown. 0.2.0 celeborn.worker.graceful.shutdown.recoverDbBackend ROCKSDB false Specifies a disk-based store used in local db. ROCKSDB or LEVELDB (deprecated). 0.4.0 celeborn.worker.graceful.shutdown.recoverPath <tmp>/recover false The path to store DB. 0.2.0 celeborn.worker.graceful.shutdown.saveCommittedFileInfo.interval 5s false Interval for a Celeborn worker to flush committed file infos into Level DB. 0.3.1 celeborn.worker.graceful.shutdown.saveCommittedFileInfo.sync false false Whether to call sync method to save committed file infos into Level DB to handle OS crash. 0.3.1 celeborn.worker.graceful.shutdown.timeout 600s false The worker's graceful shutdown timeout time. 0.2.0 celeborn.worker.http.host <localhost> false Worker's http host. 0.4.0 celeborn.metrics.worker.prometheus.host,celeborn.worker.metrics.prometheus.host celeborn.worker.http.idleTimeout 30s false Worker http server idle timeout. 0.5.0 celeborn.worker.http.maxWorkerThreads 200 false Maximum number of threads in the worker http worker thread pool. 0.5.0 celeborn.worker.http.port 9096 false Worker's http port. 0.4.0 celeborn.metrics.worker.prometheus.port,celeborn.worker.metrics.prometheus.port celeborn.worker.http.stopTimeout 5s false Worker http server stop timeout. 0.5.0 celeborn.worker.internal.port 0 false Internal server port on the Worker where the master nodes connect. 0.5.0 celeborn.worker.jvmProfiler.enabled false false Turn on code profiling via async_profiler in workers. 0.5.0 celeborn.worker.jvmProfiler.localDir . false Local file system path on worker where profiler output is saved. Defaults to the working directory of the worker process. 0.5.0 celeborn.worker.jvmProfiler.options event=wall,interval=10ms,alloc=2m,lock=10ms,chunktime=300s false Options to pass on to the async profiler. 0.5.0 celeborn.worker.jvmQuake.check.interval 1s false Interval of gc behavior checking for worker jvm quake. 0.4.0 celeborn.worker.jvmQuake.dump.enabled true false Whether to heap dump for the maximum GC 'deficit' during worker jvm quake. 0.4.0 celeborn.worker.jvmQuake.dump.path <tmp>/jvm-quake/dump/<pid> false The path of heap dump for the maximum GC 'deficit' during worker jvm quake. 0.4.0 celeborn.worker.jvmQuake.dump.threshold 30s false The threshold of heap dump for the maximum GC 'deficit' which can be accumulated before jvmquake takes action. Meanwhile, there is no heap dump generated when dump threshold is greater than kill threshold. 0.4.0 celeborn.worker.jvmQuake.enabled false false When true, Celeborn worker will start the jvm quake to monitor of gc behavior, which enables early detection of memory management issues and facilitates fast failure. 0.4.0 celeborn.worker.jvmQuake.exitCode 502 false The exit code of system kill for the maximum GC 'deficit' during worker jvm quake. 0.4.0 celeborn.worker.jvmQuake.kill.threshold 60s false The threshold of system kill for the maximum GC 'deficit' which can be accumulated before jvmquake takes action. 0.4.0 celeborn.worker.jvmQuake.runtimeWeight 5.0 false The factor by which to multiply running JVM time, when weighing it against GCing time. 'Deficit' is accumulated as gc_time - runtime * runtime_weight , and is compared against threshold to determine whether to take action. 0.4.0 celeborn.worker.memoryFileStorage.evict.aggressiveMode.enabled false false If this set to true, memory shuffle files will be evicted when worker is in PAUSED state. If the worker's offheap memory is not ample, set this to true and decrease celeborn.worker.directMemoryRatioForMemoryFileStorage will be helpful. 0.5.1 celeborn.worker.memoryFileStorage.evict.ratio 0.5 false If memory shuffle storage usage rate is above this config, the memory storage shuffle files will evict to free memory. 0.5.1 celeborn.worker.memoryFileStorage.maxFileSize 8MB false Max size for a memory storage file. It must be less than 2GB. 0.5.0 celeborn.worker.monitor.disk.check.interval 30s false Intervals between device monitor to check disk. 0.3.0 celeborn.worker.monitor.disk.checkInterval celeborn.worker.monitor.disk.check.timeout 30s false Timeout time for worker check device status. 0.3.0 celeborn.worker.disk.check.timeout celeborn.worker.monitor.disk.checklist readwrite,diskusage false Monitor type for disk, available items are: iohang, readwrite and diskusage. 0.2.0 celeborn.worker.monitor.disk.enabled true false When true, worker will monitor device and report to master. 0.3.0 celeborn.worker.monitor.disk.notifyError.expireTimeout 10m false The expire timeout of non-critical device error. Only notify critical error when the number of non-critical errors for a period of time exceeds threshold. 0.3.0 celeborn.worker.monitor.disk.notifyError.threshold 64 false Device monitor will only notify critical error once the accumulated valid non-critical error number exceeding this threshold. 0.3.0 celeborn.worker.monitor.disk.sys.block.dir /sys/block false The directory where linux file block information is stored. 0.2.0 celeborn.worker.monitor.memory.check.interval 10ms false Interval of worker direct memory checking. 0.3.0 celeborn.worker.memory.checkInterval celeborn.worker.monitor.memory.report.interval 10s false Interval of worker direct memory tracker reporting to log. 0.3.0 celeborn.worker.memory.reportInterval celeborn.worker.monitor.memory.trimChannelWaitInterval 1s false Wait time after worker trigger channel to trim cache. 0.3.0 celeborn.worker.monitor.memory.trimFlushWaitInterval 1s false Wait time after worker trigger StorageManger to flush data. 0.3.0 celeborn.worker.partition.initial.readBuffersMax 1024 false Max number of initial read buffers 0.3.0 celeborn.worker.partition.initial.readBuffersMin 1 false Min number of initial read buffers 0.3.0 celeborn.worker.partitionSorter.directMemoryRatioThreshold 0.1 false Max ratio of partition sorter's memory for sorting, when reserved memory is higher than max partition sorter memory, partition sorter will stop sorting. 0.2.0 celeborn.worker.push.heartbeat.enabled false false enable the heartbeat from worker to client when pushing data 0.3.0 celeborn.worker.push.io.threads <undefined> false Netty IO thread number of worker to handle client push data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.push.port 0 false Server port for Worker to receive push data request from ShuffleClient. 0.2.0 celeborn.worker.readBuffer.allocationWait 50ms false The time to wait when buffer dispatcher can not allocate a buffer. 0.3.0 celeborn.worker.readBuffer.target.changeThreshold 1mb false The target ratio for pre read memory usage. 0.3.0 celeborn.worker.readBuffer.target.ratio 0.9 false The target ratio for read ahead buffer's memory usage. 0.3.0 celeborn.worker.readBuffer.target.updateInterval 100ms false The interval for memory manager to calculate new read buffer's target memory. 0.3.0 celeborn.worker.readBuffer.toTriggerReadMin 32 false Min buffers count for map data partition to trigger read. 0.3.0 celeborn.worker.register.timeout 180s false Worker register timeout. 0.2.0 celeborn.worker.replicate.fastFail.duration 60s false If a replicate request not replied during the duration, worker will mark the replicate data request as failed.It's recommended to set at least 240s when HDFS is enabled in celeborn.storage.activeTypes . 0.2.0 celeborn.worker.replicate.io.threads <undefined> false Netty IO thread number of worker to replicate shuffle data. The default threads number is the number of flush thread. 0.2.0 celeborn.worker.replicate.port 0 false Server port for Worker to receive replicate data request from other Workers. 0.2.0 celeborn.worker.replicate.randomConnection.enabled true false Whether worker will create random connection to peer when replicate data. When false, worker tend to reuse the same cached TransportClient to a specific replicate worker; when true, worker tend to use different cached TransportClient. Netty will use the same thread to serve the same connection, so with more connections replicate server can leverage more netty threads 0.2.1 celeborn.worker.replicate.threads 64 false Thread number of worker to replicate shuffle data. 0.2.0 celeborn.worker.rpc.port 0 false Server port for Worker to receive RPC request. 0.2.0 celeborn.worker.shuffle.partitionSplit.enabled true false enable the partition split on worker side 0.3.0 celeborn.worker.partition.split.enabled celeborn.worker.shuffle.partitionSplit.max 2g false Specify the maximum partition size for splitting, and ensure that individual partition files are always smaller than this limit. 0.3.0 celeborn.worker.shuffle.partitionSplit.min 1m false Min size for a partition to split 0.3.0 celeborn.shuffle.partitionSplit.min celeborn.worker.sortPartition.indexCache.expire 180s false PartitionSorter's cache item expire time. 0.4.0 celeborn.worker.sortPartition.indexCache.maxWeight 100000 false PartitionSorter's cache max weight for index buffer. 0.4.0 celeborn.worker.sortPartition.prefetch.enabled true false When true, partition sorter will prefetch the original partition files to page cache and reserve memory configured by celeborn.worker.sortPartition.reservedMemoryPerPartition to allocate a block of memory for prefetching while sorting a shuffle file off-heap with page cache for non-hdfs files. Otherwise, partition sorter seeks to position of each block and does not prefetch for non-hdfs files. 0.5.0 celeborn.worker.sortPartition.reservedMemoryPerPartition 1mb false Reserved memory when sorting a shuffle file off-heap. 0.3.0 celeborn.worker.partitionSorter.reservedMemoryPerPartition celeborn.worker.sortPartition.threads <undefined> false PartitionSorter's thread counts. It's recommended to set at least 64 when HDFS is enabled in celeborn.storage.activeTypes . 0.3.0 celeborn.worker.partitionSorter.threads celeborn.worker.sortPartition.timeout 220s false Timeout for a shuffle file to sort. 0.3.0 celeborn.worker.partitionSorter.sort.timeout celeborn.worker.storage.checkDirsEmpty.maxRetries 3 false The number of retries for a worker to check if the working directory is cleaned up before registering with the master. 0.3.0 celeborn.worker.disk.checkFileClean.maxRetries celeborn.worker.storage.checkDirsEmpty.timeout 1000ms false The wait time per retry for a worker to check if the working directory is cleaned up before registering with the master. 0.3.0 celeborn.worker.disk.checkFileClean.timeout celeborn.worker.storage.dirs <undefined> false Directory list to store shuffle data. It's recommended to configure one directory on each disk. Storage size limit can be set for each directory. For the sake of performance, there should be no more than 2 flush threads on the same disk partition if you are using HDD, and should be 8 or more flush threads on the same disk partition if you are using SSD. For example: dir1[:capacity=][:disktype=][:flushthread=],dir2[:capacity=][:disktype=][:flushthread=] 0.2.0 celeborn.worker.storage.disk.reserve.ratio <undefined> false Celeborn worker reserved ratio for each disk. The minimum usable size for each disk is the max space between the reserved space and the space calculate via reserved ratio. 0.3.2 celeborn.worker.storage.disk.reserve.size 5G false Celeborn worker reserved space for each disk. 0.3.0 celeborn.worker.disk.reserve.size celeborn.worker.storage.expireDirs.timeout 1h false The timeout for a expire dirs to be deleted on disk. 0.3.2 celeborn.worker.storage.workingDir celeborn-worker/shuffle_data false Worker's working dir path name. 0.3.0 celeborn.worker.workingDir celeborn.worker.writer.close.timeout 120s false Timeout for a file writer to close 0.2.0 celeborn.worker.writer.create.maxAttempts 3 false Retry count for a file writer to create if its creation was failed. 0.2.0","title":"Worker"},{"location":"developers/client/","text":"Overview Celeborn Client is separated into two roles : LifecycleManager for control plane, responsible for managing all shuffle metadata for the application, resides in driver for Apache Spark and JobMaster for Apache Flink. See LifecycleManager ShuffleClient for data plane, responsible for write/read data to/from Workers, resides in executors for Apache Spark and TaskManager for Apache Flink. See ShuffleClient","title":"Overview"},{"location":"developers/client/#overview","text":"Celeborn Client is separated into two roles : LifecycleManager for control plane, responsible for managing all shuffle metadata for the application, resides in driver for Apache Spark and JobMaster for Apache Flink. See LifecycleManager ShuffleClient for data plane, responsible for write/read data to/from Workers, resides in executors for Apache Spark and TaskManager for Apache Flink. See ShuffleClient","title":"Overview"},{"location":"developers/configuration/","text":"Configuration The configuration of Celeborn is divided into static and dynamic categories, with details provided in the Configuration Guide . Static Configuration Static configuration, referred to as CelebornConf , loads configurations from the default file located at $CELEBORN_HOME/conf/celeborn-defaults.conf . Dynamic Configuration Dynamic configuration allows for changes to be applied at runtime, as necessary, and it takes precedence over the corresponding static configuration in the Celeborn Master and Worker . A configuration key's dynamic nature is indicated by the isDynamic property, as listed in All Configurations . This means that configurations tagged with the dynamic property can be updated and refreshed while Celeborn is running. Config Level At present dynamic configuration supports various config levels including: SYSTEM : The system configurations. TENANT : The dynamic configurations of tenant id. TENANT_USER : The dynamic configurations of tenant id and username. When applying dynamic configuration, the following is the order of precedence for configuration levels: SYSTEM level configuration takes precedence over static configuration and the default CelebornConf . If the system-level configuration is absent, it will fall back to the static configuration defined in CelebornConf . TENANT level configuration supersedes the SYSTEM level, meaning that configurations specific to a tenant id will override those set at the system level. If tenant-level configuration is absent, it will fall back to the system-level dynamic configuration. TENANT_USER level configuration takes precedence over TENANT level. Configurations specific to both a tenant id and username will override those set at the tenant level. If tenant-user-level configuration is missing, it will fall back to the tenant-level dynamic configuration. Config Service The config service provides a configuration management service with a local cache for both static and dynamic configurations. Moreover, ConfigService is a pluggable service interface whose implementation can vary based on different storage backends. The storage backend for ConfigService is specified by the configuration key celeborn.dynamicConfig.store.backend , and it currently supports both filesystem ( FS ) and database ( DB ) as storage backends. If no storage backend is specified, this indicates that the config service is disabled. FileSystem Config Service The filesystem config service enables the use of dynamic configuration files, the location of which is set by the configuration key celeborn.dynamicConfig.store.fs.path . The template for the dynamic configuration is as follows: # SYSTEM level configuration - level : SYSTEM config : [ config_key ]: [ config_val ] ... # TENANT level configuration - tenantId : [ tenant_id ] level : TENANT config : [ config_key ]: [ config_val ] ... users : # TENANT_USER level configuration - name : [ name ] config : [ config_key ]: [ config_val ] ... For example, a Celeborn worker celeborn-worker has 10 storage directories or disks and the buffer size is set to 256 KiB. A tenant tenantId1 only uses half of the storage and sets the buffer size to 128 KiB. Meanwhile, a user user1 needs to change the buffer size to 96 KiB at runtime. The example configurations are as follows: # SYSTEM level configuration - level : SYSTEM config : celeborn.worker.flusher.buffer.size : 256K # sets buffer size of worker to 256 KiB # TENANT level configuration - tenantId : tenantId1 level : TENANT config : celeborn.worker.flusher.buffer.size : 128K # sets buffer size of tenantId1 to 128 KiB users : # TENANT_USER level configuration - name : user1 config : celeborn.worker.flusher.buffer.size : 96K # sets buffer size of tenantId1 and user1 to 128 KiB Database Config Service The database config service updates dynamic configurations stored in the database using the JDBC approach. Configuration settings for the database storage backend are defined by the celeborn.dynamicConfig.store.db.* series of configuration keys. To use the database as a config store backend, it is necessary to create tables for dynamic configurations at the various configuration levels. The sql script for MySQL configuration tables is located under $CELEBORN_HOME/db-scripts directory. After the creation of configuration tables, dynamic configuration of config levels is specified via inserting a configuration record in corresponding config level table. Above example dynamic configurations can be supported via the following sql: CREATE TABLE IF NOT EXISTS celeborn_cluster_info ( id int NOT NULL AUTO_INCREMENT , name varchar ( 255 ) NOT NULL COMMENT 'celeborn cluster name' , namespace varchar ( 255 ) DEFAULT NULL COMMENT 'celeborn cluster namespace' , endpoint varchar ( 255 ) DEFAULT NULL COMMENT 'celeborn cluster endpoint' , gmt_create timestamp NOT NULL , gmt_modify timestamp NOT NULL , PRIMARY KEY ( id ), UNIQUE KEY ` index_cluster_unique_name ` ( ` name ` ) ); # SYSTEM level configuration CREATE TABLE IF NOT EXISTS celeborn_cluster_system_config ( id int NOT NULL AUTO_INCREMENT , cluster_id int NOT NULL , config_key varchar ( 255 ) NOT NULL , config_value varchar ( 255 ) NOT NULL , type varchar ( 255 ) DEFAULT NULL COMMENT 'conf categories, such as quota' , gmt_create timestamp NOT NULL , gmt_modify timestamp NOT NULL , PRIMARY KEY ( id ), UNIQUE KEY ` index_unique_system_config_key ` ( ` cluster_id ` , ` config_key ` ) ); # TENANT / TENANT_USER level configuration CREATE TABLE IF NOT EXISTS celeborn_cluster_tenant_config ( id int NOT NULL AUTO_INCREMENT , cluster_id int NOT NULL , tenant_id varchar ( 255 ) NOT NULL , level varchar ( 255 ) NOT NULL COMMENT 'config level, valid level is TENANT,USER' , name varchar ( 255 ) DEFAULT NULL COMMENT 'tenant sub user' , config_key varchar ( 255 ) NOT NULL , config_value varchar ( 255 ) NOT NULL , type varchar ( 255 ) DEFAULT NULL COMMENT 'conf categories, such as quota' , gmt_create timestamp NOT NULL , gmt_modify timestamp NOT NULL , PRIMARY KEY ( id ), UNIQUE KEY ` index_unique_tenant_config_key ` ( ` cluster_id ` , ` tenant_id ` , ` user ` , ` config_key ` ) ); INSERT INTO celeborn_cluster_info ( ` id ` , ` name ` , ` namespace ` , ` endpoint ` , ` gmt_create ` , ` gmt_modify ` ) VALUES ( 1 , 'default' , 'celeborn-worker' , 'celeborn-namespace.endpoint.com' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ); # SYSTEM level configuration # sets buffer size of celeborn - worker to 256 KiB INSERT INTO ` celeborn_cluster_system_config ` ( ` id ` , ` cluster_id ` , ` config_key ` , ` config_value ` , ` type ` , ` gmt_create ` , ` gmt_modify ` ) VALUES ( 1 , 1 , 'celeborn.worker.flusher.buffer.size' , '256K' , 'QUOTA' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ); # TENANT / TENANT_USER level configuration # TENANT : sets buffer size of tenantId1 to 128 KiB # TENANT_USER : sets buffer size of tenantId1 and user1 to 96 KiB INSERT INTO ` celeborn_cluster_tenant_config ` ( ` id ` , ` cluster_id ` , ` tenant_id ` , ` level ` , ` name ` , ` config_key ` , ` config_value ` , ` type ` , ` gmt_create ` , ` gmt_modify ` ) VALUES ( 1 , 1 , 'tenantId1' , 'TENANT' , '' , 'celeborn.worker.flusher.buffer.size' , '128K' , 'worker' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ), ( 2 , 1 , 'tenantId1' , 'TENANT_USER' , 'user1' , 'celeborn.worker.flusher.buffer.size' , '96K' , 'worker' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ); Rest API In addition to viewing the configurations, Celeborn support REST API available for both master and worker including: /conf : List the conf setting of master and worker. /listDynamicConfigs : List the dynamic configs of master and worker. The API providers of listing configurations refer to Available API providers","title":"Configuration"},{"location":"developers/configuration/#configuration","text":"The configuration of Celeborn is divided into static and dynamic categories, with details provided in the Configuration Guide .","title":"Configuration"},{"location":"developers/configuration/#static-configuration","text":"Static configuration, referred to as CelebornConf , loads configurations from the default file located at $CELEBORN_HOME/conf/celeborn-defaults.conf .","title":"Static Configuration"},{"location":"developers/configuration/#dynamic-configuration","text":"Dynamic configuration allows for changes to be applied at runtime, as necessary, and it takes precedence over the corresponding static configuration in the Celeborn Master and Worker . A configuration key's dynamic nature is indicated by the isDynamic property, as listed in All Configurations . This means that configurations tagged with the dynamic property can be updated and refreshed while Celeborn is running.","title":"Dynamic Configuration"},{"location":"developers/configuration/#config-level","text":"At present dynamic configuration supports various config levels including: SYSTEM : The system configurations. TENANT : The dynamic configurations of tenant id. TENANT_USER : The dynamic configurations of tenant id and username. When applying dynamic configuration, the following is the order of precedence for configuration levels: SYSTEM level configuration takes precedence over static configuration and the default CelebornConf . If the system-level configuration is absent, it will fall back to the static configuration defined in CelebornConf . TENANT level configuration supersedes the SYSTEM level, meaning that configurations specific to a tenant id will override those set at the system level. If tenant-level configuration is absent, it will fall back to the system-level dynamic configuration. TENANT_USER level configuration takes precedence over TENANT level. Configurations specific to both a tenant id and username will override those set at the tenant level. If tenant-user-level configuration is missing, it will fall back to the tenant-level dynamic configuration.","title":"Config Level"},{"location":"developers/configuration/#config-service","text":"The config service provides a configuration management service with a local cache for both static and dynamic configurations. Moreover, ConfigService is a pluggable service interface whose implementation can vary based on different storage backends. The storage backend for ConfigService is specified by the configuration key celeborn.dynamicConfig.store.backend , and it currently supports both filesystem ( FS ) and database ( DB ) as storage backends. If no storage backend is specified, this indicates that the config service is disabled.","title":"Config Service"},{"location":"developers/configuration/#filesystem-config-service","text":"The filesystem config service enables the use of dynamic configuration files, the location of which is set by the configuration key celeborn.dynamicConfig.store.fs.path . The template for the dynamic configuration is as follows: # SYSTEM level configuration - level : SYSTEM config : [ config_key ]: [ config_val ] ... # TENANT level configuration - tenantId : [ tenant_id ] level : TENANT config : [ config_key ]: [ config_val ] ... users : # TENANT_USER level configuration - name : [ name ] config : [ config_key ]: [ config_val ] ... For example, a Celeborn worker celeborn-worker has 10 storage directories or disks and the buffer size is set to 256 KiB. A tenant tenantId1 only uses half of the storage and sets the buffer size to 128 KiB. Meanwhile, a user user1 needs to change the buffer size to 96 KiB at runtime. The example configurations are as follows: # SYSTEM level configuration - level : SYSTEM config : celeborn.worker.flusher.buffer.size : 256K # sets buffer size of worker to 256 KiB # TENANT level configuration - tenantId : tenantId1 level : TENANT config : celeborn.worker.flusher.buffer.size : 128K # sets buffer size of tenantId1 to 128 KiB users : # TENANT_USER level configuration - name : user1 config : celeborn.worker.flusher.buffer.size : 96K # sets buffer size of tenantId1 and user1 to 128 KiB","title":"FileSystem Config Service"},{"location":"developers/configuration/#database-config-service","text":"The database config service updates dynamic configurations stored in the database using the JDBC approach. Configuration settings for the database storage backend are defined by the celeborn.dynamicConfig.store.db.* series of configuration keys. To use the database as a config store backend, it is necessary to create tables for dynamic configurations at the various configuration levels. The sql script for MySQL configuration tables is located under $CELEBORN_HOME/db-scripts directory. After the creation of configuration tables, dynamic configuration of config levels is specified via inserting a configuration record in corresponding config level table. Above example dynamic configurations can be supported via the following sql: CREATE TABLE IF NOT EXISTS celeborn_cluster_info ( id int NOT NULL AUTO_INCREMENT , name varchar ( 255 ) NOT NULL COMMENT 'celeborn cluster name' , namespace varchar ( 255 ) DEFAULT NULL COMMENT 'celeborn cluster namespace' , endpoint varchar ( 255 ) DEFAULT NULL COMMENT 'celeborn cluster endpoint' , gmt_create timestamp NOT NULL , gmt_modify timestamp NOT NULL , PRIMARY KEY ( id ), UNIQUE KEY ` index_cluster_unique_name ` ( ` name ` ) ); # SYSTEM level configuration CREATE TABLE IF NOT EXISTS celeborn_cluster_system_config ( id int NOT NULL AUTO_INCREMENT , cluster_id int NOT NULL , config_key varchar ( 255 ) NOT NULL , config_value varchar ( 255 ) NOT NULL , type varchar ( 255 ) DEFAULT NULL COMMENT 'conf categories, such as quota' , gmt_create timestamp NOT NULL , gmt_modify timestamp NOT NULL , PRIMARY KEY ( id ), UNIQUE KEY ` index_unique_system_config_key ` ( ` cluster_id ` , ` config_key ` ) ); # TENANT / TENANT_USER level configuration CREATE TABLE IF NOT EXISTS celeborn_cluster_tenant_config ( id int NOT NULL AUTO_INCREMENT , cluster_id int NOT NULL , tenant_id varchar ( 255 ) NOT NULL , level varchar ( 255 ) NOT NULL COMMENT 'config level, valid level is TENANT,USER' , name varchar ( 255 ) DEFAULT NULL COMMENT 'tenant sub user' , config_key varchar ( 255 ) NOT NULL , config_value varchar ( 255 ) NOT NULL , type varchar ( 255 ) DEFAULT NULL COMMENT 'conf categories, such as quota' , gmt_create timestamp NOT NULL , gmt_modify timestamp NOT NULL , PRIMARY KEY ( id ), UNIQUE KEY ` index_unique_tenant_config_key ` ( ` cluster_id ` , ` tenant_id ` , ` user ` , ` config_key ` ) ); INSERT INTO celeborn_cluster_info ( ` id ` , ` name ` , ` namespace ` , ` endpoint ` , ` gmt_create ` , ` gmt_modify ` ) VALUES ( 1 , 'default' , 'celeborn-worker' , 'celeborn-namespace.endpoint.com' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ); # SYSTEM level configuration # sets buffer size of celeborn - worker to 256 KiB INSERT INTO ` celeborn_cluster_system_config ` ( ` id ` , ` cluster_id ` , ` config_key ` , ` config_value ` , ` type ` , ` gmt_create ` , ` gmt_modify ` ) VALUES ( 1 , 1 , 'celeborn.worker.flusher.buffer.size' , '256K' , 'QUOTA' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ); # TENANT / TENANT_USER level configuration # TENANT : sets buffer size of tenantId1 to 128 KiB # TENANT_USER : sets buffer size of tenantId1 and user1 to 96 KiB INSERT INTO ` celeborn_cluster_tenant_config ` ( ` id ` , ` cluster_id ` , ` tenant_id ` , ` level ` , ` name ` , ` config_key ` , ` config_value ` , ` type ` , ` gmt_create ` , ` gmt_modify ` ) VALUES ( 1 , 1 , 'tenantId1' , 'TENANT' , '' , 'celeborn.worker.flusher.buffer.size' , '128K' , 'worker' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' ), ( 2 , 1 , 'tenantId1' , 'TENANT_USER' , 'user1' , 'celeborn.worker.flusher.buffer.size' , '96K' , 'worker' , '2024-02-27 22:08:30' , '2024-02-27 22:08:30' );","title":"Database Config Service"},{"location":"developers/configuration/#rest-api","text":"In addition to viewing the configurations, Celeborn support REST API available for both master and worker including: /conf : List the conf setting of master and worker. /listDynamicConfigs : List the dynamic configs of master and worker. The API providers of listing configurations refer to Available API providers","title":"Rest API"},{"location":"developers/faulttolerant/","text":"Fault Tolerant This article describes the detailed design of Celeborn's fault-tolerant. In addition to data replication to handle Worker lost, Celeborn tries to handle exceptions during shuffle as much as possible, especially the following: When PushData / PushMergedData fail When fetch chunk fails When disk is unhealthy or reaching limit This article is based on ReducePartition . Handle PushData Failure The detailed description of push data can be found in PushData . Push data can fail for various reasons, i.e. CPU high load, network fluctuation, JVM GC, Worker lost. Celeborn does not eagerly consider Worker lost when push data fails, instead it considers it as temporary unavailable, and asks for another (pair of) PartitionLocation (s) on different Worker (s) to continue pushing. The process is called Revive : Handling PushMergedData failure is similar but more complex. Currently, PushMergedData is in all-or-nothing fashion, meaning either all data batches in the request succeed or all fail. Partial success is not supported yet. Upon PushMergedData failure, ShuffleClient first unpacks and revives for every data batch. Notice that previously all data batches in PushMergedData have the same primary and replica (if any) destination, after reviving new PartitionLocation s can spread across multiple Worker s. Then ShuffleClient groups the new PartitionLocations in the same way as before, resulting in multiple PushMergedData requests, then send them to their destinations. Celeborn detects data lost when processing CommitFiles (See Worker ). Celeborn considers no DataLost if and only if every PartitionLocation has succeeded to commit at least one replica (if replication is turned off, there is only one replica for each PartitionLocation ). When a Worker is down, all PartitionLocation s on the Worker will be revived, causing Revive RPC flood to LifecycleManager . To alleviate this, ShuffleClient batches all Revive requests before sending to LifecycleManager : Handle Fetch Failure As ReducePartition describes, data file consists of chunks, ShuffleClient asks for a chunk once a time. ShuffleClient defines the max number of retries for each replica(defaults to 3). When fetch chunk fails, ShuffleClient will try another replica (in case where replication is off, retry the same one). If the max retry number exceeds, ShuffleClient gives up retrying and throws Exception. Disk Check Worker periodically checks disk health and usage. When health check fails, Worker isolates the disk and will not allocate slots on it until it becomes healthy again. Similarly, if usable space goes less than threshold (defaults to 5GiB), Worker will not allocate slots on it. In addition, to avoid exceeding space, Worker will trigger HARD_SPLIT for all PartitionLocation s on the disk to avoid file size growth. Exactly Once It can happen that Worker successfully receives and writes a data batch but fails to send ACK to ShuffleClient , or primary successfully receives and writes a data batch but replica fails. Also, different task attempts (i.e. speculative execution) will push the same data twice. In a word, it can happen that the same data batch are duplicated across PartitionLocation splits. To guarantee exactly once, Celeborn ensures no data is lost, and no duplicate read: For each data batch, ShuffleClient adds a (Map Id, Attempt Id, Batch Id) header, in which Batch Id is a unique id for the data batch in the map attempt LifecycleManager keeps all PartitionLocation s with the same partition id For each PartitionLocation split, at least one replica is successfully committed before shuffle read LifecycleManager records the successful task attempt for each map id, and only data from that attempt is read for the map id ShuffleClient discards data batches with a batch id that it has already read","title":"Fault Tolerant"},{"location":"developers/faulttolerant/#fault-tolerant","text":"This article describes the detailed design of Celeborn's fault-tolerant. In addition to data replication to handle Worker lost, Celeborn tries to handle exceptions during shuffle as much as possible, especially the following: When PushData / PushMergedData fail When fetch chunk fails When disk is unhealthy or reaching limit This article is based on ReducePartition .","title":"Fault Tolerant"},{"location":"developers/faulttolerant/#handle-pushdata-failure","text":"The detailed description of push data can be found in PushData . Push data can fail for various reasons, i.e. CPU high load, network fluctuation, JVM GC, Worker lost. Celeborn does not eagerly consider Worker lost when push data fails, instead it considers it as temporary unavailable, and asks for another (pair of) PartitionLocation (s) on different Worker (s) to continue pushing. The process is called Revive : Handling PushMergedData failure is similar but more complex. Currently, PushMergedData is in all-or-nothing fashion, meaning either all data batches in the request succeed or all fail. Partial success is not supported yet. Upon PushMergedData failure, ShuffleClient first unpacks and revives for every data batch. Notice that previously all data batches in PushMergedData have the same primary and replica (if any) destination, after reviving new PartitionLocation s can spread across multiple Worker s. Then ShuffleClient groups the new PartitionLocations in the same way as before, resulting in multiple PushMergedData requests, then send them to their destinations. Celeborn detects data lost when processing CommitFiles (See Worker ). Celeborn considers no DataLost if and only if every PartitionLocation has succeeded to commit at least one replica (if replication is turned off, there is only one replica for each PartitionLocation ). When a Worker is down, all PartitionLocation s on the Worker will be revived, causing Revive RPC flood to LifecycleManager . To alleviate this, ShuffleClient batches all Revive requests before sending to LifecycleManager :","title":"Handle PushData Failure"},{"location":"developers/faulttolerant/#handle-fetch-failure","text":"As ReducePartition describes, data file consists of chunks, ShuffleClient asks for a chunk once a time. ShuffleClient defines the max number of retries for each replica(defaults to 3). When fetch chunk fails, ShuffleClient will try another replica (in case where replication is off, retry the same one). If the max retry number exceeds, ShuffleClient gives up retrying and throws Exception.","title":"Handle Fetch Failure"},{"location":"developers/faulttolerant/#disk-check","text":"Worker periodically checks disk health and usage. When health check fails, Worker isolates the disk and will not allocate slots on it until it becomes healthy again. Similarly, if usable space goes less than threshold (defaults to 5GiB), Worker will not allocate slots on it. In addition, to avoid exceeding space, Worker will trigger HARD_SPLIT for all PartitionLocation s on the disk to avoid file size growth.","title":"Disk Check"},{"location":"developers/faulttolerant/#exactly-once","text":"It can happen that Worker successfully receives and writes a data batch but fails to send ACK to ShuffleClient , or primary successfully receives and writes a data batch but replica fails. Also, different task attempts (i.e. speculative execution) will push the same data twice. In a word, it can happen that the same data batch are duplicated across PartitionLocation splits. To guarantee exactly once, Celeborn ensures no data is lost, and no duplicate read: For each data batch, ShuffleClient adds a (Map Id, Attempt Id, Batch Id) header, in which Batch Id is a unique id for the data batch in the map attempt LifecycleManager keeps all PartitionLocation s with the same partition id For each PartitionLocation split, at least one replica is successfully committed before shuffle read LifecycleManager records the successful task attempt for each map id, and only data from that attempt is read for the map id ShuffleClient discards data batches with a batch id that it has already read","title":"Exactly Once"},{"location":"developers/glutensupport/","text":"Gluten Support Velox Backend Gluten with velox backend supports Celeborn as remote shuffle service. Below introduction is used to enable this feature. First refer to Get Started With Velox to build Gluten with velox backend. When compiling the Gluten Java module, it's required to enable celeborn profile, as follows: mvn clean package -Pbackends-velox -Pspark-3.3 -Pceleborn -DskipTests Then add the Gluten and Spark Celeborn Client packages to your Spark application's classpath(usually add them into $SPARK_HOME/jars ). Celeborn: celeborn-client-spark-3-shaded_2.12-[celebornVersion].jar Gluten: gluten-velox-bundle-spark3.x_2.12-xx-xx-SNAPSHOT.jar , gluten-thirdparty-lib-xx.jar ClickHouse Backend Gluten with clickhouse backend supports Celeborn as remote shuffle service. Below introduction is used to enable this feature. First refer to Get Started With ClickHouse to build Gluten with clickhouse backend. When compiling the Gluten Java module, it's required to enable celeborn profile, as follows: mvn clean package -Pbackends-clickhouse -Pspark-3.3 -Pceleborn -DskipTests Then add the Spark Celeborn Client packages to your Spark application's classpath(usually add them into $SPARK_HOME/jars ). Celeborn: celeborn-client-spark-3-shaded_2.12-[celebornVersion].jar Gluten Configuration Currently, to use Gluten following configurations are required in spark-defaults.conf . spark.shuffle.manager org.apache.spark.shuffle.gluten.celeborn.CelebornShuffleManager # celeborn master spark.celeborn.master.endpoints clb-master:9097 # we recommend set `spark.celeborn.push.replicate.enabled` to true to enable server-side data replication # If you have only one worker, this setting must be false spark.celeborn.client.push.replicate.enabled true spark.celeborn.client.spark.shuffle.writer hash # This is not necessary if your Spark external shuffle service is Spark 3.1 or newer spark.shuffle.service.enabled false spark.sql.adaptive.localShuffleReader.enabled false # If you want to use dynamic resource allocation, # please refer to this URL (https://github.com/apache/celeborn/tree/main/assets/spark-patch) to apply the patch into your own Spark. spark.dynamicAllocation.enabled false Availability Celeborn Version Available in Gluten? < 0.2.0 No >= 0.2.0 Yes","title":"Gluten Support"},{"location":"developers/glutensupport/#gluten-support","text":"","title":"Gluten Support"},{"location":"developers/glutensupport/#velox-backend","text":"Gluten with velox backend supports Celeborn as remote shuffle service. Below introduction is used to enable this feature. First refer to Get Started With Velox to build Gluten with velox backend. When compiling the Gluten Java module, it's required to enable celeborn profile, as follows: mvn clean package -Pbackends-velox -Pspark-3.3 -Pceleborn -DskipTests Then add the Gluten and Spark Celeborn Client packages to your Spark application's classpath(usually add them into $SPARK_HOME/jars ). Celeborn: celeborn-client-spark-3-shaded_2.12-[celebornVersion].jar Gluten: gluten-velox-bundle-spark3.x_2.12-xx-xx-SNAPSHOT.jar , gluten-thirdparty-lib-xx.jar","title":"Velox Backend"},{"location":"developers/glutensupport/#clickhouse-backend","text":"Gluten with clickhouse backend supports Celeborn as remote shuffle service. Below introduction is used to enable this feature. First refer to Get Started With ClickHouse to build Gluten with clickhouse backend. When compiling the Gluten Java module, it's required to enable celeborn profile, as follows: mvn clean package -Pbackends-clickhouse -Pspark-3.3 -Pceleborn -DskipTests Then add the Spark Celeborn Client packages to your Spark application's classpath(usually add them into $SPARK_HOME/jars ). Celeborn: celeborn-client-spark-3-shaded_2.12-[celebornVersion].jar","title":"ClickHouse Backend"},{"location":"developers/glutensupport/#gluten-configuration","text":"Currently, to use Gluten following configurations are required in spark-defaults.conf . spark.shuffle.manager org.apache.spark.shuffle.gluten.celeborn.CelebornShuffleManager # celeborn master spark.celeborn.master.endpoints clb-master:9097 # we recommend set `spark.celeborn.push.replicate.enabled` to true to enable server-side data replication # If you have only one worker, this setting must be false spark.celeborn.client.push.replicate.enabled true spark.celeborn.client.spark.shuffle.writer hash # This is not necessary if your Spark external shuffle service is Spark 3.1 or newer spark.shuffle.service.enabled false spark.sql.adaptive.localShuffleReader.enabled false # If you want to use dynamic resource allocation, # please refer to this URL (https://github.com/apache/celeborn/tree/main/assets/spark-patch) to apply the patch into your own Spark. spark.dynamicAllocation.enabled false","title":"Gluten Configuration"},{"location":"developers/glutensupport/#availability","text":"Celeborn Version Available in Gluten? < 0.2.0 No >= 0.2.0 Yes","title":"Availability"},{"location":"developers/helm-charts/","text":"Helm Charts Run Helm Unit Tests To run unit tests against Helm charts, first you need to install the helm unittest plugin as follows: helm plugin install https://github.com/helm-unittest/helm-unittest.git --version = 0 .5.1 For detailed information about how to write helm chart unit tests, please refer helm-unittest/helm-unittest . When you want to modify the chart templates or values, remember to update the related unit tests as well, otherwise the github CI may fail. Unit tests are placed under charts/celeborn/tests directory, and can be running using the following command: $ helm unittest charts/celeborn --file \"tests/**/*_test.yaml\" --strict --debug ### Chart [ celeborn ] charts/celeborn PASS Test Celeborn configmap charts/celeborn/tests/configmap_test.yaml PASS Test Celeborn master pod monitor charts/celeborn/tests/master/podmonitor_test.yaml PASS Test Celeborn master priority class charts/celeborn/tests/master/priorityclass_test.yaml PASS Test Celeborn master service charts/celeborn/tests/master/service_test.yaml PASS Test Celeborn master statefulset charts/celeborn/tests/master/statefulset_test.yaml PASS Test Celeborn worker pod monitor charts/celeborn/tests/worker/podmonitor_test.yaml PASS Test Celeborn worker priority class charts/celeborn/tests/worker/priorityclass_test.yaml PASS Test Celeborn worker service charts/celeborn/tests/worker/service_test.yaml PASS Test Celeborn worker statefulset charts/celeborn/tests/worker/statefulset_test.yaml Charts: 1 passed, 1 total Test Suites: 9 passed, 9 total Tests: 46 passed, 46 total Snapshot: 0 passed, 0 total Time: 177 .518ms","title":"Helm Charts"},{"location":"developers/helm-charts/#helm-charts","text":"","title":"Helm Charts"},{"location":"developers/helm-charts/#run-helm-unit-tests","text":"To run unit tests against Helm charts, first you need to install the helm unittest plugin as follows: helm plugin install https://github.com/helm-unittest/helm-unittest.git --version = 0 .5.1 For detailed information about how to write helm chart unit tests, please refer helm-unittest/helm-unittest . When you want to modify the chart templates or values, remember to update the related unit tests as well, otherwise the github CI may fail. Unit tests are placed under charts/celeborn/tests directory, and can be running using the following command: $ helm unittest charts/celeborn --file \"tests/**/*_test.yaml\" --strict --debug ### Chart [ celeborn ] charts/celeborn PASS Test Celeborn configmap charts/celeborn/tests/configmap_test.yaml PASS Test Celeborn master pod monitor charts/celeborn/tests/master/podmonitor_test.yaml PASS Test Celeborn master priority class charts/celeborn/tests/master/priorityclass_test.yaml PASS Test Celeborn master service charts/celeborn/tests/master/service_test.yaml PASS Test Celeborn master statefulset charts/celeborn/tests/master/statefulset_test.yaml PASS Test Celeborn worker pod monitor charts/celeborn/tests/worker/podmonitor_test.yaml PASS Test Celeborn worker priority class charts/celeborn/tests/worker/priorityclass_test.yaml PASS Test Celeborn worker service charts/celeborn/tests/worker/service_test.yaml PASS Test Celeborn worker statefulset charts/celeborn/tests/worker/statefulset_test.yaml Charts: 1 passed, 1 total Test Suites: 9 passed, 9 total Tests: 46 passed, 46 total Snapshot: 0 passed, 0 total Time: 177 .518ms","title":"Run Helm Unit Tests"},{"location":"developers/integrate/","text":"Integrating Celeborn Overview The core components of Celeborn, i.e. Master , Worker , and Client are all engine irrelevant. Developers can integrate Celeborn with various engines or applications by using or extending Celeborn's Client , as the officially supported plugins for Spark/Flink/MapReduce. This article briefly describes an example of integrating Celeborn into a simple distributed application using Celeborn Client . Background Say we have a distributed application who has two phases: Write phase that parallel tasks write data to some data service, each record is classified into some logical id, say partition id. Read phase that parallel tasks read data from the data service, each task read data from a specified partition id. Suppose the application has failover mechanism so that it's acceptable that when some data is lost the application will rerun tasks. Say developers of this application is searching for a suitable data service, and accidentally finds this article. Step One: Setup Celeborn Cluster First, you need an available Celeborn Cluster. Refer to QuickStart to set up a simple cluster in a single node, or Deploy to set up a multi-node cluster, standalone or on K8s. Step Two: Create LifecycleManager As described in Client , Client is separated into LifecycleManager , which is singleton through an application; and ShuffleClient , which can have multiple instances. Step two is to create a LifecycleManager instance, using the following API: class LifecycleManager ( val appUniqueId : String , val conf : CelebornConf ) appUniqueId is the application id. Celeborn cluster stores, serves, and cleans up data in the granularity of (application id, shuffle id) conf is an object of CelebornConf . The only required configuration is the address of Celeborn Master . For the thorough description of configs, refer to Configuration The example java code to create an LifecycleManager instance is as follows: CelebornConf celebornConf = new CelebornConf (); celebornConf . set ( \"celeborn.master.endpoints\" , \"<Master IP>:<Master Port>\" ); LifecycleManager lm = new LifecycleManager ( \"myApp\" , celebornConf ); LifecycleManager object automatically starts necessary service after creation, so there is no need to call other APIs to initialize it. You can get LifecycleManager 's address after creating it, which is needed to create ShuffleClient . String host = lm . getHost (); int = lm . getPort (); Step Three: Create ShuffleClient With LifecycleManager 's host and port, you can create ShuffleClient using the following API: public static ShuffleClient get ( String appUniqueId , String host , int port , CelebornConf conf , UserIdentifier userIdentifier ) appUniqueId is the application id, same as above. host is the host of LifecycleManager port is the port of LifecycleManager conf is an object of CelebornConf , safe to pass an empty object userIdentifier specifies user identity, safe to pass null You can create a ShuffleClient object using the following code: ShuffleClient shuffleClient = ShuffleClient . get ( \"myApp\" , < LifecycleManager Host > , < LifecycleManager Port > , new CelebornConf (), null ); This method returns a singleton ShuffleClientImpl object, and it's recommended to use this way as ShuffleClientImpl maintains status and reuses resource across all shuffles. To make it work, you have to ensure that the LifecycleManager 's host and port are reachable. In practice, one ShuffleClient instance is created in each Executor process of Spark, or in each TaskManager process of Flink. Step Four: Push Data You can then push data with ShuffleClient with pushData , like the following: int bytesWritten = shuffleClient . pushData ( shuffleId , mapId , attemptId , partitionId , data , 0 , length , numMappers , numPartitions ); Each call of pushData passes a byte array containing data from the same partition id. In addition to specifying the shuffleId, mapId, attemptId that the data belongs, ShuffleClient should also specify the number of mappers and the number of partitions for Lazy Register . After the map task finishes, ShuffleClient should call mapperEnd to tell LifecycleManager that the map task finishes pushing its data: public abstract void mapperEnd ( int shuffleId , int mapId , int attempted , int numMappers ) shuffleId shuffle id of the current task mapId map id of the current task attemptId attempt id of the current task numMappers number of map ids in this shuffle Step Five: Read Data After all tasks successfully called mapperEnd , you can start reading data from some partition id, using the readPartition API , as the following code: InputStream inputStream = shuffleClient . readPartition ( shuffleId , partitionId , attemptId , startMapIndex , endMapIndex ); int byte = inputstream . read (); For simplicity, to read the whole data from the partition, you can pass 0 and Integer.MAX_VALUE to startMapIndex and endMapIndex . This method will create an InputStream for the data, and guarantees no data lost and no duplicate reading, else exception will be thrown. Step Six: Clean Up After the shuffle finishes, you can call LifecycleManager.unregisterShuffle to clean up resources related to the shuffle: lm . unregisterShuffle ( 0 ); It's safe not to call unregisterShuffle , because Celeborn Master recognizes application finish by heartbeat timeout, and will self-clean resources in the cluster.","title":"Integrating Celeborn"},{"location":"developers/integrate/#integrating-celeborn","text":"","title":"Integrating Celeborn"},{"location":"developers/integrate/#overview","text":"The core components of Celeborn, i.e. Master , Worker , and Client are all engine irrelevant. Developers can integrate Celeborn with various engines or applications by using or extending Celeborn's Client , as the officially supported plugins for Spark/Flink/MapReduce. This article briefly describes an example of integrating Celeborn into a simple distributed application using Celeborn Client .","title":"Overview"},{"location":"developers/integrate/#background","text":"Say we have a distributed application who has two phases: Write phase that parallel tasks write data to some data service, each record is classified into some logical id, say partition id. Read phase that parallel tasks read data from the data service, each task read data from a specified partition id. Suppose the application has failover mechanism so that it's acceptable that when some data is lost the application will rerun tasks. Say developers of this application is searching for a suitable data service, and accidentally finds this article.","title":"Background"},{"location":"developers/integrate/#step-one-setup-celeborn-cluster","text":"First, you need an available Celeborn Cluster. Refer to QuickStart to set up a simple cluster in a single node, or Deploy to set up a multi-node cluster, standalone or on K8s.","title":"Step One: Setup Celeborn Cluster"},{"location":"developers/integrate/#step-two-create-lifecyclemanager","text":"As described in Client , Client is separated into LifecycleManager , which is singleton through an application; and ShuffleClient , which can have multiple instances. Step two is to create a LifecycleManager instance, using the following API: class LifecycleManager ( val appUniqueId : String , val conf : CelebornConf ) appUniqueId is the application id. Celeborn cluster stores, serves, and cleans up data in the granularity of (application id, shuffle id) conf is an object of CelebornConf . The only required configuration is the address of Celeborn Master . For the thorough description of configs, refer to Configuration The example java code to create an LifecycleManager instance is as follows: CelebornConf celebornConf = new CelebornConf (); celebornConf . set ( \"celeborn.master.endpoints\" , \"<Master IP>:<Master Port>\" ); LifecycleManager lm = new LifecycleManager ( \"myApp\" , celebornConf ); LifecycleManager object automatically starts necessary service after creation, so there is no need to call other APIs to initialize it. You can get LifecycleManager 's address after creating it, which is needed to create ShuffleClient . String host = lm . getHost (); int = lm . getPort ();","title":"Step Two: Create LifecycleManager"},{"location":"developers/integrate/#step-three-create-shuffleclient","text":"With LifecycleManager 's host and port, you can create ShuffleClient using the following API: public static ShuffleClient get ( String appUniqueId , String host , int port , CelebornConf conf , UserIdentifier userIdentifier ) appUniqueId is the application id, same as above. host is the host of LifecycleManager port is the port of LifecycleManager conf is an object of CelebornConf , safe to pass an empty object userIdentifier specifies user identity, safe to pass null You can create a ShuffleClient object using the following code: ShuffleClient shuffleClient = ShuffleClient . get ( \"myApp\" , < LifecycleManager Host > , < LifecycleManager Port > , new CelebornConf (), null ); This method returns a singleton ShuffleClientImpl object, and it's recommended to use this way as ShuffleClientImpl maintains status and reuses resource across all shuffles. To make it work, you have to ensure that the LifecycleManager 's host and port are reachable. In practice, one ShuffleClient instance is created in each Executor process of Spark, or in each TaskManager process of Flink.","title":"Step Three: Create ShuffleClient"},{"location":"developers/integrate/#step-four-push-data","text":"You can then push data with ShuffleClient with pushData , like the following: int bytesWritten = shuffleClient . pushData ( shuffleId , mapId , attemptId , partitionId , data , 0 , length , numMappers , numPartitions ); Each call of pushData passes a byte array containing data from the same partition id. In addition to specifying the shuffleId, mapId, attemptId that the data belongs, ShuffleClient should also specify the number of mappers and the number of partitions for Lazy Register . After the map task finishes, ShuffleClient should call mapperEnd to tell LifecycleManager that the map task finishes pushing its data: public abstract void mapperEnd ( int shuffleId , int mapId , int attempted , int numMappers ) shuffleId shuffle id of the current task mapId map id of the current task attemptId attempt id of the current task numMappers number of map ids in this shuffle","title":"Step Four: Push Data"},{"location":"developers/integrate/#step-five-read-data","text":"After all tasks successfully called mapperEnd , you can start reading data from some partition id, using the readPartition API , as the following code: InputStream inputStream = shuffleClient . readPartition ( shuffleId , partitionId , attemptId , startMapIndex , endMapIndex ); int byte = inputstream . read (); For simplicity, to read the whole data from the partition, you can pass 0 and Integer.MAX_VALUE to startMapIndex and endMapIndex . This method will create an InputStream for the data, and guarantees no data lost and no duplicate reading, else exception will be thrown.","title":"Step Five: Read Data"},{"location":"developers/integrate/#step-six-clean-up","text":"After the shuffle finishes, you can call LifecycleManager.unregisterShuffle to clean up resources related to the shuffle: lm . unregisterShuffle ( 0 ); It's safe not to call unregisterShuffle , because Celeborn Master recognizes application finish by heartbeat timeout, and will self-clean resources in the cluster.","title":"Step Six: Clean Up"},{"location":"developers/jvmprofiler/","text":"JVM Profiler Since version 0.5.0, Celeborn supports JVM sampling profiler to capture CPU and memory profiles. This article provides a detailed guide of Celeborn Worker 's code profiling. Worker Code Profiling The JVM profiler enables code profiling of workers based on the async profiler , a low overhead sampling profiler. This allows a Worker instance to capture CPU and memory profiles for Worker which is later analyzed for performance issues. The profiler captures Java Flight Recorder (jfr) files for each worker that can be read by tools like Java Mission Control and Intellij etc. The profiler writes the jfr files to the Worker 's working directory in the Worker 's local file system and the files can grow to be large, so it is advisable that the Worker machines have adequate storage. Code profiling is currently only supported for Linux (x64) Linux (arm 64) Linux (musl, x64) MacOS To get maximum profiling information set the following jvm options for the Worker : -XX:+UnlockDiagnosticVMOptions -XX:+DebugNonSafepoints -XX:+PreserveFramePointer For more information on async_profiler see the Async Profiler Manual . To enable code profiling, enable the code profiling in the configuration. celeborn.worker.jvmProfiler.enabled true For more configuration of code profiling refer to celeborn.worker.jvmProfiler.* . Profiling Configuration Example celeborn.worker.jvmProfiler.enabled true celeborn.worker.jvmProfiler.options event = wall,interval=10ms,alloc=2m,lock=10ms,chunktime=300s","title":"JVM Profiler"},{"location":"developers/jvmprofiler/#jvm-profiler","text":"Since version 0.5.0, Celeborn supports JVM sampling profiler to capture CPU and memory profiles. This article provides a detailed guide of Celeborn Worker 's code profiling.","title":"JVM Profiler"},{"location":"developers/jvmprofiler/#worker-code-profiling","text":"The JVM profiler enables code profiling of workers based on the async profiler , a low overhead sampling profiler. This allows a Worker instance to capture CPU and memory profiles for Worker which is later analyzed for performance issues. The profiler captures Java Flight Recorder (jfr) files for each worker that can be read by tools like Java Mission Control and Intellij etc. The profiler writes the jfr files to the Worker 's working directory in the Worker 's local file system and the files can grow to be large, so it is advisable that the Worker machines have adequate storage. Code profiling is currently only supported for Linux (x64) Linux (arm 64) Linux (musl, x64) MacOS To get maximum profiling information set the following jvm options for the Worker : -XX:+UnlockDiagnosticVMOptions -XX:+DebugNonSafepoints -XX:+PreserveFramePointer For more information on async_profiler see the Async Profiler Manual . To enable code profiling, enable the code profiling in the configuration. celeborn.worker.jvmProfiler.enabled true For more configuration of code profiling refer to celeborn.worker.jvmProfiler.* .","title":"Worker Code Profiling"},{"location":"developers/jvmprofiler/#profiling-configuration-example","text":"celeborn.worker.jvmProfiler.enabled true celeborn.worker.jvmProfiler.options event = wall,interval=10ms,alloc=2m,lock=10ms,chunktime=300s","title":"Profiling Configuration Example"},{"location":"developers/lifecyclemanager/","text":"LifecycleManager Overview LifecycleManager maintains information of each shuffle for the application: All active shuffle ids Worker s that are serving each shuffle, and what PartitionLocation s are on each Worker Status of each shuffle, i.e. not committed, committing, committed, data lost, expired The newest PartitionLocation with the largest epoch of each partition id User identifier for this application Also, LifecycleManager handles control messages with ShuffleClient and Celeborn Master , typically, it receives requests from ShuffleClient : RegisterShuffle Revive/PartitionSplit MapperEnd/StageEnd GetReducerFileGroup to handle the requests, LifecycleManager will send requests to Master and Worker s: Heartbeat to Master RequestSlots to Master UnregisterShuffle to Master ReserveSlots to Worker CommitFiles to Worker DestroyWorkerSlots to Worker RegisterShuffle As described in PushData , ShuffleClient lazily send RegisterShuffle to LifecycleManager, so many concurrent requests will be sent to LifecycleManager . To ensure only one request for each shuffle is handled, LifecycleManager puts tail requests in a set and only let go the first request. When the first request finishes, LifecycleManager responds to all cached requests. The process of handling RegisterShuffle is as follows: LifecycleManager sends RequestSlots to Master , Master allocates slots for the shuffle, as Here describes. Upon receiving slots allocation result, LifecycleManager sends ReserveSlots to all Workers s allocated in parallel. Worker s then select a disk and initialize for each PartitionLocation , see Here . After all related Worker s successfully reserved slots, LifecycleManager stores the shuffle information in memory and responds to all pending and future requests. Revive/PartitionSplit Celeborn handles push data failure in a so-called Revive mechanism, see Here . Similar to Split , they both asks LifecycleManager for a new epoch of PartitionLocation for future data pushing. Upon receiving Revive/PartitionSplit, LifecycleManager first checks whether it has a newer epoch locally, if so it just responds the newer one. If not, like handling RegisterShuffle, it puts tail requests for the same partition id in a set and only let go the first one. Unlike RegisterShuffle, LifecycleManager does not send RequestSlots to Master to ask for new Worker s. Instead, it randomly picks Worker s from local Worker list, excluding the failing ones. This design is to avoid too many RPCs to Master . Then LifecycleManager sends ReserveSlots to the picked Worker s. When success, it responds the new PartitionLocation s to ShuffleClient s. MapperEnd/StageEnd Celeborn needs to known when shuffle write stage ends to persist shuffle data, check if any data lost, and prepare for shuffle read. Many compute engines do not signal such event (for example, Spark's ShuffleManager does not have such API), Celeborn has to recognize that itself. To achieve this, Celeborn requires ShuffleClient to specify the number of map tasks in RegisterShuffle request, and send MapperEnd request to LifecycleManager when a map task succeeds. When MapperEnd are received for every map id, LifecycleManager knows that the shuffle write stage ends, and sends CommitFiles to related Worker s. For many compute engines, a map task may launch multiple attempts (i.e. speculative execution), and the engine chooses one of them as successful attempt. However, there is no way for Celeborn to know about the chosen attempt. Instead, LifecycleManager records the first attempt sending MapperEnd as the success one for each map task, and ignores other attempts. This is correct because compute engines guarantee that all attempts for a map task generate the same output data. Upon receiving CommitFiles, Worker s flush buffered data to files and responds the succeeded and failed PartitionLocation s to LifecycleManager , see Here . LifecycleManager then checks if any of PartitionLocation loses both primary and replica data (mark data lost if so), and stores the information in memory. GetReducerFileGroup Reduce task asks LifecycleManager for PartitionLocation s of each partition id to read data. To reduce the number of RPCs, ShuffleClient asks for the mapping from all partition ids to their PartitionLocation s and caches in memory, through GetReducerFileGroup request Upon receiving the request, LifecycleManager responds the cached mapping or indicates data lost. Heartbeat to Master LifecycleManager periodically sends heartbeat to Master , piggybacking the following information: Bytes and files written by the application, used to calculate estimated partition size, see Here Worker list that LifecycleManager wants Master to tell status UnregisterShuffle When compute engines tells Celeborn that some shuffle is complete (i.e. through unregisterShuffle for Spark), LifecycleManager first checks and waits for write stage end, then put the shuffle id into unregistered set, after some expire time it removes the id and sends UnregisterShuffle to Master for cleanup, see Here DestroyWorkerSlots Normally, Worker s cleanup resources for PartitionLocation s after notified shuffle unregistered. In some abnormal cases, Master will send DestroyWorkerSlots to early cleanup, for example if some Worker s fail to reserve slots, LifecycleManager will tell the successfully reserved Worker s to release the slots. Batch RPCs Some RPCs are of high frequent, for example Revive/PartitionSplit, CommitFiles, DestroyWorkerSlots. To reduce the number of RPCs, LifecycleManager batches the same kind of RPCs and periodically checks and sends to Master through a dedicated thread. Users can enable and tune batch RPC through the following configs: celeborn.client.shuffle.batch*","title":"LifecycleManager"},{"location":"developers/lifecyclemanager/#lifecyclemanager","text":"","title":"LifecycleManager"},{"location":"developers/lifecyclemanager/#overview","text":"LifecycleManager maintains information of each shuffle for the application: All active shuffle ids Worker s that are serving each shuffle, and what PartitionLocation s are on each Worker Status of each shuffle, i.e. not committed, committing, committed, data lost, expired The newest PartitionLocation with the largest epoch of each partition id User identifier for this application Also, LifecycleManager handles control messages with ShuffleClient and Celeborn Master , typically, it receives requests from ShuffleClient : RegisterShuffle Revive/PartitionSplit MapperEnd/StageEnd GetReducerFileGroup to handle the requests, LifecycleManager will send requests to Master and Worker s: Heartbeat to Master RequestSlots to Master UnregisterShuffle to Master ReserveSlots to Worker CommitFiles to Worker DestroyWorkerSlots to Worker","title":"Overview"},{"location":"developers/lifecyclemanager/#registershuffle","text":"As described in PushData , ShuffleClient lazily send RegisterShuffle to LifecycleManager, so many concurrent requests will be sent to LifecycleManager . To ensure only one request for each shuffle is handled, LifecycleManager puts tail requests in a set and only let go the first request. When the first request finishes, LifecycleManager responds to all cached requests. The process of handling RegisterShuffle is as follows: LifecycleManager sends RequestSlots to Master , Master allocates slots for the shuffle, as Here describes. Upon receiving slots allocation result, LifecycleManager sends ReserveSlots to all Workers s allocated in parallel. Worker s then select a disk and initialize for each PartitionLocation , see Here . After all related Worker s successfully reserved slots, LifecycleManager stores the shuffle information in memory and responds to all pending and future requests.","title":"RegisterShuffle"},{"location":"developers/lifecyclemanager/#revivepartitionsplit","text":"Celeborn handles push data failure in a so-called Revive mechanism, see Here . Similar to Split , they both asks LifecycleManager for a new epoch of PartitionLocation for future data pushing. Upon receiving Revive/PartitionSplit, LifecycleManager first checks whether it has a newer epoch locally, if so it just responds the newer one. If not, like handling RegisterShuffle, it puts tail requests for the same partition id in a set and only let go the first one. Unlike RegisterShuffle, LifecycleManager does not send RequestSlots to Master to ask for new Worker s. Instead, it randomly picks Worker s from local Worker list, excluding the failing ones. This design is to avoid too many RPCs to Master . Then LifecycleManager sends ReserveSlots to the picked Worker s. When success, it responds the new PartitionLocation s to ShuffleClient s.","title":"Revive/PartitionSplit"},{"location":"developers/lifecyclemanager/#mapperendstageend","text":"Celeborn needs to known when shuffle write stage ends to persist shuffle data, check if any data lost, and prepare for shuffle read. Many compute engines do not signal such event (for example, Spark's ShuffleManager does not have such API), Celeborn has to recognize that itself. To achieve this, Celeborn requires ShuffleClient to specify the number of map tasks in RegisterShuffle request, and send MapperEnd request to LifecycleManager when a map task succeeds. When MapperEnd are received for every map id, LifecycleManager knows that the shuffle write stage ends, and sends CommitFiles to related Worker s. For many compute engines, a map task may launch multiple attempts (i.e. speculative execution), and the engine chooses one of them as successful attempt. However, there is no way for Celeborn to know about the chosen attempt. Instead, LifecycleManager records the first attempt sending MapperEnd as the success one for each map task, and ignores other attempts. This is correct because compute engines guarantee that all attempts for a map task generate the same output data. Upon receiving CommitFiles, Worker s flush buffered data to files and responds the succeeded and failed PartitionLocation s to LifecycleManager , see Here . LifecycleManager then checks if any of PartitionLocation loses both primary and replica data (mark data lost if so), and stores the information in memory.","title":"MapperEnd/StageEnd"},{"location":"developers/lifecyclemanager/#getreducerfilegroup","text":"Reduce task asks LifecycleManager for PartitionLocation s of each partition id to read data. To reduce the number of RPCs, ShuffleClient asks for the mapping from all partition ids to their PartitionLocation s and caches in memory, through GetReducerFileGroup request Upon receiving the request, LifecycleManager responds the cached mapping or indicates data lost.","title":"GetReducerFileGroup"},{"location":"developers/lifecyclemanager/#heartbeat-to-master","text":"LifecycleManager periodically sends heartbeat to Master , piggybacking the following information: Bytes and files written by the application, used to calculate estimated partition size, see Here Worker list that LifecycleManager wants Master to tell status","title":"Heartbeat to Master"},{"location":"developers/lifecyclemanager/#unregistershuffle","text":"When compute engines tells Celeborn that some shuffle is complete (i.e. through unregisterShuffle for Spark), LifecycleManager first checks and waits for write stage end, then put the shuffle id into unregistered set, after some expire time it removes the id and sends UnregisterShuffle to Master for cleanup, see Here","title":"UnregisterShuffle"},{"location":"developers/lifecyclemanager/#destroyworkerslots","text":"Normally, Worker s cleanup resources for PartitionLocation s after notified shuffle unregistered. In some abnormal cases, Master will send DestroyWorkerSlots to early cleanup, for example if some Worker s fail to reserve slots, LifecycleManager will tell the successfully reserved Worker s to release the slots.","title":"DestroyWorkerSlots"},{"location":"developers/lifecyclemanager/#batch-rpcs","text":"Some RPCs are of high frequent, for example Revive/PartitionSplit, CommitFiles, DestroyWorkerSlots. To reduce the number of RPCs, LifecycleManager batches the same kind of RPCs and periodically checks and sends to Master through a dedicated thread. Users can enable and tune batch RPC through the following configs: celeborn.client.shuffle.batch*","title":"Batch RPCs"},{"location":"developers/master/","text":"Master The main functions of Celeborn Master are: Maintain overall status of Celeborn cluster Maintain active shuffles Pursue High Availability Allocate slots for every shuffle according to cluster status Maintain Cluster Status Upon start, Worker will register itself to Master . After that, Worker periodically sends heartbeat to Master , carrying the following information: Disk status for each disk on the Worker Active shuffle id list served on the Worker The disk status contains the following information: Health status Usable space Active slots Flush/Fetch speed in the last time window When a Worker 's heartbeat times out, Master will consider it lost and removes it. If in the future Master receives heartbeat from an unknown Worker , it tells the Worker to register itself. When Master finds all disks in a Worker unavailable, it excludes the Worker from allocating slots until future heartbeat renews the disk status. Upon graceful shut down, Worker sends ReportWorkerUnavailable to Master . Master puts it in shutdown-workers list. If in the future Master receives register request from that worker again, it removes it from the list. Upon decommission or immediately shut down, Worker sends WorkerLost to Master , Master just removes the Worker information. Maintain Active Shuffles Application failure is common, Celeborn needs a way to decide whether an app is alive to clean up resource. To achieve this, LifecycleManager periodically sends heartbeat to Master . If Master finds an app's heartbeat times out, it considers the app fails, even though the app resends heartbeat in the future. Master keeps all shuffle ids it has allocated slots for. Upon app heartbeat timeout or receiving UnregisterShuffle, it removes the related shuffle ids. Upon receiving heartbeat from Worker , Master compares local shuffle ids with Worker 's, and tells the Worker to clean up the unknown shuffles. Heartbeat for LifecycleManager also carries total file count and bytes written by the app. Master calculates estimated file size by Sum(bytes) / Sum(files) every 10 minutes using the newest metrics. To resist from impact of small files, only files larger than threshold (defaults to 8MiB) will be considered. High Availability Celeborn achieves Master HA through Raft. Practically, Master replicates cluster and shuffle information among multiple participants of Ratis . Any state-changing RPC will only be ACKed after the leader replicates logs to the majority of participants. Slots Allocation Upon receiving RequestSlots , Master allocates a (pair of) slot for each PartitionLocation of the shuffle. As Master maintains all disks' status of all Worker s, it can leverage that information to achieve better load balance. Currently, Celeborn supports two allocation strategies: Round Robin Load Aware For both strategies, Master will only allocate slots on active Worker s with available disks. During the allocation process, Master also simulates the space usage. For example, say a disk's usable space is 1GiB, and the estimated file size for each PartitionLocation is 64MiB, then at most 16 slots will be allocated on that disk. Round Robin Round Robin is the simplest allocation strategy. The basic idea is: Calculate available slots that can be allocated on each disk Allocate slots among all Worker s and all disks in a round-robin fashion, decrement one after allocating, and exclude if no slots available on a disk or Worker If the cluster's total available slots is not enough, re-run the algorithm for un-allocated slots as if each disk has infinite capacity Load Aware For heterogeneous clusters, Worker s may have different CPU/disk/network performance, so it's necessary to allocate different workloads based on metrics. Currently, Celeborn allocates slots on disks based on flush and fetch performance in the last time window. As mentioned before, disk status in heartbeat from Worker contains flush and fetch speed. Master put all available disks into different groups based on performance metrics, then assign slots into different groups in a gradient descent way. Inside each group, how many slots should be assigned on each disk is calculated according to their usable space. For example, totally four disks are put into two groups with gradient 0.5, say I want to allocate 1500 slots, then Master will assign the faster group 1000 slots, and the slower group 500 slots. Say the two disks in faster group have 1GiB and 3GiB space, then they will be assigned 250 and 750 slots respectively.","title":"Overview"},{"location":"developers/master/#master","text":"The main functions of Celeborn Master are: Maintain overall status of Celeborn cluster Maintain active shuffles Pursue High Availability Allocate slots for every shuffle according to cluster status","title":"Master"},{"location":"developers/master/#maintain-cluster-status","text":"Upon start, Worker will register itself to Master . After that, Worker periodically sends heartbeat to Master , carrying the following information: Disk status for each disk on the Worker Active shuffle id list served on the Worker The disk status contains the following information: Health status Usable space Active slots Flush/Fetch speed in the last time window When a Worker 's heartbeat times out, Master will consider it lost and removes it. If in the future Master receives heartbeat from an unknown Worker , it tells the Worker to register itself. When Master finds all disks in a Worker unavailable, it excludes the Worker from allocating slots until future heartbeat renews the disk status. Upon graceful shut down, Worker sends ReportWorkerUnavailable to Master . Master puts it in shutdown-workers list. If in the future Master receives register request from that worker again, it removes it from the list. Upon decommission or immediately shut down, Worker sends WorkerLost to Master , Master just removes the Worker information.","title":"Maintain Cluster Status"},{"location":"developers/master/#maintain-active-shuffles","text":"Application failure is common, Celeborn needs a way to decide whether an app is alive to clean up resource. To achieve this, LifecycleManager periodically sends heartbeat to Master . If Master finds an app's heartbeat times out, it considers the app fails, even though the app resends heartbeat in the future. Master keeps all shuffle ids it has allocated slots for. Upon app heartbeat timeout or receiving UnregisterShuffle, it removes the related shuffle ids. Upon receiving heartbeat from Worker , Master compares local shuffle ids with Worker 's, and tells the Worker to clean up the unknown shuffles. Heartbeat for LifecycleManager also carries total file count and bytes written by the app. Master calculates estimated file size by Sum(bytes) / Sum(files) every 10 minutes using the newest metrics. To resist from impact of small files, only files larger than threshold (defaults to 8MiB) will be considered.","title":"Maintain Active Shuffles"},{"location":"developers/master/#high-availability","text":"Celeborn achieves Master HA through Raft. Practically, Master replicates cluster and shuffle information among multiple participants of Ratis . Any state-changing RPC will only be ACKed after the leader replicates logs to the majority of participants.","title":"High Availability"},{"location":"developers/master/#slots-allocation","text":"Upon receiving RequestSlots , Master allocates a (pair of) slot for each PartitionLocation of the shuffle. As Master maintains all disks' status of all Worker s, it can leverage that information to achieve better load balance. Currently, Celeborn supports two allocation strategies: Round Robin Load Aware For both strategies, Master will only allocate slots on active Worker s with available disks. During the allocation process, Master also simulates the space usage. For example, say a disk's usable space is 1GiB, and the estimated file size for each PartitionLocation is 64MiB, then at most 16 slots will be allocated on that disk.","title":"Slots Allocation"},{"location":"developers/master/#round-robin","text":"Round Robin is the simplest allocation strategy. The basic idea is: Calculate available slots that can be allocated on each disk Allocate slots among all Worker s and all disks in a round-robin fashion, decrement one after allocating, and exclude if no slots available on a disk or Worker If the cluster's total available slots is not enough, re-run the algorithm for un-allocated slots as if each disk has infinite capacity","title":"Round Robin"},{"location":"developers/master/#load-aware","text":"For heterogeneous clusters, Worker s may have different CPU/disk/network performance, so it's necessary to allocate different workloads based on metrics. Currently, Celeborn allocates slots on disks based on flush and fetch performance in the last time window. As mentioned before, disk status in heartbeat from Worker contains flush and fetch speed. Master put all available disks into different groups based on performance metrics, then assign slots into different groups in a gradient descent way. Inside each group, how many slots should be assigned on each disk is calculated according to their usable space. For example, totally four disks are put into two groups with gradient 0.5, say I want to allocate 1500 slots, then Master will assign the faster group 1000 slots, and the slower group 500 slots. Say the two disks in faster group have 1GiB and 3GiB space, then they will be assigned 250 and 750 slots respectively.","title":"Load Aware"},{"location":"developers/overview/","text":"Celeborn Architecture This article introduces high level Apache Celeborn\u2122 Architecture. For more detailed description of each module/process, please refer to dedicated articles. Why Celeborn In distributed compute engines, data exchange between compute nodes is common but expensive. The cost comes from the disk and network inefficiency (M * N between Mappers and Reducers) in traditional shuffle frame, as following: Besides inefficiency, traditional shuffle framework requires large local storage in compute node to store shuffle data, thus blocks the adoption of disaggregated architecture. Apache Celeborn solves the problems by reorganizing shuffle data in a more efficient way, and storing the data in a separate service. The high level architecture of Celeborn is as follows: Components Celeborn has three primary components: Master, Worker, and Client. Master manages Celeborn cluster and achieves high availability(HA) based on Raft. Worker processes read-write requests. Client writes/reads data to/from Celeborn cluster, and manages shuffle metadata for the application. In most distributed compute engines, there are typically two roles: one role for application lifecycle management and task orchestration, i.e. Driver in Spark and JobMaster for Flink; the other role for executing tasks, i.e. Executor in Spark and TaskManager for Flink. Similarly, Celeborn Client is also divided into two roles: LifecycleManager for control plane, responsible for managing all shuffle metadata for the application; and ShuffleClient for data plane, responsible for write/read data to/from Workers. LifecycleManager resides in Driver or JobMaster , one instance in each application; ShuffleClient resides in each Executor or TaskManager , one instance in each process of Executor / TaskManager . Shuffle Lifecycle A typical lifecycle of a shuffle with Celeborn is as follows: Client sends RegisterShuffle to Master. Master allocates slots among Workers and responds to Client. Client sends ReserveSlots to Workers. Workers reserve slots for the shuffle and responds to Client. Clients push data to allocated Workers. Data of the same partitionId are pushed to the same logical PartitionLocation . After all Clients finishes pushing data, Client sends CommitFiles to each Worker. Workers commit data for the shuffle then respond to Client. Clients send OpenStream to Workers for each partition split file to prepare for reading. Clients send ChunkFetchRequest to Workers to read chunks. After Client finishes reading data, Client sends UnregisterShuffle to Master to release resources. Data Reorganization Celeborn improves disk and network efficiency through data reorganization. Typically, Celeborn stores all shuffle data with the same partitionId in a logical PartitionLocation . In normal cases each PartitionLocation corresponds to a single file. When a reducer requires for the partition's data, it just needs one network connection and sequentially read the coarse grained file. In abnormal cases, such as when the file grows too large, or push data fails, Celeborn spawns a new split of the PartitionLocation , and future data within the partition will be pushed to the new split. LifecycleManager keeps the split information and tells reducer to read from all splits of the PartitionLocation to guarantee no data is lost. Data Storage Celeborn stores shuffle data in configurable multiple layers, i.e. Memory , Local Disks , Distributed File System , and Object Store . Users can specify any combination of the layers on each Worker. Currently, Celeborn only supports Local Disks and HDFS . Supporting for other storage systems are under working. Compute Engine Integration Celeborn's primary components(i.e. Master, Worker, Client) are engine irrelevant. The Client APIs are extensible and easy to implement plugins for various engines. Currently, Celeborn officially supports Spark (both Spark 2.x and Spark 3.x), Flink (1.14/1.15/1.17/1.18/1.19), and Gluten . Also, developers are integrating Celeborn with other engines, for example MR3 . Celeborn community is also working on integrating Celeborn with other engines. Graceful Shutdown In order not to impact running applications when upgrading Celeborn Cluster, Celeborn implements Graceful Upgrade. When graceful shutdown is turned on, upon shutdown, Celeborn will do the following things: Master will not allocate slots on the Worker. Worker will inform Clients to split. Client will send CommitFiles to the Worker. Then the Worker waits until all PartitionLocation flushes data to persistent storage, stores states in local RocksDB or LevelDB(deprecated), then stops itself. The process is typically within one minute. For more details, please refer to Rolling upgrade","title":"Overview"},{"location":"developers/overview/#celeborn-architecture","text":"This article introduces high level Apache Celeborn\u2122 Architecture. For more detailed description of each module/process, please refer to dedicated articles.","title":"Celeborn Architecture"},{"location":"developers/overview/#why-celeborn","text":"In distributed compute engines, data exchange between compute nodes is common but expensive. The cost comes from the disk and network inefficiency (M * N between Mappers and Reducers) in traditional shuffle frame, as following: Besides inefficiency, traditional shuffle framework requires large local storage in compute node to store shuffle data, thus blocks the adoption of disaggregated architecture. Apache Celeborn solves the problems by reorganizing shuffle data in a more efficient way, and storing the data in a separate service. The high level architecture of Celeborn is as follows:","title":"Why Celeborn"},{"location":"developers/overview/#components","text":"Celeborn has three primary components: Master, Worker, and Client. Master manages Celeborn cluster and achieves high availability(HA) based on Raft. Worker processes read-write requests. Client writes/reads data to/from Celeborn cluster, and manages shuffle metadata for the application. In most distributed compute engines, there are typically two roles: one role for application lifecycle management and task orchestration, i.e. Driver in Spark and JobMaster for Flink; the other role for executing tasks, i.e. Executor in Spark and TaskManager for Flink. Similarly, Celeborn Client is also divided into two roles: LifecycleManager for control plane, responsible for managing all shuffle metadata for the application; and ShuffleClient for data plane, responsible for write/read data to/from Workers. LifecycleManager resides in Driver or JobMaster , one instance in each application; ShuffleClient resides in each Executor or TaskManager , one instance in each process of Executor / TaskManager .","title":"Components"},{"location":"developers/overview/#shuffle-lifecycle","text":"A typical lifecycle of a shuffle with Celeborn is as follows: Client sends RegisterShuffle to Master. Master allocates slots among Workers and responds to Client. Client sends ReserveSlots to Workers. Workers reserve slots for the shuffle and responds to Client. Clients push data to allocated Workers. Data of the same partitionId are pushed to the same logical PartitionLocation . After all Clients finishes pushing data, Client sends CommitFiles to each Worker. Workers commit data for the shuffle then respond to Client. Clients send OpenStream to Workers for each partition split file to prepare for reading. Clients send ChunkFetchRequest to Workers to read chunks. After Client finishes reading data, Client sends UnregisterShuffle to Master to release resources.","title":"Shuffle Lifecycle"},{"location":"developers/overview/#data-reorganization","text":"Celeborn improves disk and network efficiency through data reorganization. Typically, Celeborn stores all shuffle data with the same partitionId in a logical PartitionLocation . In normal cases each PartitionLocation corresponds to a single file. When a reducer requires for the partition's data, it just needs one network connection and sequentially read the coarse grained file. In abnormal cases, such as when the file grows too large, or push data fails, Celeborn spawns a new split of the PartitionLocation , and future data within the partition will be pushed to the new split. LifecycleManager keeps the split information and tells reducer to read from all splits of the PartitionLocation to guarantee no data is lost.","title":"Data Reorganization"},{"location":"developers/overview/#data-storage","text":"Celeborn stores shuffle data in configurable multiple layers, i.e. Memory , Local Disks , Distributed File System , and Object Store . Users can specify any combination of the layers on each Worker. Currently, Celeborn only supports Local Disks and HDFS . Supporting for other storage systems are under working.","title":"Data Storage"},{"location":"developers/overview/#compute-engine-integration","text":"Celeborn's primary components(i.e. Master, Worker, Client) are engine irrelevant. The Client APIs are extensible and easy to implement plugins for various engines. Currently, Celeborn officially supports Spark (both Spark 2.x and Spark 3.x), Flink (1.14/1.15/1.17/1.18/1.19), and Gluten . Also, developers are integrating Celeborn with other engines, for example MR3 . Celeborn community is also working on integrating Celeborn with other engines.","title":"Compute Engine Integration"},{"location":"developers/overview/#graceful-shutdown","text":"In order not to impact running applications when upgrading Celeborn Cluster, Celeborn implements Graceful Upgrade. When graceful shutdown is turned on, upon shutdown, Celeborn will do the following things: Master will not allocate slots on the Worker. Worker will inform Clients to split. Client will send CommitFiles to the Worker. Then the Worker waits until all PartitionLocation flushes data to persistent storage, stores states in local RocksDB or LevelDB(deprecated), then stops itself. The process is typically within one minute. For more details, please refer to Rolling upgrade","title":"Graceful Shutdown"},{"location":"developers/sbt/","text":"Building via SBT Starting from version 0.4.0, the Celeborn project supports building and packaging using SBT. This article provides a detailed guide on how to build the Celeborn project using SBT. System Requirements Celeborn Service (master/worker) supports Scala 2.11/2.12/2.13 and Java 8/11/17. The following table indicates the compatibility of Celeborn Spark and Flink clients with different versions of Spark and Flink for various Java and Scala versions: Java 8/Scala 2.11 Java 8/Scala 2.12 Java 11/Scala 2.12 Java 17/Scala 2.12 Java 8/Scala 2.13 Java 11/Scala 2.13 Java 17/Scala 2.13 Spark 2.4 \u2714 \u2714 \u274c \u274c \u274c \u274c \u274c Spark 3.0 \u274c \u2714 \u2714 \u274c \u274c \u274c \u274c Spark 3.1 \u274c \u2714 \u2714 \u274c \u274c \u274c \u274c Spark 3.2 \u274c \u2714 \u2714 \u274c \u2714 \u2714 \u274c Spark 3.3 \u274c \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 Spark 3.4 \u274c \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 Spark 3.5 \u274c \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 Flink 1.14 \u274c \u2714 \u2714 \u274c \u274c \u274c \u274c Flink 1.15 \u274c \u2714 \u2714 \u274c \u274c \u274c \u274c Flink 1.17 \u274c \u2714 \u2714 \u274c \u274c \u274c \u274c Flink 1.18 \u274c \u2714 \u2714 \u274c \u274c \u274c \u274c Flink 1.19 \u274c \u2714 \u2714 \u274c \u274c \u274c \u274c Useful SBT commands Packaging the Project As an example, one can build a version of Celeborn as follows: ./build/sbt clean package To create a Celeborn distribution like those distributed by the Celeborn Downloads page, and that is laid out to be runnable, use ./build/make-distribution.sh in the project root directory. ./build/make-distribution.sh --sbt-enabled --release Maven-Style Profile Management We have adopted the Maven-style profile management for our Client module. For example, you can enable the Spark 3.3 client module by adding -Pspark-3.3 : # ./build/sbt -Pspark-3.3 projects [info] set current project to celeborn (in build file:/root/celeborn/) [info] In file:/root/celeborn/ [info] * celeborn [info] celeborn-client [info] celeborn-client-spark-3 [info] celeborn-client-spark-3-shaded [info] celeborn-common [info] celeborn-master [info] celeborn-service [info] celeborn-spark-common [info] celeborn-spark-group [info] celeborn-spark-it [info] celeborn-worker To enable the Flink 1.15 client module, add -Pflink-1.15 : # ./build/sbt -Pflink-1.15 projects [info] set current project to celeborn (in build file:/root/celeborn/) [info] In file:/root/celeborn/ [info] * celeborn [info] celeborn-client [info] celeborn-client-flink-1_15 [info] celeborn-client-flink-1_15-shaded [info] celeborn-common [info] celeborn-flink-common [info] celeborn-flink-group [info] celeborn-flink-it [info] celeborn-master [info] celeborn-service [info] celeborn-worker By using these profiles, you can easily switch between different client modules for Spark and Flink. These profiles enable specific dependencies and configurations relevant to the chosen version. This way, you can conveniently manage and build the desired configurations of the Celeborn project. Building Spark/Flink Assembly Client Jars For example, you can build the Spark 3.3 client assembly jar by running the following commands: $ ./build/sbt -Pspark-3.3 > project celeborn-client-spark-3-shaded > assembly $ # Or, you can use sbt directly with the `-Pspark-3.3` profile: $ ./build/sbt -Pspark-3.3 celeborn-client-spark-3-shaded/assembly Similarly, you can build the Flink 1.15 client assembly jar using the following commands: $ ./build/sbt -Pflink-1.15 > project celeborn-client-flink-1_15-shaded > assembly $ # Or, you can use sbt directly with the `-Pflink-1.15` profile: $ ./build/sbt -Pflink-1.15 celeborn-client-flink-1_15-shaded/assembly By executing these commands, you will create assembly jar files for the respective Spark and Flink client modules. The assembly jar bundles all the dependencies, allowing the client module to be used independently with all required dependencies included. Building submodules individually For instance, you can build the Celeborn Master module using: $ # sbt $ ./build/sbt > project celeborn-master > package $ # Or, you can build the celeborn-master module with sbt directly using: $ ./build/sbt celeborn-master/package Testing with SBT To run all tests for the Celeborn project, you can use the following command: ./build/sbt test Running tests for specific versions of Spark/Flink client. For example, to run the test cases for the Spark 3.3 client, use the following command: $ ./build/sbt -Pspark-3.3 test $ # only run spark client related modules tests $ ./build/sbt -Pspark-3.3 celeborn-spark-group/test Similarly, to run the test cases for the Flink 1.15 client, use the following command: $ ./build/sbt -Pflink-1.15 test $ # only run flink client related modules tests $ ./build/sbt -Pflink-1.15 celeborn-flink-group/test Running Individual Tests When developing locally, it\u2019s often convenient to run a single test or a few tests, rather than running the entire test suite. The fastest way to run individual tests is to use the sbt console. It\u2019s fastest to keep a sbt console open, and use it to re-run tests as necessary. For example, to run all the tests in a particular project, e.g., master: $ ./build/sbt > project celeborn-master > test You can run a single test suite using the testOnly command. For example, to run the SlotsAllocatorSuiteJ : > testOnly org.apache.celeborn.service.deploy.master.SlotsAllocatorSuiteJ The testOnly command accepts wildcards; e.g., you can also run the SlotsAllocatorSuiteJ with: > testOnly *SlotsAllocatorSuiteJ Or you could run all the tests in the master package: > testOnly org.apache.celeborn.service.deploy.master.* If you\u2019d like to run just a single Java test in the SlotsAllocatorSuiteJ , e.g., a test that with the name testAllocateSlotsForSinglePartitionId , you run the following command in the sbt console: > testOnly *SlotsAllocatorSuiteJ -- *SlotsAllocatorSuiteJ.testAllocateSlotsForSinglePartitionId If you\u2019d like to run just a single Scala test in the AppDiskUsageMetricSuite , e.g., a test that includes \"app usage snapshot\" in the name, you run the following command in the sbt console: > testOnly *AppDiskUsageMetricSuite -- -z \"app usage snapshot\" If you\u2019d prefer, you can run all of these commands on the command line (but this will be slower than running tests using an open console). To do this, you need to surround testOnly and the following arguments in quotes: $ ./build/sbt \"celeborn-master/testOnly *AppDiskUsageMetricSuite -- -z \\\"app usage snapshot\\\"\" For more about how to run individual tests with sbt, see the sbt documentation and JUnit Interface . Accelerating SBT This section provides instructions on setting up repository mirrors or proxies for a smoother SBT experience. Depending on your location and network conditions, you can choose the appropriate approach to accelerate SBT startup and enhance dependency retrieval. Accelerating SBT Startup The SBT startup process involves fetching the SBT bootstrap jar, which is typically obtained from the Maven Central Repository (https://repo1.maven.org/maven2/). If you encounter slow access to this repository or if it's inaccessible in your network environment, you can expedite the SBT startup by configuring a custom artifact repository using the DEFAULT_ARTIFACT_REPOSITORY environment variable. $ # The following command fetches sbt-launch-x.y.z.jar from https://maven.aliyun.com/nexus/content/groups/public/ $ # Ensure that the URL ends with a trailing slash \"/\" $ export DEFAULT_ARTIFACT_REPOSITORY = https://maven.aliyun.com/nexus/content/groups/public/ $ ./build/sbt This will initiate SBT using the specified repository, allowing for faster download and startup times. Custom SBT Repositories The current repositories embedded within the Celeborn project are detailed below: [repositories] local mavenLocal: file://${user.home}/.m2/repository/ local-preloaded-ivy: file:///${sbt.preloaded-${sbt.global.base-${user.home}/.sbt}/preloaded/}, [organization]/[module]/[revision]/[type]s/[artifact](-[classifier]).[ext] local-preloaded: file:///${sbt.preloaded-${sbt.global.base-${user.home}/.sbt}/preloaded/} # The system property value of `celeborn.sbt.default.artifact.repository` is # fetched from the environment variable `DEFAULT_ARTIFACT_REPOSITORY` and # assigned within the build/sbt-launch-lib.bash script. private: ${celeborn.sbt.default.artifact.repository-file:///dev/null} gcs-maven-central-mirror: https://maven-central.storage-download.googleapis.com/repos/central/data/ maven-central typesafe-ivy-releases: https://repo.typesafe.com/typesafe/ivy-releases/, [organization]/[module]/[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnly sbt-ivy-snapshots: https://repo.scala-sbt.org/scalasbt/ivy-snapshots/, [organization]/[module]/[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnly sbt-plugin-releases: https://repo.scala-sbt.org/scalasbt/sbt-plugin-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext] bintray-typesafe-sbt-plugin-releases: https://dl.bintray.com/typesafe/sbt-plugins/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext] bintray-spark-packages: https://dl.bintray.com/spark-packages/maven/ typesafe-releases: https://repo.typesafe.com/typesafe/releases/ For numerous developers across various regions, the default repository download speeds are less than optimal. To address this concern, we have curated a selection of verified public mirror templates tailored for specific regions with a significant local developer presence. For instance, we provide the repositories-cn.template template for developers situated within the expanse of the Chinese mainland, and the repositories-asia.template template designed for developers across the Asian continent. In such cases, the following command can be employed to enhance dependency download speeds: cp build/sbt-config/repositories-cn.template build/sbt-config/repositories-local Furthermore, it is strongly encouraged that developers from various regions contribute templates tailored to their respective areas. Note build/sbt-config/repositories-local takes precedence over build/sbt-config/repositories and is ignored by .gitignore . Should the environment variable DEFAULT_ARTIFACT_REPOSITORY be set, it attains the highest priority among non-local repositories. Repository priority is determined by the file order; repositories listed earlier possess higher precedence. Similarly, if your objective involves compiling and packaging within an intranet environment, you can edit build/sbt-config/repositories-local as demonstrated below: [repositories] local mavenLocal: file://${user.home}/.m2/repository/ private: ${celeborn.sbt.default.artifact.repository-file:///dev/null} private-central: https://example.com/repository/maven/ private-central-http: http://example.com/repository/maven/, allowInsecureProtocol allowInsecureProtocol is required if you want to use a repository which only supports HTTP protocol but not HTTPS, otherwise, an error will be raised ( insecure HTTP request is unsupported ), please refer to the sbt Launcher Configuration . For more details on sbt repository configuration, please refer to the SBT documentation . Publish SBT supports publishing shade clients (Spark/Flink/MapReduce) to an internal Maven private repository, such as Sonatype Nexus or JFrog . Before executing the publish command, ensure that the following environment variables are correctly set: Environment Variable Description ASF_USERNAME Sonatype repository username ASF_PASSWORD Sonatype repository password SONATYPE_SNAPSHOTS_URL Sonatype repository URL for snapshot version releases, default is \"https://repository.apache.org/content/repositories/snapshots\" SONATYPE_RELEASES_URL Sonatype repository URL for official release versions, default is \"https://repository.apache.org/service/local/staging/deploy/maven2\" For example: export SONATYPE_SNAPSHOTS_URL = http://192.168.3.46:8081/repository/maven-snapshots/ export SONATYPE_RELEASES_URL = http://192.168.3.46:8081/repository/maven-releases/ export ASF_USERNAME = admin export ASF_PASSWORD = 123456 Publish the shade client for Spark 3.5: $ ./build/sbt -Pspark-3.5 celeborn-client-spark-3-shaded/publish Publish the shade client for Flink 1.18: $ ./build/sbt -Pflink-1.18 celeborn-client-flink-1_18-shaded/publish Publish the shade client for MapReduce: $ ./build/sbt -Pmr celeborn-client-mr-shaded/publish Make sure to complete the necessary build and testing before executing the publish commands.","title":"SBT Build"},{"location":"developers/sbt/#building-via-sbt","text":"Starting from version 0.4.0, the Celeborn project supports building and packaging using SBT. This article provides a detailed guide on how to build the Celeborn project using SBT.","title":"Building via SBT"},{"location":"developers/sbt/#system-requirements","text":"Celeborn Service (master/worker) supports Scala 2.11/2.12/2.13 and Java 8/11/17. The following table indicates the compatibility of Celeborn Spark and Flink clients with different versions of Spark and Flink for various Java and Scala versions: Java 8/Scala 2.11 Java 8/Scala 2.12 Java 11/Scala 2.12 Java 17/Scala 2.12 Java 8/Scala 2.13 Java 11/Scala 2.13 Java 17/Scala 2.13 Spark 2.4 \u2714 \u2714 \u274c \u274c \u274c \u274c \u274c Spark 3.0 \u274c \u2714 \u2714 \u274c \u274c \u274c \u274c Spark 3.1 \u274c \u2714 \u2714 \u274c \u274c \u274c \u274c Spark 3.2 \u274c \u2714 \u2714 \u274c \u2714 \u2714 \u274c Spark 3.3 \u274c \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 Spark 3.4 \u274c \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 Spark 3.5 \u274c \u2714 \u2714 \u2714 \u2714 \u2714 \u2714 Flink 1.14 \u274c \u2714 \u2714 \u274c \u274c \u274c \u274c Flink 1.15 \u274c \u2714 \u2714 \u274c \u274c \u274c \u274c Flink 1.17 \u274c \u2714 \u2714 \u274c \u274c \u274c \u274c Flink 1.18 \u274c \u2714 \u2714 \u274c \u274c \u274c \u274c Flink 1.19 \u274c \u2714 \u2714 \u274c \u274c \u274c \u274c","title":"System Requirements"},{"location":"developers/sbt/#useful-sbt-commands","text":"","title":"Useful SBT commands"},{"location":"developers/sbt/#packaging-the-project","text":"As an example, one can build a version of Celeborn as follows: ./build/sbt clean package To create a Celeborn distribution like those distributed by the Celeborn Downloads page, and that is laid out to be runnable, use ./build/make-distribution.sh in the project root directory. ./build/make-distribution.sh --sbt-enabled --release","title":"Packaging the Project"},{"location":"developers/sbt/#maven-style-profile-management","text":"We have adopted the Maven-style profile management for our Client module. For example, you can enable the Spark 3.3 client module by adding -Pspark-3.3 : # ./build/sbt -Pspark-3.3 projects [info] set current project to celeborn (in build file:/root/celeborn/) [info] In file:/root/celeborn/ [info] * celeborn [info] celeborn-client [info] celeborn-client-spark-3 [info] celeborn-client-spark-3-shaded [info] celeborn-common [info] celeborn-master [info] celeborn-service [info] celeborn-spark-common [info] celeborn-spark-group [info] celeborn-spark-it [info] celeborn-worker To enable the Flink 1.15 client module, add -Pflink-1.15 : # ./build/sbt -Pflink-1.15 projects [info] set current project to celeborn (in build file:/root/celeborn/) [info] In file:/root/celeborn/ [info] * celeborn [info] celeborn-client [info] celeborn-client-flink-1_15 [info] celeborn-client-flink-1_15-shaded [info] celeborn-common [info] celeborn-flink-common [info] celeborn-flink-group [info] celeborn-flink-it [info] celeborn-master [info] celeborn-service [info] celeborn-worker By using these profiles, you can easily switch between different client modules for Spark and Flink. These profiles enable specific dependencies and configurations relevant to the chosen version. This way, you can conveniently manage and build the desired configurations of the Celeborn project.","title":"Maven-Style Profile Management"},{"location":"developers/sbt/#building-sparkflink-assembly-client-jars","text":"For example, you can build the Spark 3.3 client assembly jar by running the following commands: $ ./build/sbt -Pspark-3.3 > project celeborn-client-spark-3-shaded > assembly $ # Or, you can use sbt directly with the `-Pspark-3.3` profile: $ ./build/sbt -Pspark-3.3 celeborn-client-spark-3-shaded/assembly Similarly, you can build the Flink 1.15 client assembly jar using the following commands: $ ./build/sbt -Pflink-1.15 > project celeborn-client-flink-1_15-shaded > assembly $ # Or, you can use sbt directly with the `-Pflink-1.15` profile: $ ./build/sbt -Pflink-1.15 celeborn-client-flink-1_15-shaded/assembly By executing these commands, you will create assembly jar files for the respective Spark and Flink client modules. The assembly jar bundles all the dependencies, allowing the client module to be used independently with all required dependencies included.","title":"Building Spark/Flink Assembly Client Jars"},{"location":"developers/sbt/#building-submodules-individually","text":"For instance, you can build the Celeborn Master module using: $ # sbt $ ./build/sbt > project celeborn-master > package $ # Or, you can build the celeborn-master module with sbt directly using: $ ./build/sbt celeborn-master/package","title":"Building submodules individually"},{"location":"developers/sbt/#testing-with-sbt","text":"To run all tests for the Celeborn project, you can use the following command: ./build/sbt test Running tests for specific versions of Spark/Flink client. For example, to run the test cases for the Spark 3.3 client, use the following command: $ ./build/sbt -Pspark-3.3 test $ # only run spark client related modules tests $ ./build/sbt -Pspark-3.3 celeborn-spark-group/test Similarly, to run the test cases for the Flink 1.15 client, use the following command: $ ./build/sbt -Pflink-1.15 test $ # only run flink client related modules tests $ ./build/sbt -Pflink-1.15 celeborn-flink-group/test","title":"Testing with SBT"},{"location":"developers/sbt/#running-individual-tests","text":"When developing locally, it\u2019s often convenient to run a single test or a few tests, rather than running the entire test suite. The fastest way to run individual tests is to use the sbt console. It\u2019s fastest to keep a sbt console open, and use it to re-run tests as necessary. For example, to run all the tests in a particular project, e.g., master: $ ./build/sbt > project celeborn-master > test You can run a single test suite using the testOnly command. For example, to run the SlotsAllocatorSuiteJ : > testOnly org.apache.celeborn.service.deploy.master.SlotsAllocatorSuiteJ The testOnly command accepts wildcards; e.g., you can also run the SlotsAllocatorSuiteJ with: > testOnly *SlotsAllocatorSuiteJ Or you could run all the tests in the master package: > testOnly org.apache.celeborn.service.deploy.master.* If you\u2019d like to run just a single Java test in the SlotsAllocatorSuiteJ , e.g., a test that with the name testAllocateSlotsForSinglePartitionId , you run the following command in the sbt console: > testOnly *SlotsAllocatorSuiteJ -- *SlotsAllocatorSuiteJ.testAllocateSlotsForSinglePartitionId If you\u2019d like to run just a single Scala test in the AppDiskUsageMetricSuite , e.g., a test that includes \"app usage snapshot\" in the name, you run the following command in the sbt console: > testOnly *AppDiskUsageMetricSuite -- -z \"app usage snapshot\" If you\u2019d prefer, you can run all of these commands on the command line (but this will be slower than running tests using an open console). To do this, you need to surround testOnly and the following arguments in quotes: $ ./build/sbt \"celeborn-master/testOnly *AppDiskUsageMetricSuite -- -z \\\"app usage snapshot\\\"\" For more about how to run individual tests with sbt, see the sbt documentation and JUnit Interface .","title":"Running Individual Tests"},{"location":"developers/sbt/#accelerating-sbt","text":"This section provides instructions on setting up repository mirrors or proxies for a smoother SBT experience. Depending on your location and network conditions, you can choose the appropriate approach to accelerate SBT startup and enhance dependency retrieval.","title":"Accelerating SBT"},{"location":"developers/sbt/#accelerating-sbt-startup","text":"The SBT startup process involves fetching the SBT bootstrap jar, which is typically obtained from the Maven Central Repository (https://repo1.maven.org/maven2/). If you encounter slow access to this repository or if it's inaccessible in your network environment, you can expedite the SBT startup by configuring a custom artifact repository using the DEFAULT_ARTIFACT_REPOSITORY environment variable. $ # The following command fetches sbt-launch-x.y.z.jar from https://maven.aliyun.com/nexus/content/groups/public/ $ # Ensure that the URL ends with a trailing slash \"/\" $ export DEFAULT_ARTIFACT_REPOSITORY = https://maven.aliyun.com/nexus/content/groups/public/ $ ./build/sbt This will initiate SBT using the specified repository, allowing for faster download and startup times.","title":"Accelerating SBT Startup"},{"location":"developers/sbt/#custom-sbt-repositories","text":"The current repositories embedded within the Celeborn project are detailed below: [repositories] local mavenLocal: file://${user.home}/.m2/repository/ local-preloaded-ivy: file:///${sbt.preloaded-${sbt.global.base-${user.home}/.sbt}/preloaded/}, [organization]/[module]/[revision]/[type]s/[artifact](-[classifier]).[ext] local-preloaded: file:///${sbt.preloaded-${sbt.global.base-${user.home}/.sbt}/preloaded/} # The system property value of `celeborn.sbt.default.artifact.repository` is # fetched from the environment variable `DEFAULT_ARTIFACT_REPOSITORY` and # assigned within the build/sbt-launch-lib.bash script. private: ${celeborn.sbt.default.artifact.repository-file:///dev/null} gcs-maven-central-mirror: https://maven-central.storage-download.googleapis.com/repos/central/data/ maven-central typesafe-ivy-releases: https://repo.typesafe.com/typesafe/ivy-releases/, [organization]/[module]/[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnly sbt-ivy-snapshots: https://repo.scala-sbt.org/scalasbt/ivy-snapshots/, [organization]/[module]/[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnly sbt-plugin-releases: https://repo.scala-sbt.org/scalasbt/sbt-plugin-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext] bintray-typesafe-sbt-plugin-releases: https://dl.bintray.com/typesafe/sbt-plugins/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext] bintray-spark-packages: https://dl.bintray.com/spark-packages/maven/ typesafe-releases: https://repo.typesafe.com/typesafe/releases/ For numerous developers across various regions, the default repository download speeds are less than optimal. To address this concern, we have curated a selection of verified public mirror templates tailored for specific regions with a significant local developer presence. For instance, we provide the repositories-cn.template template for developers situated within the expanse of the Chinese mainland, and the repositories-asia.template template designed for developers across the Asian continent. In such cases, the following command can be employed to enhance dependency download speeds: cp build/sbt-config/repositories-cn.template build/sbt-config/repositories-local Furthermore, it is strongly encouraged that developers from various regions contribute templates tailored to their respective areas. Note build/sbt-config/repositories-local takes precedence over build/sbt-config/repositories and is ignored by .gitignore . Should the environment variable DEFAULT_ARTIFACT_REPOSITORY be set, it attains the highest priority among non-local repositories. Repository priority is determined by the file order; repositories listed earlier possess higher precedence. Similarly, if your objective involves compiling and packaging within an intranet environment, you can edit build/sbt-config/repositories-local as demonstrated below: [repositories] local mavenLocal: file://${user.home}/.m2/repository/ private: ${celeborn.sbt.default.artifact.repository-file:///dev/null} private-central: https://example.com/repository/maven/ private-central-http: http://example.com/repository/maven/, allowInsecureProtocol allowInsecureProtocol is required if you want to use a repository which only supports HTTP protocol but not HTTPS, otherwise, an error will be raised ( insecure HTTP request is unsupported ), please refer to the sbt Launcher Configuration . For more details on sbt repository configuration, please refer to the SBT documentation .","title":"Custom SBT Repositories"},{"location":"developers/sbt/#publish","text":"SBT supports publishing shade clients (Spark/Flink/MapReduce) to an internal Maven private repository, such as Sonatype Nexus or JFrog . Before executing the publish command, ensure that the following environment variables are correctly set: Environment Variable Description ASF_USERNAME Sonatype repository username ASF_PASSWORD Sonatype repository password SONATYPE_SNAPSHOTS_URL Sonatype repository URL for snapshot version releases, default is \"https://repository.apache.org/content/repositories/snapshots\" SONATYPE_RELEASES_URL Sonatype repository URL for official release versions, default is \"https://repository.apache.org/service/local/staging/deploy/maven2\" For example: export SONATYPE_SNAPSHOTS_URL = http://192.168.3.46:8081/repository/maven-snapshots/ export SONATYPE_RELEASES_URL = http://192.168.3.46:8081/repository/maven-releases/ export ASF_USERNAME = admin export ASF_PASSWORD = 123456 Publish the shade client for Spark 3.5: $ ./build/sbt -Pspark-3.5 celeborn-client-spark-3-shaded/publish Publish the shade client for Flink 1.18: $ ./build/sbt -Pflink-1.18 celeborn-client-flink-1_18-shaded/publish Publish the shade client for MapReduce: $ ./build/sbt -Pmr celeborn-client-mr-shaded/publish Make sure to complete the necessary build and testing before executing the publish commands.","title":"Publish"},{"location":"developers/shuffleclient/","text":"ShuffleClient Overview ShuffleClient is responsible for pushing and reading shuffle data. It's a singleton in each leaf process, i.e. Executor in Apache Spark, or TaskManager in Apache Flink. This article describes the detailed design for push data and read data. Push Data API specification The push data API is as follows: public abstract int pushData ( int shuffleId , int mapId , int attemptId , int partitionId , byte [] data , int offset , int length , int numMappers , int numPartitions ) shuffleId is the unique shuffle id of the application mapId is the map id of the shuffle attemptId is the attempt id of the map task, i.e. speculative task or task rerun for Apache Spark partitionId is the partition id the data belongs to data , offset , length specifies the bytes to be pushed numMappers is the number map tasks in the shuffle numPartitions is the number of partitions in the shuffle Lazy Shuffle Register The first time pushData is called, Client will check whether the shuffle id has been registered. If not, it sends RegisterShuffle to LifecycleManager , LifecycleManager then sends RequestSlots to Master . RequestSlots specifies how many PartitionLocation s this shuffle requires, each PartitionLocation logically responds to data of some partition id. Upon receiving RequestSlots , Master allocates slots for the shuffle among Worker s. If replication is turned on, Master allocates a pair of Worker s for each PartitionLocation to store two replicas for each PartitionLocation . The detailed allocation strategy can be found in Slots Allocation . Master then responds to LifecycleManager with the allocated PartitionLocation s. LifcycleManager caches the PartitionLocation s for the shuffle and responds to each RegisterShuffle RPCs from ShuffleClient s. Normal Push In normal cases, the process of pushing data is as follows: ShuffleClient compresses data, currently supports zstd and lz4 ShuffleClient adds Header for the data: mapId , attemptId , batchId and size . The bastchId is a unique id for the data batch inside the ( mapId , attemptId ), for the purpose of de-duplication ShuffleClient sends PushData to the Worker on which the current PartitionLocation is allocated, and holds push state for this pushing Worker receives the data, do replication if needed, then responds success ACK to ShuffleClient . For more details about how data is replicated and stored in Worker s, please refer to Worker Upon receiving success ACK from Worker , ShuffleClient considers success for this pushing and modifies the push state Push or Merge? If the size of data to be pushed is small, say hundreds of bytes, it will be very inefficient to send to the wire. So ShuffleClient offers another API: mergeData to batch data locally before sending to Worker . mergeData merges data with the same target into DataBatches . Same target means the destination for both the primary and replica are the same. When the size of a DataBatches exceeds a threshold (defaults to 64KiB ), ShuffleClient triggers pushing and sends PushMergedData to the destination. Upon receiving PushMergedData , Worker unpacks it into data segments each for a specific PartitionLocation , then stores them accordingly. Async Push Celeborn's ShuffleClient does not block compute engine's execution by asynchronous pushing, implemented in DataPusher . Whenever compute engine decides to push data, it calls DataPusher#addTask , DataPusher creates a PushTask which contains the data, and added the PushTask in a non-blocking queue. DataPusher continuously poll the queue and invokes ShuffleClient#pushData to do actual push. Split As mentioned before, Celeborn will split a PartitionLocation when any of the following conditions happens: PartitionLocation file exceeds threshold (defaults to 1GiB) Usable space of local disk is less than threshold (defaults to 5GiB) Worker is in Graceful Shutdown state Push data fails For the first three cases, Worker informs ShuffleClient that it should trigger split; for the last case, ShuffleClient triggers split itself. There are two kinds of Split: HARD_SPLIT , meaning old PartitionLocation epoch refuses to accept any data, and future data of the PartitionLocation will only be pushed after new PartitionLocation epoch is ready SOFT_SPLIT , meaning old PartitionLocation epoch continues to accept data, when new epoch is ready, ShuffleClient switches to the new location transparently The process of SOFT_SPLIT is as follows: LifecycleManager keeps the split information and tells reducer to read from all splits of the PartitionLocation to guarantee no data is lost. Read Data API specification ShuffleClient provides an API that creates an InputStream to read data from a partition id. Users can also set startMapIndex and endMapIndex to read data within the map range. public abstract CelebornInputStream readPartition ( int shuffleId , int appShuffleId , int partitionId , int attemptNumber , int startMapIndex , int endMapIndex , ExceptionMaker exceptionMaker , MetricsCallback metricsCallback ) shuffleId is the unique shuffle id of Celeborn appShuffleId is the unique shuffle id of the application partitionId is the partition id to read from attemptNumber is the attempt id of reduce task, can be safely set to any value startMapIndex is the index of start map index of interested map range, set to 0 if you want to read all partition data endMapIndex is the index of end map index of interested map range, set to Integer.MAX_VALUE if you want to read all partition data exceptionMaker is the marker of exception including fetch failure exception. metricsCallback is the callback of monitoring metrics to increase read bytes and time etc. The returned input stream is guaranteed to be Exactly Once , meaning no data lost and no duplicated reading, or else an exception will be thrown, see Here . Get PartitionLocations To read data from a partition id, ShuffleClient first checks whether the mapping from partition id to all PartitionLocation s are locally cached, if not, ShuffleClient sends GetReducerFileGroup to LifecycleManager for the mapping, see Here . Read from PartitionLocation ShuffleClient creates a PartitionReader for each PartitinLocation . As described Here , PartitionLocation data can be stored in different medium, i.e. memory, local disk, distributed filesystem. For the former two, it creates a WorkerPartitionReader to read from Worker , for the last one, it creates a DfsPartitionReader to read directly from the distributed filesystem. As described Here , the file is chunked. WorkerPartitionReader asynchronously requests multiple chunks from Worker , and reduce task consumes the data whenever available. If exception occurs when fetching a chunk, ShuffleClient will restart reading from the beginning of another (if replication is turned on, else retry the same) PartitionLocation . The reason to restart reading the whole PartitionLocation instead of the chunk is because chunks with the same index in primary and replica are not guaranteed to contain the same data, as explained Here . ShuffleClient chained the PartitionReader s and wrap them in an InputStream. To avoid duplicated read, CelebornInputStream discards data from un-successful attempts, and records batch ids it has seen within an attempt. Read from Map Range If user specifies startMapIndex and endMapIndex , CelebornInputStream will only return data within the range. Under the hood is that Worker only responds data within the range. This is achieved by sorting and indexing the file by map id upon receiving such range read request, then return the continuous data range of interest. Notice that the sort on read is only triggered upon map range read, not for the common cases where whole partition data is requested. Celeborn also optionally records map ids for each PartitionLocation , in the case of map range reading, CelebornInputStream will filter out PartitionLocation s that are out of the specified range.","title":"ShuffleClient"},{"location":"developers/shuffleclient/#shuffleclient","text":"","title":"ShuffleClient"},{"location":"developers/shuffleclient/#overview","text":"ShuffleClient is responsible for pushing and reading shuffle data. It's a singleton in each leaf process, i.e. Executor in Apache Spark, or TaskManager in Apache Flink. This article describes the detailed design for push data and read data.","title":"Overview"},{"location":"developers/shuffleclient/#push-data","text":"","title":"Push Data"},{"location":"developers/shuffleclient/#api-specification","text":"The push data API is as follows: public abstract int pushData ( int shuffleId , int mapId , int attemptId , int partitionId , byte [] data , int offset , int length , int numMappers , int numPartitions ) shuffleId is the unique shuffle id of the application mapId is the map id of the shuffle attemptId is the attempt id of the map task, i.e. speculative task or task rerun for Apache Spark partitionId is the partition id the data belongs to data , offset , length specifies the bytes to be pushed numMappers is the number map tasks in the shuffle numPartitions is the number of partitions in the shuffle","title":"API specification"},{"location":"developers/shuffleclient/#lazy-shuffle-register","text":"The first time pushData is called, Client will check whether the shuffle id has been registered. If not, it sends RegisterShuffle to LifecycleManager , LifecycleManager then sends RequestSlots to Master . RequestSlots specifies how many PartitionLocation s this shuffle requires, each PartitionLocation logically responds to data of some partition id. Upon receiving RequestSlots , Master allocates slots for the shuffle among Worker s. If replication is turned on, Master allocates a pair of Worker s for each PartitionLocation to store two replicas for each PartitionLocation . The detailed allocation strategy can be found in Slots Allocation . Master then responds to LifecycleManager with the allocated PartitionLocation s. LifcycleManager caches the PartitionLocation s for the shuffle and responds to each RegisterShuffle RPCs from ShuffleClient s.","title":"Lazy Shuffle Register"},{"location":"developers/shuffleclient/#normal-push","text":"In normal cases, the process of pushing data is as follows: ShuffleClient compresses data, currently supports zstd and lz4 ShuffleClient adds Header for the data: mapId , attemptId , batchId and size . The bastchId is a unique id for the data batch inside the ( mapId , attemptId ), for the purpose of de-duplication ShuffleClient sends PushData to the Worker on which the current PartitionLocation is allocated, and holds push state for this pushing Worker receives the data, do replication if needed, then responds success ACK to ShuffleClient . For more details about how data is replicated and stored in Worker s, please refer to Worker Upon receiving success ACK from Worker , ShuffleClient considers success for this pushing and modifies the push state","title":"Normal Push"},{"location":"developers/shuffleclient/#push-or-merge","text":"If the size of data to be pushed is small, say hundreds of bytes, it will be very inefficient to send to the wire. So ShuffleClient offers another API: mergeData to batch data locally before sending to Worker . mergeData merges data with the same target into DataBatches . Same target means the destination for both the primary and replica are the same. When the size of a DataBatches exceeds a threshold (defaults to 64KiB ), ShuffleClient triggers pushing and sends PushMergedData to the destination. Upon receiving PushMergedData , Worker unpacks it into data segments each for a specific PartitionLocation , then stores them accordingly.","title":"Push or Merge?"},{"location":"developers/shuffleclient/#async-push","text":"Celeborn's ShuffleClient does not block compute engine's execution by asynchronous pushing, implemented in DataPusher . Whenever compute engine decides to push data, it calls DataPusher#addTask , DataPusher creates a PushTask which contains the data, and added the PushTask in a non-blocking queue. DataPusher continuously poll the queue and invokes ShuffleClient#pushData to do actual push.","title":"Async Push"},{"location":"developers/shuffleclient/#split","text":"As mentioned before, Celeborn will split a PartitionLocation when any of the following conditions happens: PartitionLocation file exceeds threshold (defaults to 1GiB) Usable space of local disk is less than threshold (defaults to 5GiB) Worker is in Graceful Shutdown state Push data fails For the first three cases, Worker informs ShuffleClient that it should trigger split; for the last case, ShuffleClient triggers split itself. There are two kinds of Split: HARD_SPLIT , meaning old PartitionLocation epoch refuses to accept any data, and future data of the PartitionLocation will only be pushed after new PartitionLocation epoch is ready SOFT_SPLIT , meaning old PartitionLocation epoch continues to accept data, when new epoch is ready, ShuffleClient switches to the new location transparently The process of SOFT_SPLIT is as follows: LifecycleManager keeps the split information and tells reducer to read from all splits of the PartitionLocation to guarantee no data is lost.","title":"Split"},{"location":"developers/shuffleclient/#read-data","text":"","title":"Read Data"},{"location":"developers/shuffleclient/#api-specification_1","text":"ShuffleClient provides an API that creates an InputStream to read data from a partition id. Users can also set startMapIndex and endMapIndex to read data within the map range. public abstract CelebornInputStream readPartition ( int shuffleId , int appShuffleId , int partitionId , int attemptNumber , int startMapIndex , int endMapIndex , ExceptionMaker exceptionMaker , MetricsCallback metricsCallback ) shuffleId is the unique shuffle id of Celeborn appShuffleId is the unique shuffle id of the application partitionId is the partition id to read from attemptNumber is the attempt id of reduce task, can be safely set to any value startMapIndex is the index of start map index of interested map range, set to 0 if you want to read all partition data endMapIndex is the index of end map index of interested map range, set to Integer.MAX_VALUE if you want to read all partition data exceptionMaker is the marker of exception including fetch failure exception. metricsCallback is the callback of monitoring metrics to increase read bytes and time etc. The returned input stream is guaranteed to be Exactly Once , meaning no data lost and no duplicated reading, or else an exception will be thrown, see Here .","title":"API specification"},{"location":"developers/shuffleclient/#get-partitionlocations","text":"To read data from a partition id, ShuffleClient first checks whether the mapping from partition id to all PartitionLocation s are locally cached, if not, ShuffleClient sends GetReducerFileGroup to LifecycleManager for the mapping, see Here .","title":"Get PartitionLocations"},{"location":"developers/shuffleclient/#read-from-partitionlocation","text":"ShuffleClient creates a PartitionReader for each PartitinLocation . As described Here , PartitionLocation data can be stored in different medium, i.e. memory, local disk, distributed filesystem. For the former two, it creates a WorkerPartitionReader to read from Worker , for the last one, it creates a DfsPartitionReader to read directly from the distributed filesystem. As described Here , the file is chunked. WorkerPartitionReader asynchronously requests multiple chunks from Worker , and reduce task consumes the data whenever available. If exception occurs when fetching a chunk, ShuffleClient will restart reading from the beginning of another (if replication is turned on, else retry the same) PartitionLocation . The reason to restart reading the whole PartitionLocation instead of the chunk is because chunks with the same index in primary and replica are not guaranteed to contain the same data, as explained Here . ShuffleClient chained the PartitionReader s and wrap them in an InputStream. To avoid duplicated read, CelebornInputStream discards data from un-successful attempts, and records batch ids it has seen within an attempt.","title":"Read from PartitionLocation"},{"location":"developers/shuffleclient/#read-from-map-range","text":"If user specifies startMapIndex and endMapIndex , CelebornInputStream will only return data within the range. Under the hood is that Worker only responds data within the range. This is achieved by sorting and indexing the file by map id upon receiving such range read request, then return the continuous data range of interest. Notice that the sort on read is only triggered upon map range read, not for the common cases where whole partition data is requested. Celeborn also optionally records map ids for each PartitionLocation , in the case of map range reading, CelebornInputStream will filter out PartitionLocation s that are out of the specified range.","title":"Read from Map Range"},{"location":"developers/slotsallocation/","text":"Slots allocation This article describes the detailed design of Celeborn workers' slots allocation. Slots allocation is the core components about how Celeborn distribute workload amount workers. We have achieved two approaches of slots allocation. Principle Allocate slots to local disks unless explicit assigned to HDFS. LoadAware Related configs celeborn.master.slot.assign.policy LOADAWARE celeborn.master.slot.assign.loadAware.numDiskGroups 5 celeborn.master.slot.assign.loadAware.diskGroupGradient 0.1 celeborn.master.slot.assign.loadAware.flushTimeWeight 0 celeborn.master.slot.assign.loadAware.fetchTimeWeight 0 [spark.client.]celeborn.storage.availableTypes HDD,SSD Detail Load-aware slots allocation will take following elements into consideration. disk's fetch time disk's flush time disk's usable space disk's used slot Slots allocator will find out all worker involved in this allocation and sort their disks by disk's average flushtime * flush time weight + disk's average fetch time * fetch time weight . After getting the sorted disks list, Celeborn will split the disks into celeborn.master.slot.assign.loadAware.numDiskGroups groups. The slots number to be placed into a disk group is controlled by the celeborn.master.slot.assign.loadAware.diskGroupGradient which means that a group's allocated slots number will be (1+ celeborn.master.slot.assign.loadAware.diskGroupGradient ) times to the group's slower than it. For example, there is 5 groups, G1 , G2, G3, G4 and G5. If the G5 is allocated 100 slots. Other groups will be G4:110, G3:121, G2:133, G1:146. After Celeborn has decided the slots number of a disk group, slots will be distributed in disks of a disk group. Each disk has a usableSlots which is calculated by (disk's usable space)/(average partition size)-usedSlots . The slots number to allocate in a disk is calculated by slots of this disk group * ( current disk's usableSlots / the sum of all disks' usableSlots in this group) . For example, G5 need to allocate 100 slots and have 3 disks D1 with usable slots 100, D2 with usable slots 50, D3 with usable slots 20. The distribution will be D1:59, D2: 29, D3: 12. If all slots can be place in disk groups, the slots allocation process is done. requested slots are more than all usable slots, slots can not be placed into disks. Worker will need to allocate these slots to workers with local disks one by one. RoundRobin Detail Roundrobin slots allocation will distribute all slots into all registered workers with disks. Celeborn will treat all workers as an array and place 1 slots in a worker until all slots are allocated. If a worker has multiple disks, the chosen disk index is (monotone increasing disk index +1) % disk count . Celeborn Worker's Behavior When reserve slots Celeborn worker will decide a slot be placed in local disks or HDFS when reserve slots. If a partition is evicted from memory, the partition might be placed in HDFS. If a slot is explicitly assigned to HDFS, worker will put the slot in HDFS.","title":"Slots Allocation"},{"location":"developers/slotsallocation/#slots-allocation","text":"This article describes the detailed design of Celeborn workers' slots allocation. Slots allocation is the core components about how Celeborn distribute workload amount workers. We have achieved two approaches of slots allocation.","title":"Slots allocation"},{"location":"developers/slotsallocation/#principle","text":"Allocate slots to local disks unless explicit assigned to HDFS.","title":"Principle"},{"location":"developers/slotsallocation/#loadaware","text":"","title":"LoadAware"},{"location":"developers/slotsallocation/#related-configs","text":"celeborn.master.slot.assign.policy LOADAWARE celeborn.master.slot.assign.loadAware.numDiskGroups 5 celeborn.master.slot.assign.loadAware.diskGroupGradient 0.1 celeborn.master.slot.assign.loadAware.flushTimeWeight 0 celeborn.master.slot.assign.loadAware.fetchTimeWeight 0 [spark.client.]celeborn.storage.availableTypes HDD,SSD","title":"Related configs"},{"location":"developers/slotsallocation/#detail","text":"Load-aware slots allocation will take following elements into consideration. disk's fetch time disk's flush time disk's usable space disk's used slot Slots allocator will find out all worker involved in this allocation and sort their disks by disk's average flushtime * flush time weight + disk's average fetch time * fetch time weight . After getting the sorted disks list, Celeborn will split the disks into celeborn.master.slot.assign.loadAware.numDiskGroups groups. The slots number to be placed into a disk group is controlled by the celeborn.master.slot.assign.loadAware.diskGroupGradient which means that a group's allocated slots number will be (1+ celeborn.master.slot.assign.loadAware.diskGroupGradient ) times to the group's slower than it. For example, there is 5 groups, G1 , G2, G3, G4 and G5. If the G5 is allocated 100 slots. Other groups will be G4:110, G3:121, G2:133, G1:146. After Celeborn has decided the slots number of a disk group, slots will be distributed in disks of a disk group. Each disk has a usableSlots which is calculated by (disk's usable space)/(average partition size)-usedSlots . The slots number to allocate in a disk is calculated by slots of this disk group * ( current disk's usableSlots / the sum of all disks' usableSlots in this group) . For example, G5 need to allocate 100 slots and have 3 disks D1 with usable slots 100, D2 with usable slots 50, D3 with usable slots 20. The distribution will be D1:59, D2: 29, D3: 12. If all slots can be place in disk groups, the slots allocation process is done. requested slots are more than all usable slots, slots can not be placed into disks. Worker will need to allocate these slots to workers with local disks one by one.","title":"Detail"},{"location":"developers/slotsallocation/#roundrobin","text":"","title":"RoundRobin"},{"location":"developers/slotsallocation/#detail_1","text":"Roundrobin slots allocation will distribute all slots into all registered workers with disks. Celeborn will treat all workers as an array and place 1 slots in a worker until all slots are allocated. If a worker has multiple disks, the chosen disk index is (monotone increasing disk index +1) % disk count .","title":"Detail"},{"location":"developers/slotsallocation/#celeborn-workers-behavior","text":"When reserve slots Celeborn worker will decide a slot be placed in local disks or HDFS when reserve slots. If a partition is evicted from memory, the partition might be placed in HDFS. If a slot is explicitly assigned to HDFS, worker will put the slot in HDFS.","title":"Celeborn Worker's Behavior"},{"location":"developers/storage/","text":"Storage This article describes the detailed design of Celeborn Worker 's storage management. PartitionLocation Physical Storage Logically, PartitionLocation contains all data with the same partition id. Physically, Celeborn stores PartitionLocation in multiple files, each file corresponds to one PartitionLocation object with a unique epoch for the partition. All PartitionLocation s with the same partition id but different epochs aggregate to the complete data for the partition. The file can be in memory, local disks, or DFS/OSS, see Multi-layered Storage below. A PartitionLocation file can be read only after it is committed, trigger by CommitFiles RPC. File Layout Celeborn supports two kinds of partitions: ReducePartition , where each PartitionLocation file stores a portion of data with the same partition id, currently used for Apache Spark. MapPartition , where each PartitionLocation file stores a portion of data from the same map id, currently used for Apache Flink. ReducePartition The layout of ReducePartition is as follows: ReducePartition data file consists of several chunks (defaults to 8 MiB). Each data file has an in-memory index which points to start positions of each chunk. Upon requesting data from some partition, Worker first returns the index, then sequentially reads and returns a chunk upon each ChunkFetchRequest , which is very efficient. Notice that chunk boundaries is simply decided by the current chunk's size. In case of replication, since the order of data batch arrival is not guaranteed to be the same for primary and replica, chunks with the same chunk index will probably contain different data in primary and replica. Nevertheless, the whole files in primary and replica contain the same data batches in normal cases. MapPartition The layout of MapPartition is as follows: MapPartition data file consists of several regions (defaults to 64MiB), each region is sorted by partition id. Each region has an in-memory index which points to start positions of each partition. Upon requesting data from some partition, Worker reads the partition data from every region. Local Disk and Memory Buffer To the time this article is written, the most common case is local disk only. Users specify directories and capacity that Celeborn can use to store data. It is recommended to specify one directory per disk. If users specify more directories on one disk, Celeborn will try to figure it out and manage in the disk-level granularity. Worker periodically checks disk health, isolates unhealthy or spaceless disks, and reports to Master through heartbeat. Upon receiving ReserveSlots , Worker will first try to create a FileWriter on the hinted disk. If that disk is unavailable, Worker will choose a healthy one. Upon receiving PushData or PushMergedData , Worker unpacks the data (for PushMergedData ) and logically appends to the buffered data for each PartitionLocation (no physical memory copy). If the buffer exceeds the threshold (defaults to 256KiB), data will be flushed to the file asynchronously. If data replication is turned on, Worker will send the data to replica asynchronously. Only after Worker receives ACK from replica will it return ACK to ShuffleClient . Notice that it's not required that data is flushed to file before sending ACK. Upon receiving CommitFiles , Worker will flush all buffered data for PartitionLocation s specified in the RPC and close files, then responds the succeeded and failed PartitionLocation lists. Trigger Split Upon receiving PushData (note: currently receiving PushMergedData does not trigger Split, it's future work), Worker will check whether disk usage exceeds disk reservation (defaults to 5GiB). If so, Worker will respond Split to ShuffleClient . Celeborn supports two configurable kinds of split: HARD_SPLIT , meaning old PartitionLocation epoch refuses to accept any data, and future data of the PartitionLocation will only be pushed after new PartitionLocation epoch is ready SOFT_SPLIT , meaning old PartitionLocation epoch continues to accept data, when new epoch is ready, ShuffleClient switches to the new location transparently The detailed design of split can be found Here . Self Check In additional to health and space check on each disk, Worker also collects perf statistics to feed Master for better slots allocation : Average flush time of the last time window Average fetch time of the last time window Multi-layered Storage Celeborn aims to store data in multiple layers, i.e. memory, local disks and distributed file systems(or object store like S3, OSS). To the time this article is written, Celeborn supports local disks and HDFS. The principles of data placement are: Try to cache small data in memory Always prefer faster storage Trade off between faster storage's space and cost of data movement The high-level design of multi-layered storage is: Worker 's memory is divided into two logical regions: Push Region and Cache Region . ShuffleClient pushes data into Push Region , as \u2460 indicates. Whenever the buffered data in PushRegion for a PartitionLocation exceeds the threshold (defaults to 256KiB), Worker flushes it to some storage layer. The policy of data movement is as follows: If the PartitionLocation is not in Cache Region and Cache Region has enough space, logically move the data to Cache Region . Notice this just counts the data in Cache Region and does not physically do memory copy. As \u2461 indicates. If the PartitionLocation is in Cache Region , logically append the current data, as \u2462 indicates. If the PartitionLocation is not in Cache Region and Cache Region does not have enough memory, flush the data into local disk, as \u2463 indicates. If the PartitionLocation is not in Cache Region and both Cache Region and local disk do not have enough memory, flush the data into DFS/OSS, as \u2464 indicates. If the Cache Region exceeds the threshold, choose the largest PartitionLocation and flush it to local disk, as \u2465 indicates. Optionally, if local disk does not have enough memory, choose a PartitionLocation split and evict to HDFS/OSS.","title":"Storage"},{"location":"developers/storage/#storage","text":"This article describes the detailed design of Celeborn Worker 's storage management.","title":"Storage"},{"location":"developers/storage/#partitionlocation-physical-storage","text":"Logically, PartitionLocation contains all data with the same partition id. Physically, Celeborn stores PartitionLocation in multiple files, each file corresponds to one PartitionLocation object with a unique epoch for the partition. All PartitionLocation s with the same partition id but different epochs aggregate to the complete data for the partition. The file can be in memory, local disks, or DFS/OSS, see Multi-layered Storage below. A PartitionLocation file can be read only after it is committed, trigger by CommitFiles RPC.","title":"PartitionLocation Physical Storage"},{"location":"developers/storage/#file-layout","text":"Celeborn supports two kinds of partitions: ReducePartition , where each PartitionLocation file stores a portion of data with the same partition id, currently used for Apache Spark. MapPartition , where each PartitionLocation file stores a portion of data from the same map id, currently used for Apache Flink.","title":"File Layout"},{"location":"developers/storage/#reducepartition","text":"The layout of ReducePartition is as follows: ReducePartition data file consists of several chunks (defaults to 8 MiB). Each data file has an in-memory index which points to start positions of each chunk. Upon requesting data from some partition, Worker first returns the index, then sequentially reads and returns a chunk upon each ChunkFetchRequest , which is very efficient. Notice that chunk boundaries is simply decided by the current chunk's size. In case of replication, since the order of data batch arrival is not guaranteed to be the same for primary and replica, chunks with the same chunk index will probably contain different data in primary and replica. Nevertheless, the whole files in primary and replica contain the same data batches in normal cases.","title":"ReducePartition"},{"location":"developers/storage/#mappartition","text":"The layout of MapPartition is as follows: MapPartition data file consists of several regions (defaults to 64MiB), each region is sorted by partition id. Each region has an in-memory index which points to start positions of each partition. Upon requesting data from some partition, Worker reads the partition data from every region.","title":"MapPartition"},{"location":"developers/storage/#local-disk-and-memory-buffer","text":"To the time this article is written, the most common case is local disk only. Users specify directories and capacity that Celeborn can use to store data. It is recommended to specify one directory per disk. If users specify more directories on one disk, Celeborn will try to figure it out and manage in the disk-level granularity. Worker periodically checks disk health, isolates unhealthy or spaceless disks, and reports to Master through heartbeat. Upon receiving ReserveSlots , Worker will first try to create a FileWriter on the hinted disk. If that disk is unavailable, Worker will choose a healthy one. Upon receiving PushData or PushMergedData , Worker unpacks the data (for PushMergedData ) and logically appends to the buffered data for each PartitionLocation (no physical memory copy). If the buffer exceeds the threshold (defaults to 256KiB), data will be flushed to the file asynchronously. If data replication is turned on, Worker will send the data to replica asynchronously. Only after Worker receives ACK from replica will it return ACK to ShuffleClient . Notice that it's not required that data is flushed to file before sending ACK. Upon receiving CommitFiles , Worker will flush all buffered data for PartitionLocation s specified in the RPC and close files, then responds the succeeded and failed PartitionLocation lists.","title":"Local Disk and Memory Buffer"},{"location":"developers/storage/#trigger-split","text":"Upon receiving PushData (note: currently receiving PushMergedData does not trigger Split, it's future work), Worker will check whether disk usage exceeds disk reservation (defaults to 5GiB). If so, Worker will respond Split to ShuffleClient . Celeborn supports two configurable kinds of split: HARD_SPLIT , meaning old PartitionLocation epoch refuses to accept any data, and future data of the PartitionLocation will only be pushed after new PartitionLocation epoch is ready SOFT_SPLIT , meaning old PartitionLocation epoch continues to accept data, when new epoch is ready, ShuffleClient switches to the new location transparently The detailed design of split can be found Here .","title":"Trigger Split"},{"location":"developers/storage/#self-check","text":"In additional to health and space check on each disk, Worker also collects perf statistics to feed Master for better slots allocation : Average flush time of the last time window Average fetch time of the last time window","title":"Self Check"},{"location":"developers/storage/#multi-layered-storage","text":"Celeborn aims to store data in multiple layers, i.e. memory, local disks and distributed file systems(or object store like S3, OSS). To the time this article is written, Celeborn supports local disks and HDFS. The principles of data placement are: Try to cache small data in memory Always prefer faster storage Trade off between faster storage's space and cost of data movement The high-level design of multi-layered storage is: Worker 's memory is divided into two logical regions: Push Region and Cache Region . ShuffleClient pushes data into Push Region , as \u2460 indicates. Whenever the buffered data in PushRegion for a PartitionLocation exceeds the threshold (defaults to 256KiB), Worker flushes it to some storage layer. The policy of data movement is as follows: If the PartitionLocation is not in Cache Region and Cache Region has enough space, logically move the data to Cache Region . Notice this just counts the data in Cache Region and does not physically do memory copy. As \u2461 indicates. If the PartitionLocation is in Cache Region , logically append the current data, as \u2462 indicates. If the PartitionLocation is not in Cache Region and Cache Region does not have enough memory, flush the data into local disk, as \u2463 indicates. If the PartitionLocation is not in Cache Region and both Cache Region and local disk do not have enough memory, flush the data into DFS/OSS, as \u2464 indicates. If the Cache Region exceeds the threshold, choose the largest PartitionLocation and flush it to local disk, as \u2465 indicates. Optionally, if local disk does not have enough memory, choose a PartitionLocation split and evict to HDFS/OSS.","title":"Multi-layered Storage"},{"location":"developers/trafficcontrol/","text":"Traffic Control This article describes the detailed design of Celeborn Worker 's traffic control. Design Goal The design goal of Traffic Control is to prevent Worker OOM without harming performance. At the same time, Celeborn tries to achieve fairness without harming performance. Celeborn reaches the goal through Back Pressure and Congestion Control . Data Flow From the Worker 's perspective, the income data flow comes from two sources: ShuffleClient that pushes primary data to the primary Worker Primary Worker that sends data replication to the replica Worker The buffered memory can be released when the following conditions are satisfied: Data is flushed to file If replication is on, after primary data is written to wire The basic idea is that, when Worker is under high memory pressure, slow down or stop income data, and at same time force flush to release memory. Back Pressure Back Pressure defines three watermarks: Pause Receive watermark (defaults to 0.85). If used direct memory ratio exceeds this, Worker will pause receiving data from ShuffleClient , and force flush buffered data into file. Pause Replicate watermark (defaults to 0.95). If used direct memory ratio exceeds this, Worker will pause receiving both data from ShuffleClient and replica data from primary Worker , and force flush buffered data into file. Resume watermark (defaults to 0.7). When either Pause Receive or Pause Replicate is triggered, to resume receiving data from ShuffleClient , the used direct memory ratio should decrease under this watermark. Worker high-frequently checks used direct memory ratio, and triggers Pause Receive , Pause Replicate and Resume accordingly. The state machine is as follows: Back Pressure is the basic traffic control and can't be disabled. Users can tune the three watermarks through the following configuration. celeborn.worker.directMemoryRatio* Congestion Control Congestion Control is an optional mechanism for traffic control, the purpose is to slow down the push rate from ShuffleClient when memory is under pressure, and suppress those who occupied the most resources in the last time window. It defines two watermarks: Low Watermark , under which everything goes OK High Watermark , when exceeds this, top users will be Congestion Controlled Celeborn uses UserIdentifier to identify users. Worker collects bytes pushed from each user in the last time window. When used direct memory exceeds High Watermark , users who occupied more resources than the average occupation will receive Congestion Control message. ShuffleClient controls the push ratio in a fashion that is very like TCP Congestion Control . Initially, it's in Slow Start phase, with a low push rate but increases very fast. When threshold is reached, it transfers to Congestion Avoidance phase, which slowly increases push rate. Upon receiving Congestion Control , it goes back to Slow Start phase. Congestion Control can be enabled and tuned by the following configurations: celeborn.worker.congestionControl.*","title":"Traffic Control"},{"location":"developers/trafficcontrol/#traffic-control","text":"This article describes the detailed design of Celeborn Worker 's traffic control.","title":"Traffic Control"},{"location":"developers/trafficcontrol/#design-goal","text":"The design goal of Traffic Control is to prevent Worker OOM without harming performance. At the same time, Celeborn tries to achieve fairness without harming performance. Celeborn reaches the goal through Back Pressure and Congestion Control .","title":"Design Goal"},{"location":"developers/trafficcontrol/#data-flow","text":"From the Worker 's perspective, the income data flow comes from two sources: ShuffleClient that pushes primary data to the primary Worker Primary Worker that sends data replication to the replica Worker The buffered memory can be released when the following conditions are satisfied: Data is flushed to file If replication is on, after primary data is written to wire The basic idea is that, when Worker is under high memory pressure, slow down or stop income data, and at same time force flush to release memory.","title":"Data Flow"},{"location":"developers/trafficcontrol/#back-pressure","text":"Back Pressure defines three watermarks: Pause Receive watermark (defaults to 0.85). If used direct memory ratio exceeds this, Worker will pause receiving data from ShuffleClient , and force flush buffered data into file. Pause Replicate watermark (defaults to 0.95). If used direct memory ratio exceeds this, Worker will pause receiving both data from ShuffleClient and replica data from primary Worker , and force flush buffered data into file. Resume watermark (defaults to 0.7). When either Pause Receive or Pause Replicate is triggered, to resume receiving data from ShuffleClient , the used direct memory ratio should decrease under this watermark. Worker high-frequently checks used direct memory ratio, and triggers Pause Receive , Pause Replicate and Resume accordingly. The state machine is as follows: Back Pressure is the basic traffic control and can't be disabled. Users can tune the three watermarks through the following configuration. celeborn.worker.directMemoryRatio*","title":"Back Pressure"},{"location":"developers/trafficcontrol/#congestion-control","text":"Congestion Control is an optional mechanism for traffic control, the purpose is to slow down the push rate from ShuffleClient when memory is under pressure, and suppress those who occupied the most resources in the last time window. It defines two watermarks: Low Watermark , under which everything goes OK High Watermark , when exceeds this, top users will be Congestion Controlled Celeborn uses UserIdentifier to identify users. Worker collects bytes pushed from each user in the last time window. When used direct memory exceeds High Watermark , users who occupied more resources than the average occupation will receive Congestion Control message. ShuffleClient controls the push ratio in a fashion that is very like TCP Congestion Control . Initially, it's in Slow Start phase, with a low push rate but increases very fast. When threshold is reached, it transfers to Congestion Avoidance phase, which slowly increases push rate. Upon receiving Congestion Control , it goes back to Slow Start phase. Congestion Control can be enabled and tuned by the following configurations: celeborn.worker.congestionControl.*","title":"Congestion Control"},{"location":"developers/worker/","text":"Worker The main functions of Celeborn Worker are: Store, serve, and manage PartitionLocation data. See Storage Traffic control through Back Pressure and Congestion Control . See Traffic Control Support rolling upgrade through Graceful Shutdown Support elasticity through Decommission Shutdown Self health check Celeborn Worker has four dedicated servers: Controller handles control messages, i.e. ReserveSlots , CommitFiles , and DestroyWorkerSlots Push Server handles primary input data, i.e. PushData and PushMergedData , and push related control messages Replicate Server handles replica input data, it has the same logic with Push Server Fetch Server handles fetch requests, i.e. ChunkFetchRequest , and fetch related control messages","title":"Overview"},{"location":"developers/worker/#worker","text":"The main functions of Celeborn Worker are: Store, serve, and manage PartitionLocation data. See Storage Traffic control through Back Pressure and Congestion Control . See Traffic Control Support rolling upgrade through Graceful Shutdown Support elasticity through Decommission Shutdown Self health check Celeborn Worker has four dedicated servers: Controller handles control messages, i.e. ReserveSlots , CommitFiles , and DestroyWorkerSlots Push Server handles primary input data, i.e. PushData and PushMergedData , and push related control messages Replicate Server handles replica input data, it has the same logic with Push Server Fetch Server handles fetch requests, i.e. ChunkFetchRequest , and fetch related control messages","title":"Worker"},{"location":"developers/workerexclusion/","text":"Worker Exclusion Worker s can fail, temporarily or permanently. To reduce the impact of Worker failure, Celeborn tries to figure out Worker status as soon as possible, and as correct as possible. This article describes detailed design of Worker exclusion. Participants As described Previously , Celeborn has three components: Master , Worker , and Client . Client is further separated into LifecycleManager and ShuffleClient . Master / LifecycleManager / ShuffleClient need to know about Worker status, actively or reactively. Master Side Exclusion Master maintains the ground-truth status of Worker s, with relatively longer delay. Master maintains four lists of Worker s with different status: Active list. Worker s that have successfully registered to Master , and heartbeat never timed out. Excluded list. Worker s that are inside active list, but have no available disks for allocating new slots. Master recognizes such Worker s through heartbeat from Worker s. Graceful shutdown list. Worker s that are inside active list, but have triggered Graceful Shutdown . Master expects these Worker s should re-register themselves soon. Lost list. Worker s whose heartbeat timed out. These Worker s will be removed from active and excluded list, but will not be removed from graceful shutdown list. Upon receiving RequestSlots, Master will choose Worker s in active list subtracting excluded and graceful shutdown list. Since Master only exclude Worker s upon heartbeat, it has relative long delay. ShuffleClient Side Exclusion ShuffleClient 's local exclusion list is essential to performance. Say the timeout to create network connection is 10s, if ShuffleClient blindly pushes data to a non-exist Worker , the task will hang for a long time. Waiting for Master to inform the exclusion list is unacceptable because of the delay. Instead, ShuffleClient actively exclude Worker s when it encounters critical exceptions, for example: Fail to create network connection Fail to push data Fail to fetch data Connection exception happened In addition to exclude the Worker s locally, ShuffleClient also carries the cause of push failure with Revive to LifecycleManager , see the section below. Such strategy is aggressive, false negative may happen. To rectify, ShuffleClient removes a Worker from the excluded list whenever an event happens that indicates that Worker is available, for example: When the Worker is allocated slots in register shuffle When LifecycleManager says the Worker is available in response of Revive Currently, exclusion in ShuffleClient is optional, users can configure using the following configs: celeborn.client.push/fetch.excludeWorkerOnFailure.enabled LifecycleManager Side Exclusion The accuracy and delay in LifecycleManager 's exclusion list stands between Master and Worker . LifecyleManager excludes a Worker in the following scenarios: Receives Revive request and the cause is critical Fail to send RPC to a Worker From Master 's excluded list, carried in the heartbeat response LifecycleManager will remove Worker from the excluded list in the following scenarios: For critical causes, when timeout expires (defaults to 180s) For non-critical causes, when it's not in Master 's exclusion list In the response of Revive, LifecycleManager checks the status of the Worker where previous push data has failed. ShuffleClient will remove from local exclusion list if the Worker is available.","title":"Worker Exclusion"},{"location":"developers/workerexclusion/#worker-exclusion","text":"Worker s can fail, temporarily or permanently. To reduce the impact of Worker failure, Celeborn tries to figure out Worker status as soon as possible, and as correct as possible. This article describes detailed design of Worker exclusion.","title":"Worker Exclusion"},{"location":"developers/workerexclusion/#participants","text":"As described Previously , Celeborn has three components: Master , Worker , and Client . Client is further separated into LifecycleManager and ShuffleClient . Master / LifecycleManager / ShuffleClient need to know about Worker status, actively or reactively.","title":"Participants"},{"location":"developers/workerexclusion/#master-side-exclusion","text":"Master maintains the ground-truth status of Worker s, with relatively longer delay. Master maintains four lists of Worker s with different status: Active list. Worker s that have successfully registered to Master , and heartbeat never timed out. Excluded list. Worker s that are inside active list, but have no available disks for allocating new slots. Master recognizes such Worker s through heartbeat from Worker s. Graceful shutdown list. Worker s that are inside active list, but have triggered Graceful Shutdown . Master expects these Worker s should re-register themselves soon. Lost list. Worker s whose heartbeat timed out. These Worker s will be removed from active and excluded list, but will not be removed from graceful shutdown list. Upon receiving RequestSlots, Master will choose Worker s in active list subtracting excluded and graceful shutdown list. Since Master only exclude Worker s upon heartbeat, it has relative long delay.","title":"Master Side Exclusion"},{"location":"developers/workerexclusion/#shuffleclient-side-exclusion","text":"ShuffleClient 's local exclusion list is essential to performance. Say the timeout to create network connection is 10s, if ShuffleClient blindly pushes data to a non-exist Worker , the task will hang for a long time. Waiting for Master to inform the exclusion list is unacceptable because of the delay. Instead, ShuffleClient actively exclude Worker s when it encounters critical exceptions, for example: Fail to create network connection Fail to push data Fail to fetch data Connection exception happened In addition to exclude the Worker s locally, ShuffleClient also carries the cause of push failure with Revive to LifecycleManager , see the section below. Such strategy is aggressive, false negative may happen. To rectify, ShuffleClient removes a Worker from the excluded list whenever an event happens that indicates that Worker is available, for example: When the Worker is allocated slots in register shuffle When LifecycleManager says the Worker is available in response of Revive Currently, exclusion in ShuffleClient is optional, users can configure using the following configs: celeborn.client.push/fetch.excludeWorkerOnFailure.enabled","title":"ShuffleClient Side Exclusion"},{"location":"developers/workerexclusion/#lifecyclemanager-side-exclusion","text":"The accuracy and delay in LifecycleManager 's exclusion list stands between Master and Worker . LifecyleManager excludes a Worker in the following scenarios: Receives Revive request and the cause is critical Fail to send RPC to a Worker From Master 's excluded list, carried in the heartbeat response LifecycleManager will remove Worker from the excluded list in the following scenarios: For critical causes, when timeout expires (defaults to 180s) For non-critical causes, when it's not in Master 's exclusion list In the response of Revive, LifecycleManager checks the status of the Worker where previous push data has failed. ShuffleClient will remove from local exclusion list if the Worker is available.","title":"LifecycleManager Side Exclusion"}]}